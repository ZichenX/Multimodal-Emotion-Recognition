{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc13233",
   "metadata": {
    "papermill": {
     "duration": 0.017272,
     "end_time": "2024-09-18T17:13:47.368383",
     "exception": false,
     "start_time": "2024-09-18T17:13:47.351111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Multimodal Emotion Recognition(2/2)\n",
    "## Zichen Xu（zichenxu407@gmail.com）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585793e8-000d-4472-b9fc-dcf15c4ca69a",
   "metadata": {},
   "source": [
    "The feature abstraction procedure is primarily adapted and modified from the work presented in: https://arxiv.org/abs/1804.05788. And we have published this work on kaggle as well: https://www.kaggle.com/code/girlfriendohmygod/emo-detection-transformer/notebook\n",
    "\n",
    "The code primarily handles audio and motion capture data from the IEMOCAP dataset. It includes three functions: `get_mocap_hand`, `get_mocap_rot`, and `get_mocap_head`, which are used to read and calculate the average values of hand, rotation, and head motion capture data, respectively. Finally, the function `read_iemocap_mocap` integrates the audio, emotion labels, transcribed text, and the three types of motion capture data mentioned above. It processes and sorts them into a unified format, returning an array that contains all the processed data.\n",
    "\n",
    "- The functions used are encapsulated in the files `features.py`, `helper.py`, and `mocap_data_collect.py`.\n",
    "- **The results after data cleaning and preprocessing are stored as .pickle files. These include two datasets: \"pikle-data\" (a partial dataset using only 1/5 of the data for lightweight model tuning) and \"pikle-full-data\" (the full dataset), both of which are open-sourced on the Kaggle platform. These can be directly accessed and reused without the need for separate data cleaning and preprocessing each time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400b90c",
   "metadata": {
    "papermill": {
     "duration": 0.016177,
     "end_time": "2024-09-18T17:13:47.468590",
     "exception": false,
     "start_time": "2024-09-18T17:13:47.452413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Package loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2953ab5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:13:47.503527Z",
     "iopub.status.busy": "2024-09-18T17:13:47.502719Z",
     "iopub.status.idle": "2024-09-18T17:14:01.947673Z",
     "shell.execute_reply": "2024-09-18T17:14:01.946765Z"
    },
    "papermill": {
     "duration": 14.465027,
     "end_time": "2024-09-18T17:14:01.950026",
     "exception": false,
     "start_time": "2024-09-18T17:13:47.484999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54255918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:14:01.985767Z",
     "iopub.status.busy": "2024-09-18T17:14:01.985486Z",
     "iopub.status.idle": "2024-09-18T17:14:10.357505Z",
     "shell.execute_reply": "2024-09-18T17:14:10.356635Z"
    },
    "papermill": {
     "duration": 8.392215,
     "end_time": "2024-09-18T17:14:10.359798",
     "exception": false,
     "start_time": "2024-09-18T17:14:01.967583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import wave\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers.core import Dense, Activation\n",
    "# from keras.layers import LSTM, Input, Flatten, Merge\n",
    "# from keras.layers.wrappers import TimeDistributed\n",
    "# from keras.optimizers import SGD, Adam, RMSprop\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "module_path = '/kaggle/input/emo-detection1/'\n",
    "sys.path.append(module_path)\n",
    "from features import *\n",
    "from helper import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35fd5ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:14:10.395962Z",
     "iopub.status.busy": "2024-09-18T17:14:10.394982Z",
     "iopub.status.idle": "2024-09-18T17:14:10.400456Z",
     "shell.execute_reply": "2024-09-18T17:14:10.399663Z"
    },
    "papermill": {
     "duration": 0.025374,
     "end_time": "2024-09-18T17:14:10.402423",
     "exception": false,
     "start_time": "2024-09-18T17:14:10.377049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_feat = 34\n",
    "nb_class = 4\n",
    "nb_epoch = 80\n",
    "\n",
    "optimizer = 'Adadelta'\n",
    "\n",
    "\n",
    "# code_path = os.path.dirname('/kaggle/input/iemocapfullrelease/IEMOCAP_full_release')\n",
    "emotions_used = np.array(['ang', 'exc', 'neu', 'sad'])\n",
    "data_path = os.path.dirname('/kaggle/input/iemocapfullrelease/IEMOCAP_full_release/')\n",
    "sessions = ['/Session1']\n",
    "# sessions = ['/Session1', '/Session2', '/Session3', '/Session4', '/Session5']\n",
    "framerate = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a471a",
   "metadata": {
    "papermill": {
     "duration": 0.016794,
     "end_time": "2024-09-18T17:14:10.435863",
     "exception": false,
     "start_time": "2024-09-18T17:14:10.419069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c46e96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:14:10.506483Z",
     "iopub.status.busy": "2024-09-18T17:14:10.506233Z",
     "iopub.status.idle": "2024-09-18T17:14:10.515447Z",
     "shell.execute_reply": "2024-09-18T17:14:10.514633Z"
    },
    "papermill": {
     "duration": 0.029648,
     "end_time": "2024-09-18T17:14:10.517231",
     "exception": false,
     "start_time": "2024-09-18T17:14:10.487583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# def get_mocap_hand(path_to_mocap_hand, filename, start, end):\n",
    "#     mocap_hand_avg = []  # 初始化一个列表，用于存储手部运动捕捉数据的平均值\n",
    "#     with open(path_to_mocap_hand + filename, 'r') as file:\n",
    "#         next(file)  # 跳过文件的标题行\n",
    "#         for line in file:\n",
    "#             data2 = line.split()  # 将每行数据按空格分割成列表\n",
    "#             if len(data2) < 3:  # 检查数据行是否有效\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 time_value = float(data2[1])  # 尝试将时间值转换为浮点数\n",
    "#                 if time_value > start and time_value < end:  # 检查时间值是否在指定的范围内\n",
    "#                     mocap_hand_avg.append(np.array(data2[2:]).astype(float))  # 将运动捕捉数据（从第三列开始）转换为浮点数并添加到列表中\n",
    "#             except ValueError:\n",
    "#                 continue  # 跳过无法转换为浮点数的行\n",
    "    \n",
    "#     if mocap_hand_avg:  # 如果有有效的数据行\n",
    "#         mocap_hand_avg = np.array_split(np.array(mocap_hand_avg), 200)  # 将数据分成200个子集\n",
    "#         for spl in mocap_hand_avg:\n",
    "#             spl = np.mean(spl, axis=0)  # 计算每个子集的平均值\n",
    "#     else:\n",
    "#         mocap_hand_avg = [np.zeros(1)]  # 如果没有有效数据行，则返回一个零数组\n",
    "    \n",
    "#     return mocap_hand_avg  # 返回计算的手部运动捕捉数据的平均值\n",
    "\n",
    "\n",
    "# def get_mocap_rot(path_to_mocap_rot, filename, start, end):\n",
    "#     mocap_rot_avg = []  # 初始化一个列表，用于存储旋转运动捕捉数据的平均值\n",
    "#     with open(path_to_mocap_rot + filename, 'r') as file:\n",
    "#         next(file)  # 跳过文件的标题行\n",
    "#         for line in file:\n",
    "#             data2 = line.split()  # 将每行数据按空格分割成列表\n",
    "#             if len(data2) < 3:  # 检查数据行是否有效\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 time_value = float(data2[1])  # 尝试将时间值转换为浮点数\n",
    "#                 if time_value > start and time_value < end:  # 检查时间值是否在指定的范围内\n",
    "#                     mocap_rot_avg.append(np.array(data2[2:]).astype(float))  # 将运动捕捉数据（从第三列开始）转换为浮点数并添加到列表中\n",
    "#             except ValueError:\n",
    "#                 continue  # 跳过无法转换为浮点数的行\n",
    "    \n",
    "#     if mocap_rot_avg:  # 如果有有效的数据行\n",
    "#         mocap_rot_avg = np.array_split(np.array(mocap_rot_avg), 200)  # 将数据分成200个子集\n",
    "#         for spl in mocap_rot_avg:\n",
    "#             spl = np.mean(spl, axis=0)  # 计算每个子集的平均值\n",
    "#     else:\n",
    "#         mocap_rot_avg = [np.zeros(1)]  # 如果没有有效数据行，则返回一个零数组\n",
    "    \n",
    "#     return mocap_rot_avg  # 返回计算的旋转运动捕捉数据的平均值\n",
    "\n",
    "\n",
    "# def get_mocap_head(path_to_mocap_head, filename, start, end):\n",
    "#     mocap_head_avg = []  # 初始化一个列表，用于存储头部运动捕捉数据的平均值\n",
    "#     with open(path_to_mocap_head + filename, 'r') as file:\n",
    "#         next(file)  # 跳过文件的标题行\n",
    "#         for line in file:\n",
    "#             data2 = line.split()  # 将每行数据按空格分割成列表\n",
    "#             if len(data2) < 3:  # 检查数据行是否有效\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 time_value = float(data2[1])  # 尝试将时间值转换为浮点数\n",
    "#                 if time_value > start and time_value < end:  # 检查时间值是否在指定的范围内\n",
    "#                     mocap_head_avg.append(np.array(data2[2:]).astype(float))  # 将运动捕捉数据（从第三列开始）转换为浮点数并添加到列表中\n",
    "#             except ValueError:\n",
    "#                 continue  # 跳过无法转换为浮点数的行\n",
    "    \n",
    "#     if mocap_head_avg:  # 如果有有效的数据行\n",
    "#         mocap_head_avg = np.array_split(np.array(mocap_head_avg), 200)  # 将数据分成200个子集\n",
    "#         for spl in mocap_head_avg:\n",
    "#             spl = np.mean(spl, axis=0)  # 计算每个子集的平均值\n",
    "#     else:\n",
    "#         mocap_head_avg = [np.zeros(1)]  # 如果没有有效数据行，则返回一个零数组\n",
    "    \n",
    "#     return mocap_head_avg  # 返回计算的头部运动捕捉数据的平均值\n",
    "\n",
    "\n",
    "# def read_iemocap_mocap():\n",
    "#     data = []  # 初始化一个列表，用于存储最终整合的数据\n",
    "#     ids = {}  # 初始化一个字典，用于存储已经处理过的ID\n",
    "#     for session in sessions:  # 遍历每个会话\n",
    "#         path_to_wav = data_path + session + '/dialog/wav/'  # 构建音频文件路径\n",
    "#         path_to_emotions = data_path + session + '/dialog/EmoEvaluation/'  # 构建情感标签文件路径\n",
    "#         path_to_transcriptions = data_path + session + '/dialog/transcriptions/'  # 构建转录文本文件路径\n",
    "#         path_to_mocap_hand = data_path + session + '/dialog/MOCAP_hand/'  # 构建手部运动捕捉文件路径\n",
    "#         path_to_mocap_rot = data_path + session + '/dialog/MOCAP_rotated/'  # 构建旋转运动捕捉文件路径\n",
    "#         path_to_mocap_head = data_path + session + '/dialog/MOCAP_head/'  # 构建头部运动捕捉文件路径\n",
    "\n",
    "#         files2 = os.listdir(path_to_wav)  # 列出音频文件目录下的所有文件\n",
    "\n",
    "#         files = []\n",
    "#         for f in files2:  # 遍历所有文件\n",
    "#             if f.endswith(\".wav\"):  # 检查文件是否是.wav文件\n",
    "#                 if f[0] == '.':\n",
    "#                     files.append(f[2:-4])  # 处理隐藏文件\n",
    "#                 else:\n",
    "#                     files.append(f[:-4])  # 处理普通文件\n",
    "\n",
    "#         for f in files:  # 遍历所有音频文件\n",
    "#             print(f)\n",
    "#             mocap_f = f\n",
    "#             if f == 'Ses05M_script01_1b':\n",
    "#                 mocap_f = 'Ses05M_script01_1'  # 特殊处理某个文件名\n",
    "\n",
    "#             wav = get_audio(path_to_wav, f + '.wav')  # 读取音频文件\n",
    "#             transcriptions = get_transcriptions(path_to_transcriptions, f + '.txt')  # 读取转录文本\n",
    "#             emotions = get_emotions(path_to_emotions, f + '.txt')  # 读取情感标签\n",
    "#             sample = split_wav(wav, emotions)  # 将音频文件分割成若干段\n",
    "\n",
    "#             for ie, e in enumerate(emotions):  # 遍历每个情感标签\n",
    "#                 e['signal'] = sample[ie]['left']  # 将音频段的左声道信号添加到情感标签中\n",
    "#                 e.pop(\"left\", None)\n",
    "#                 e.pop(\"right\", None)\n",
    "#                 e['transcription'] = transcriptions[e['id']]  # 将转录文本添加到情感标签中\n",
    "#                 e['mocap_hand'] = get_mocap_hand(path_to_mocap_hand, mocap_f + '.txt', e['start'], e['end'])  # 获取手部运动捕捉数据\n",
    "#                 e['mocap_rot'] = get_mocap_rot(path_to_mocap_rot, mocap_f + '.txt', e['start'], e['end'])  # 获取旋转运动捕捉数据\n",
    "#                 e['mocap_head'] = get_mocap_head(path_to_mocap_head, mocap_f + '.txt', e['start'], e['end'])  # 获取头部运动捕捉数据\n",
    "#                 if e['emotion'] in emotions_used:  # 检查情感是否在使用的情感集合中\n",
    "#                     if e['id'] not in ids:  # 检查该ID是否已处理过\n",
    "#                         data.append(e)  # 将情感标签数据添加到最终数据列表中\n",
    "#                         ids[e['id']] = 1  # 将ID标记为已处理\n",
    "\n",
    "#     sort_key = get_field(data, \"id\")  # 获取排序键\n",
    "#     return np.array(data)[np.argsort(sort_key)]  # 按ID排序并返回数据数组\n",
    "\n",
    "# data = read_iemocap_mocap()  # 调用函数读取IEMOCAP数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e43e53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:14:10.551847Z",
     "iopub.status.busy": "2024-09-18T17:14:10.551584Z",
     "iopub.status.idle": "2024-09-18T17:14:10.555442Z",
     "shell.execute_reply": "2024-09-18T17:14:10.554649Z"
    },
    "papermill": {
     "duration": 0.023332,
     "end_time": "2024-09-18T17:14:10.557310",
     "exception": false,
     "start_time": "2024-09-18T17:14:10.533978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle  # 导入pickle模块，用于对象的序列化和反序列化\n",
    "\n",
    "# # 使用'wb'（写入二进制）模式打开一个名为'data_collected.pickle'的文件\n",
    "# # '/kaggle/working/' 是文件的路径\n",
    "# with open('/kaggle/working/'+'data_collected.pickle', 'wb') as handle:\n",
    "\n",
    "#     # 使用pickle的dump函数，将'data'对象序列化并保存到已打开的文件中\n",
    "#     # protocol=pickle.HIGHEST_PROTOCOL表示使用最高可用的pickle协议进行序列化\n",
    "#     pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545c044",
   "metadata": {
    "papermill": {
     "duration": 0.016473,
     "end_time": "2024-09-18T17:14:10.590411",
     "exception": false,
     "start_time": "2024-09-18T17:14:10.573938",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Feature abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "983056cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:14:10.624724Z",
     "iopub.status.busy": "2024-09-18T17:14:10.624474Z",
     "iopub.status.idle": "2024-09-18T17:15:00.652274Z",
     "shell.execute_reply": "2024-09-18T17:15:00.651419Z"
    },
    "papermill": {
     "duration": 50.047521,
     "end_time": "2024-09-18T17:15:00.654664",
     "exception": false,
     "start_time": "2024-09-18T17:14:10.607143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle  # 导入pickle模块，用于读取和写入pickle文件\n",
    "\n",
    "# 打开包含数据的pickle文件\n",
    "with open('/kaggle/input/pikle-full-data/data_collected_full.pickle', 'rb') as handle:\n",
    "    data2 = pickle.load(handle)  # 使用pickle.load()方法加载pickle文件中的数据，并将其存储在变量data2中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66c37ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:15:00.690070Z",
     "iopub.status.busy": "2024-09-18T17:15:00.689749Z",
     "iopub.status.idle": "2024-09-18T17:15:01.878702Z",
     "shell.execute_reply": "2024-09-18T17:15:01.877630Z"
    },
    "papermill": {
     "duration": 1.208908,
     "end_time": "2024-09-18T17:15:01.880854",
     "exception": false,
     "start_time": "2024-09-18T17:15:00.671946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4936, 500])\n"
     ]
    }
   ],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "from torchtext.data.utils import get_tokenizer  # 导入torchtext库中的分词器\n",
    "from torchtext.vocab import build_vocab_from_iterator  # 从迭代器构建词汇表\n",
    "from torch.nn.utils.rnn import pad_sequence  # 对序列进行填充\n",
    "\n",
    "# 假设 data2 已经被加载并包含需要的数据信息\n",
    "\n",
    "text = [ses_mod['transcription'] for ses_mod in data2]  # 从data2中提取出'transcription'字段的文本数据\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 500  # 设置最大序列长度\n",
    "\n",
    "# 使用 torchtext 的 basic_english 分词器\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# 构建词汇表\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text), specials=[\"<unk>\"])  # 从文本数据中构建词汇表，并指定特殊标记\"<unk>\"\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # 设置默认索引为\"<unk>\"\n",
    "\n",
    "# 将文本转换为序列\n",
    "token_tr_X = [vocab(tokenizer(t)) for t in text]\n",
    "\n",
    "# 将序列填充到相同的长度\n",
    "x_train_text = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in token_tr_X], \n",
    "                            batch_first=True, padding_value=vocab[\"<unk>\"])\n",
    "\n",
    "# 如果序列长度超过最大长度，则进行截断\n",
    "x_train_text = x_train_text[:, :MAX_SEQUENCE_LENGTH]\n",
    "\n",
    "# 如果序列长度不足最大长度，则进行填充\n",
    "if x_train_text.size(1) < MAX_SEQUENCE_LENGTH:\n",
    "    pad_size = MAX_SEQUENCE_LENGTH - x_train_text.size(1)\n",
    "    x_train_text = torch.cat((x_train_text, torch.full((x_train_text.size(0), pad_size), vocab[\"<unk>\"], dtype=torch.long)), dim=1)\n",
    "\n",
    "print(x_train_text.shape)  # 打印结果检查维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6b3a93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:15:01.918206Z",
     "iopub.status.busy": "2024-09-18T17:15:01.917522Z",
     "iopub.status.idle": "2024-09-18T17:17:42.981934Z",
     "shell.execute_reply": "2024-09-18T17:17:42.980860Z"
    },
    "papermill": {
     "duration": 161.101399,
     "end_time": "2024-09-18T17:17:43.001143",
     "exception": false,
     "start_time": "2024-09-18T17:15:01.899744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3129 unique tokens\n",
      "/kaggle/input/glove42b300dtxt/glove.42B.300d.txt\n",
      "G Word embeddings: 1917494\n",
      "G Null word embeddings: 475\n",
      "Embedding layer weights shape: torch.Size([3130, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "import numpy as np  # 导入NumPy库\n",
    "import os  # 导入操作系统模块\n",
    "from torchtext.data.utils import get_tokenizer  # 导入torchtext库中的分词器\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab  # 从迭代器构建词汇表、Vocab类\n",
    "\n",
    "EMBEDDING_DIM = 300  # 设置词嵌入的维度为300\n",
    "\n",
    "# 假设 data2 已经被加载并包含需要的数据信息\n",
    "text = [ses_mod['transcription'] for ses_mod in data2]  # 从data2中提取出'transcription'字段的文本数据\n",
    "\n",
    "# 使用 torchtext 的 basic_english 分词器\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# 构建词汇表\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text), specials=[\"<unk>\"])  # 从文本数据中构建词汇表，并指定特殊标记\"<unk>\"\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # 设置默认索引为\"<unk>\"\n",
    "word_index = vocab.get_stoi()  # 获取词汇表中词语对应的索引\n",
    "print(f'Found {len(word_index)} unique tokens')  # 打印词汇表中唯一标记的数量\n",
    "\n",
    "file_loc = os.path.join('/kaggle/input/glove42b300dtxt/glove.42B.300d.txt')  # 指定GloVe文件的路径\n",
    "print(file_loc)  # 打印文件路径\n",
    "\n",
    "# 加载GloVe词嵌入\n",
    "gembeddings_index = {}\n",
    "with open(file_loc, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "print(f'G Word embeddings: {len(gembeddings_index)}')  # 打印GloVe词嵌入的数量\n",
    "\n",
    "# 创建词嵌入矩阵\n",
    "nb_words = len(word_index) + 1\n",
    "g_word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    gembedding_vector = gembeddings_index.get(word)\n",
    "    if gembedding_vector is not None:\n",
    "        g_word_embedding_matrix[i] = gembedding_vector\n",
    "\n",
    "print(f'G Null word embeddings: {np.sum(np.sum(g_word_embedding_matrix, axis=1) == 0)}')  # 打印空词嵌入的数量\n",
    "\n",
    "# 转换为 PyTorch 的 embedding layer 所需的 tensor\n",
    "g_word_embedding_matrix = torch.tensor(g_word_embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# 使用预训练的词嵌入初始化 embedding 层\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(g_word_embedding_matrix, freeze=False)\n",
    "\n",
    "# 打印 embedding 层的一些信息以验证\n",
    "print(f'Embedding layer weights shape: {embedding_layer.weight.shape}')  # 打印embedding层权重的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df0fcf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:17:43.036926Z",
     "iopub.status.busy": "2024-09-18T17:17:43.036615Z",
     "iopub.status.idle": "2024-09-18T17:17:43.041293Z",
     "shell.execute_reply": "2024-09-18T17:17:43.040608Z"
    },
    "papermill": {
     "duration": 0.024602,
     "end_time": "2024-09-18T17:17:43.043197",
     "exception": false,
     "start_time": "2024-09-18T17:17:43.018595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4936, 500])\n"
     ]
    }
   ],
   "source": [
    "print(x_train_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d841ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:17:43.079673Z",
     "iopub.status.busy": "2024-09-18T17:17:43.079404Z",
     "iopub.status.idle": "2024-09-18T17:17:43.089291Z",
     "shell.execute_reply": "2024-09-18T17:17:43.088439Z"
    },
    "papermill": {
     "duration": 0.030454,
     "end_time": "2024-09-18T17:17:43.091128",
     "exception": false,
     "start_time": "2024-09-18T17:17:43.060674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "import numpy as np  # 导入NumPy库\n",
    "\n",
    "# 假设 stFeatureExtraction 函数已经定义\n",
    "# 如果 stFeatureExtraction 函数使用的是 numpy，你可以将其转换为 torch 实现\n",
    "\n",
    "def calculate_features(frames, freq, options):\n",
    "    window_sec = 0.2  # 设置窗口长度为0.2秒\n",
    "    window_n = int(freq * window_sec)  # 计算窗口中的样本数\n",
    "    \n",
    "    # 调用 stFeatureExtraction 函数\n",
    "    st_f = stFeatureExtraction(frames, freq, window_n, window_n // 2)  # 使用 stFeatureExtraction 函数提取特征\n",
    "    st_f = torch.tensor(st_f, dtype=torch.float32)  # 将 numpy 数组转换为 PyTorch 张量\n",
    "    \n",
    "    if st_f.shape[1] > 2:  # 如果特征的维度大于2\n",
    "        i0 = 1\n",
    "        i1 = st_f.shape[1] - 1\n",
    "        if i1 - i0 < 1:\n",
    "            i1 = i0 + 1\n",
    "        \n",
    "        deriv_st_f = torch.zeros((st_f.shape[0], i1 - i0), dtype=torch.float32)  # 创建导数特征的张量\n",
    "        for i in range(i0, i1):\n",
    "            i_left = i - 1\n",
    "            i_right = i + 1\n",
    "            deriv_st_f[:, i - i0] = st_f[:, i]  # 将特征的某些部分复制到导数特征张量中\n",
    "        return deriv_st_f  # 返回导数特征张量\n",
    "    elif st_f.shape[1] == 2:  # 如果特征的维度等于2\n",
    "        deriv_st_f = torch.zeros((st_f.shape[0], 1), dtype=torch.float32)  # 创建导数特征的张量\n",
    "        deriv_st_f[:, 0] = st_f[:, 0]  # 将特征的某些部分复制到导数特征张量中\n",
    "        return deriv_st_f  # 返回导数特征张量\n",
    "    else:  # 如果特征的维度小于2\n",
    "        deriv_st_f = torch.zeros((st_f.shape[0], 1), dtype=torch.float32)  # 创建导数特征的张量\n",
    "        deriv_st_f[:, 0] = st_f[:, 0]  # 将特征的某些部分复制到导数特征张量中\n",
    "        return deriv_st_f  # 返回导数特征张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9621e9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:17:43.128366Z",
     "iopub.status.busy": "2024-09-18T17:17:43.128126Z",
     "iopub.status.idle": "2024-09-18T17:22:40.503231Z",
     "shell.execute_reply": "2024-09-18T17:22:40.501942Z"
    },
    "papermill": {
     "duration": 297.396814,
     "end_time": "2024-09-18T17:22:40.506481",
     "exception": false,
     "start_time": "2024-09-18T17:17:43.109667",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "torch.Size([4936, 34, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "import numpy as np  # 导入NumPy库\n",
    "\n",
    "# 定义函数，用于将序列填充到数组中\n",
    "def pad_sequence_into_array(sequences, maxlen):\n",
    "    num_samples = len(sequences)  # 获取序列的数量\n",
    "    if len(sequences) == 0:\n",
    "        return torch.zeros((num_samples, maxlen, 1), dtype=torch.float32), 0  # 如果序列为空，则返回一个全零张量\n",
    "    num_features = sequences[0].shape[0] if len(sequences[0].shape) > 1 else 1  # 获取特征的数量\n",
    "    padded_array = torch.zeros((num_samples, num_features, maxlen), dtype=torch.float32)  # 创建一个全零张量，用于存储填充后的序列\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq.shape) == 1:\n",
    "            seq = seq.reshape(1, -1)  # 如果序列的形状为一维，则将其调整为二维\n",
    "        length = min(maxlen, seq.shape[1])  # 获取序列的长度，并确保不超过最大长度\n",
    "        padded_array[i, :, :length] = seq[:, :length]  # 将序列填充到数组中\n",
    "    return padded_array, length  # 返回填充后的数组和实际长度\n",
    "\n",
    "# 处理语音数据并转换为 PyTorch 张量\n",
    "x_train_speech = []  # 存储处理后的语音数据\n",
    "counter = 0\n",
    "for ses_mod in data2:\n",
    "    x_head = ses_mod['signal']  # 获取语音信号数据\n",
    "    st_features = calculate_features(x_head, framerate, None)  # 计算语音信号的特征\n",
    "    st_features, _ = pad_sequence_into_array([st_features], maxlen=100)  # 将特征序列填充到数组中\n",
    "    x_train_speech.append(st_features)  # 将处理后的语音数据添加到列表中\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        print(counter)\n",
    "\n",
    "x_train_speech = torch.cat(x_train_speech, dim=0)  # 合并所有张量\n",
    "print(x_train_speech.shape)  # 打印处理后的语音数据的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba60e4ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:22:40.568927Z",
     "iopub.status.busy": "2024-09-18T17:22:40.568647Z",
     "iopub.status.idle": "2024-09-18T17:24:09.895905Z",
     "shell.execute_reply": "2024-09-18T17:24:09.895023Z"
    },
    "papermill": {
     "duration": 89.351375,
     "end_time": "2024-09-18T17:24:09.898260",
     "exception": false,
     "start_time": "2024-09-18T17:22:40.546885",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_mocap_data(data, expected_length=200, expected_features=189):\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    \n",
    "    processed_data = np.zeros((expected_length, expected_features))\n",
    "    \n",
    "    for i, row in enumerate(data):\n",
    "        if i >= expected_length:\n",
    "            break\n",
    "        if isinstance(row, (list, np.ndarray)):\n",
    "            if len(np.array(row).shape) > 1:\n",
    "                row = np.array(row).flatten()\n",
    "            row = np.nan_to_num(row, nan=0.0)  # 将NaN替换为0\n",
    "            processed_data[i, :min(len(row), expected_features)] = row[:expected_features]\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def normalize_mocap(data):\n",
    "    mean = np.nanmean(data, axis=(0, 1), keepdims=True)\n",
    "    std = np.nanstd(data, axis=(0, 1), keepdims=True)\n",
    "    return (data - mean) / (std + 1e-8)\n",
    "\n",
    "x_train_mocap = []\n",
    "for ses_mod in data2:\n",
    "    x_head = process_mocap_data(ses_mod['mocap_head'], expected_length=200, expected_features=18)\n",
    "    x_hand = process_mocap_data(ses_mod['mocap_hand'], expected_length=200, expected_features=6)\n",
    "    x_rot = process_mocap_data(ses_mod['mocap_rot'], expected_length=200, expected_features=165)\n",
    "\n",
    "    x_head = normalize_mocap(x_head)\n",
    "    x_hand = normalize_mocap(x_hand)\n",
    "    x_rot = normalize_mocap(x_rot)\n",
    "\n",
    "    x_mocap = np.concatenate((x_head, x_hand, x_rot), axis=1)\n",
    "    x_train_mocap.append(torch.tensor(x_mocap, dtype=torch.float32))\n",
    "\n",
    "x_train_mocap = torch.stack(x_train_mocap)\n",
    "x_train_mocap = x_train_mocap.view(-1, 200, 189, 1)\n",
    "\n",
    "# 检查并替换NaN值\n",
    "x_train_mocap = torch.nan_to_num(x_train_mocap, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab426a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:09.941894Z",
     "iopub.status.busy": "2024-09-18T17:24:09.941597Z",
     "iopub.status.idle": "2024-09-18T17:24:10.193763Z",
     "shell.execute_reply": "2024-09-18T17:24:10.192850Z"
    },
    "papermill": {
     "duration": 0.27604,
     "end_time": "2024-09-18T17:24:10.195899",
     "exception": false,
     "start_time": "2024-09-18T17:24:09.919859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_nan_mocap(data, name):\n",
    "    nan_mask = torch.isnan(data)\n",
    "    if nan_mask.any():\n",
    "        print(f\"{name} contains NaN values:\")\n",
    "        print(f\"Total NaN values: {nan_mask.sum().item()}\")\n",
    "        print(f\"Samples with NaN: {nan_mask.any(dim=(1,2,3)).sum().item()}/{data.shape[0]}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "check_nan_mocap(x_train_mocap, \"Mocap data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "670dc367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:10.239684Z",
     "iopub.status.busy": "2024-09-18T17:24:10.239383Z",
     "iopub.status.idle": "2024-09-18T17:24:10.475119Z",
     "shell.execute_reply": "2024-09-18T17:24:10.474188Z"
    },
    "papermill": {
     "duration": 0.260137,
     "end_time": "2024-09-18T17:24:10.477290",
     "exception": false,
     "start_time": "2024-09-18T17:24:10.217153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def interpolate_nan_mocap(data):\n",
    "    data_np = data.numpy()\n",
    "    for i in range(data_np.shape[0]):\n",
    "        for j in range(data_np.shape[2]):\n",
    "            data_np[i, :, j, 0] = pd.Series(data_np[i, :, j, 0]).interpolate().values\n",
    "    return torch.from_numpy(data_np)\n",
    "\n",
    "if check_nan_mocap(x_train_mocap, \"Mocap data\"):\n",
    "    x_train_mocap = interpolate_nan_mocap(x_train_mocap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2b79364",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:10.520671Z",
     "iopub.status.busy": "2024-09-18T17:24:10.520377Z",
     "iopub.status.idle": "2024-09-18T17:24:10.525226Z",
     "shell.execute_reply": "2024-09-18T17:24:10.524249Z"
    },
    "papermill": {
     "duration": 0.028762,
     "end_time": "2024-09-18T17:24:10.527212",
     "exception": false,
     "start_time": "2024-09-18T17:24:10.498450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4936, 200, 189, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train_mocap.shape)  # 打印结果检查维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feb2b5fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:10.570356Z",
     "iopub.status.busy": "2024-09-18T17:24:10.570115Z",
     "iopub.status.idle": "2024-09-18T17:24:10.584314Z",
     "shell.execute_reply": "2024-09-18T17:24:10.583444Z"
    },
    "papermill": {
     "duration": 0.038194,
     "end_time": "2024-09-18T17:24:10.586219",
     "exception": false,
     "start_time": "2024-09-18T17:24:10.548025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4936])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 初始化Y列表\n",
    "Y = []\n",
    "for ses_mod in data2:\n",
    "    Y.append(ses_mod['emotion'])\n",
    "\n",
    "# 使用LabelEncoder将标签编码为整数\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(Y)\n",
    "\n",
    "# 将Y转换为PyTorch张量，确保是长整型\n",
    "Y = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "# 打印Y的形状\n",
    "print(Y.shape)  # 应该是 torch.Size([950])or4936"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792a4ae",
   "metadata": {
    "papermill": {
     "duration": 0.02094,
     "end_time": "2024-09-18T17:24:10.628313",
     "exception": false,
     "start_time": "2024-09-18T17:24:10.607373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b600f4b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:10.672452Z",
     "iopub.status.busy": "2024-09-18T17:24:10.671598Z",
     "iopub.status.idle": "2024-09-18T17:24:10.773595Z",
     "shell.execute_reply": "2024-09-18T17:24:10.772814Z"
    },
    "papermill": {
     "duration": 0.126422,
     "end_time": "2024-09-18T17:24:10.775630",
     "exception": false,
     "start_time": "2024-09-18T17:24:10.649208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, text, speech, mocap, labels):\n",
    "        self.text = [t.clone().detach().long() for t in text]\n",
    "        self.speech = [s.clone().detach().float() for s in speech]\n",
    "        self.mocap = [m.clone().detach().float() for m in mocap]\n",
    "        self.labels = [l.clone().detach().long() for l in labels]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': self.text[idx],\n",
    "            'speech': self.speech[idx],\n",
    "            'mocap': self.mocap[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20508c3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:10.820885Z",
     "iopub.status.busy": "2024-09-18T17:24:10.820234Z",
     "iopub.status.idle": "2024-09-18T17:24:11.475943Z",
     "shell.execute_reply": "2024-09-18T17:24:11.475071Z"
    },
    "papermill": {
     "duration": 0.680432,
     "end_time": "2024-09-18T17:24:11.478224",
     "exception": false,
     "start_time": "2024-09-18T17:24:10.797792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 首先，将数据集分割为训练集和测试集\n",
    "X_text_train, X_text_test, X_speech_train, X_speech_test, X_mocap_train, X_mocap_test, y_train, y_test = train_test_split(\n",
    "    x_train_text, x_train_speech, x_train_mocap, Y, test_size=0.2, random_state=66, stratify=Y\n",
    ")\n",
    "\n",
    "# 将训练集的所有特征合并为一个二维数组\n",
    "X_combined_train = np.hstack((X_text_train.numpy(), \n",
    "                              X_speech_train.numpy().reshape(X_speech_train.shape[0], -1), \n",
    "                              X_mocap_train.numpy().reshape(X_mocap_train.shape[0], -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e801fd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:11.522572Z",
     "iopub.status.busy": "2024-09-18T17:24:11.522047Z",
     "iopub.status.idle": "2024-09-18T17:24:18.917979Z",
     "shell.execute_reply": "2024-09-18T17:24:18.917025Z"
    },
    "papermill": {
     "duration": 7.420155,
     "end_time": "2024-09-18T17:24:18.920057",
     "exception": false,
     "start_time": "2024-09-18T17:24:11.499902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重采样后的类别分布: Counter({3: 1366, 2: 1366, 0: 1366, 1: 1366})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# 假设 y_train 是训练集的标签\n",
    "\n",
    "# 首先，计算每个类别的样本数量\n",
    "class_counts = Counter(y_train.numpy())\n",
    "\n",
    "# 找出多数类的样本数量\n",
    "max_samples = max(class_counts.values())\n",
    "\n",
    "# 创建重采样策略字典，将所有少数类的样本数量增加到多数类的水平\n",
    "sampling_strategy = {cls: max_samples for cls in class_counts.keys() if class_counts[cls] < max_samples}\n",
    "\n",
    "# 应用SMOTE\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=66)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_combined_train, y_train.numpy())\n",
    "\n",
    "# 打印重采样后的类别分布\n",
    "print(\"重采样后的类别分布:\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d1716ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:18.964583Z",
     "iopub.status.busy": "2024-09-18T17:24:18.964137Z",
     "iopub.status.idle": "2024-09-18T17:24:20.496597Z",
     "shell.execute_reply": "2024-09-18T17:24:20.495752Z"
    },
    "papermill": {
     "duration": 1.557341,
     "end_time": "2024-09-18T17:24:20.499001",
     "exception": false,
     "start_time": "2024-09-18T17:24:18.941660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "num_workers = 4\n",
    "\n",
    "# 将重采样后的训练数据分割回原始形状\n",
    "text_shape = X_text_train.shape[1]\n",
    "speech_shape = X_speech_train.shape[1] * X_speech_train.shape[2]\n",
    "mocap_shape = X_mocap_train.shape[1] * X_mocap_train.shape[2] * X_mocap_train.shape[3]\n",
    "\n",
    "X_text_resampled = torch.tensor(X_resampled[:, :text_shape])\n",
    "X_speech_resampled = torch.tensor(X_resampled[:, text_shape:text_shape+speech_shape]).reshape(-1, X_speech_train.shape[1], X_speech_train.shape[2])\n",
    "X_mocap_resampled = torch.tensor(X_resampled[:, text_shape+speech_shape:]).reshape(-1, X_mocap_train.shape[1], X_mocap_train.shape[2], X_mocap_train.shape[3])\n",
    "y_resampled = torch.as_tensor(y_resampled).clone().detach()\n",
    "\n",
    "# 创建重采样后的训练集和原始测试集的数据集\n",
    "train_dataset = EmotionDataset(X_text_resampled, X_speech_resampled, X_mocap_resampled, y_resampled)\n",
    "test_dataset = EmotionDataset(X_text_test, X_speech_test, X_mocap_test, y_test)\n",
    "\n",
    "# 创建pred_dataset（使用测试集的前10个样本）\n",
    "pred_dataset = EmotionDataset(X_text_test[:10], X_speech_test[:10], X_mocap_test[:10], y_test[:10])\n",
    "\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=True)\n",
    "pred_loader = DataLoader(pred_dataset, batch_size=1, shuffle=False)  # 使用batch_size=1以便单独评估每个样本\n",
    "\n",
    "\n",
    "# 计算类别权重（使用重采样后的训练集）\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_resampled), y=y_resampled.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501c8ee",
   "metadata": {
    "papermill": {
     "duration": 0.021113,
     "end_time": "2024-09-18T17:24:20.543035",
     "exception": false,
     "start_time": "2024-09-18T17:24:20.521922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Function preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b675bf1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:20.586793Z",
     "iopub.status.busy": "2024-09-18T17:24:20.586483Z",
     "iopub.status.idle": "2024-09-18T17:24:20.606573Z",
     "shell.execute_reply": "2024-09-18T17:24:20.605743Z"
    },
    "papermill": {
     "duration": 0.04444,
     "end_time": "2024-09-18T17:24:20.608550",
     "exception": false,
     "start_time": "2024-09-18T17:24:20.564110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(outputs, labels):\n",
    "    preds = torch.max(outputs, dim=1)[1].cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    # F1 score(Weighted)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    \n",
    "    # Unweighted accuracy\n",
    "    uw_acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return f1, uw_acc\n",
    "\n",
    "# 模型验证\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    val_loss = 0\n",
    "    val_f1, val_uw_acc = 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            text = batch['text'].to(device)\n",
    "            speech = batch['speech'].to(device)\n",
    "            mocap = batch['mocap'].to(device)\n",
    "            labels = batch['labels'].to(device).long()\n",
    "            \n",
    "            outputs = model(text, speech, mocap)  # 计算模型输出\n",
    "            loss = criterion(outputs, labels)  # 计算损失函数\n",
    "            val_loss += loss.item()  # 累计损失\n",
    "            \n",
    "            f1, uw_acc = calculate_metrics(outputs, labels)\n",
    "            val_f1 += f1\n",
    "            val_uw_acc += uw_acc\n",
    "\n",
    "    val_loss /= len(val_loader)  # 计算平均损失\n",
    "    val_f1 /= len(val_loader)  # 计算平均F1分数\n",
    "    val_uw_acc /= len(val_loader)  # 计算平均非加权准确率\n",
    "\n",
    "    return val_loss, val_f1, val_uw_acc\n",
    "\n",
    "# 打印训练结果\n",
    "def print_log(epoch, train_time, train_loss, train_f1, train_uw_acc, \n",
    "              val_loss, val_f1, val_uw_acc, epochs=10):\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], time: {train_time:.2f}s\")\n",
    "    print(f\"Train - loss: {train_loss:.4f}, F1: {train_f1:.4f}, UW_Acc: {train_uw_acc:.4f}\")\n",
    "    print(f\"Val - loss: {val_loss:.4f}, F1: {val_f1:.4f}, UW_Acc: {val_uw_acc:.4f}\")\n",
    "\n",
    "# 模型训练\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
    "    train_losses = []\n",
    "    train_f1s, train_uw_accs = [], []\n",
    "    val_losses = []\n",
    "    val_f1s, val_uw_accs = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_f1, train_uw_acc = 0, 0\n",
    "        \n",
    "        start_time = time.time()  # 记录本epoch开始时间\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            text = batch['text'].to(device)\n",
    "            speech = batch['speech'].to(device)\n",
    "            mocap = batch['mocap'].to(device)\n",
    "            labels = batch['labels'].to(device).long()\n",
    "            \n",
    "            # 检查输入数据\n",
    "            if torch.isnan(text).any() or torch.isnan(speech).any() or torch.isnan(mocap).any():\n",
    "                print(\"Input data contains NaN!\")\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()  # 将模型所有参数tensor的梯度变为0\n",
    "            outputs = model(text, speech, mocap)  # 计算模型输出\n",
    "            \n",
    "            loss = criterion(outputs, labels)  # 计算损失函数\n",
    "            train_loss += loss.item()  # 累计损失\n",
    "            \n",
    "            f1, uw_acc = calculate_metrics(outputs, labels)\n",
    "            train_f1 += f1\n",
    "            train_uw_acc += uw_acc\n",
    "            \n",
    "            loss.backward()  # 反向传播计算梯度\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()  # 更新模型参数\n",
    "        \n",
    "        end_time = time.time()  # 记录本epoch结束时间\n",
    "        train_time = end_time - start_time  # 计算本epoch的训练耗时\n",
    "        \n",
    "        train_loss /= len(train_loader)  # 计算平均损失\n",
    "        train_f1 /= len(train_loader)  # 计算平均F1分数\n",
    "        train_uw_acc /= len(train_loader)  # 计算平均非加权准确率\n",
    "        \n",
    "        val_loss, val_f1, val_uw_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_f1)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_f1s.append(train_f1)\n",
    "        train_uw_accs.append(train_uw_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_f1s.append(val_f1)\n",
    "        val_uw_accs.append(val_uw_acc)\n",
    "        \n",
    "        print_log(epoch, train_time, train_loss, train_f1, train_uw_acc, \n",
    "                  val_loss, val_f1, val_uw_acc, epochs=num_epochs)  # 打印训练结果\n",
    "\n",
    "    return train_losses, train_f1s, train_uw_accs, val_losses, val_f1s, val_uw_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81688f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:20.651925Z",
     "iopub.status.busy": "2024-09-18T17:24:20.651640Z",
     "iopub.status.idle": "2024-09-18T17:24:20.656355Z",
     "shell.execute_reply": "2024-09-18T17:24:20.655541Z"
    },
    "papermill": {
     "duration": 0.028627,
     "end_time": "2024-09-18T17:24:20.658247",
     "exception": false,
     "start_time": "2024-09-18T17:24:20.629620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    val_loss, f1, uw_acc = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"Loss: {val_loss:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Unweighted Accuracy: {uw_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cccff74a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:20.701646Z",
     "iopub.status.busy": "2024-09-18T17:24:20.701195Z",
     "iopub.status.idle": "2024-09-18T17:24:20.708658Z",
     "shell.execute_reply": "2024-09-18T17:24:20.707850Z"
    },
    "papermill": {
     "duration": 0.0313,
     "end_time": "2024-09-18T17:24:20.710561",
     "exception": false,
     "start_time": "2024-09-18T17:24:20.679261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_model_summary(model, input_size):\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model Structure:\\n{model}\\n\")\n",
    "    print(f\"Input size: {input_size}\")\n",
    "    \n",
    "    total_params = 0\n",
    "    for name, module in model.named_children():\n",
    "        params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "        total_params += params\n",
    "        print(f\"{name}: {params:,} parameters\")\n",
    "        \n",
    "        if hasattr(module, 'named_children'):\n",
    "            for sub_name, sub_module in module.named_children():\n",
    "                sub_params = sum(p.numel() for p in sub_module.parameters() if p.requires_grad)\n",
    "                print(f\"  {sub_name}: {sub_params:,} parameters\")\n",
    "    \n",
    "    print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
    "    \n",
    "    print(\"\\nDetailed parameter shapes:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63c0608d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:20.753911Z",
     "iopub.status.busy": "2024-09-18T17:24:20.753634Z",
     "iopub.status.idle": "2024-09-18T17:24:20.757484Z",
     "shell.execute_reply": "2024-09-18T17:24:20.756676Z"
    },
    "papermill": {
     "duration": 0.02777,
     "end_time": "2024-09-18T17:24:20.759274",
     "exception": false,
     "start_time": "2024-09-18T17:24:20.731504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 参数初始化\n",
    "nb_words = 3130 # 3130 or 1419\n",
    "embedding_dim = 300\n",
    "max_sequence_length = 500  # 最大序列长度\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3f2b16",
   "metadata": {
    "papermill": {
     "duration": 0.021131,
     "end_time": "2024-09-18T17:24:20.801996",
     "exception": false,
     "start_time": "2024-09-18T17:24:20.780865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18087041",
   "metadata": {
    "papermill": {
     "duration": 0.020943,
     "end_time": "2024-09-18T17:24:20.844505",
     "exception": false,
     "start_time": "2024-09-18T17:24:20.823562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model-5: Multi-attention Text+speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6c3960d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:20.888996Z",
     "iopub.status.busy": "2024-09-18T17:24:20.888682Z",
     "iopub.status.idle": "2024-09-18T17:24:20.920633Z",
     "shell.execute_reply": "2024-09-18T17:24:20.919816Z"
    },
    "papermill": {
     "duration": 0.056817,
     "end_time": "2024-09-18T17:24:20.922768",
     "exception": false,
     "start_time": "2024-09-18T17:24:20.865951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_dim\n",
    "        ), \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = FeedForward(embed_dim, 4 * embed_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, embed_dim = x.shape\n",
    "\n",
    "        queries = self.query(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "        keys = self.key(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "        values = self.value(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        energy = torch.matmul(queries, keys.transpose(-1, -2)) / (self.head_dim ** (1 / 2))\n",
    "        attention = self.dropout(torch.softmax(energy, dim=-1))  # 添加dropout\n",
    "\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.transpose(1, 2).contiguous().view(N, seq_length, self.embed_dim)\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        out = self.dropout(out)  # 添加dropout\n",
    "        out = self.layer_norm1(out + x)\n",
    "        ff_out = self.feed_forward(out)\n",
    "        out = self.layer_norm2(ff_out + out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class TextModel(nn.Module):\n",
    "    def __init__(self, nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads=num_heads):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(nb_words, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(g_word_embedding_matrix.clone().detach())\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.conv2 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim=32, num_heads=num_heads)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(32 * max_sequence_length, 256)\n",
    "        self.bn_dense = nn.BatchNorm1d(256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.bn_dense(self.dense(x))\n",
    "        return x\n",
    "\n",
    "class SpeechModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpeechModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(100 * 34, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dense2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.dense3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.bn1(self.dense1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.dense2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn3(self.dense3(x))\n",
    "        return x\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads=num_heads):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.text_model = TextModel(nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads)\n",
    "        self.speech_model = SpeechModel()\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim=512, num_heads=num_heads)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 4)\n",
    "    \n",
    "    def forward(self, text, speech, mocap=None):\n",
    "        text_out = self.text_model(text)\n",
    "        speech_out = self.speech_model(speech)\n",
    "        combined = torch.cat((text_out, speech_out), dim=1)\n",
    "        combined = self.attention(combined.unsqueeze(1)).squeeze(1)\n",
    "        combined = F.relu(self.bn1(self.fc1(combined)))\n",
    "        combined = self.fc2(combined)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceb254a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:20.967374Z",
     "iopub.status.busy": "2024-09-18T17:24:20.967102Z",
     "iopub.status.idle": "2024-09-18T17:24:21.349862Z",
     "shell.execute_reply": "2024-09-18T17:24:21.348991Z"
    },
    "papermill": {
     "duration": 0.407695,
     "end_time": "2024-09-18T17:24:21.352235",
     "exception": false,
     "start_time": "2024-09-18T17:24:20.944540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CombinedModel(\n",
      "  (text_model): TextModel(\n",
      "    (embedding): Embedding(3130, 300)\n",
      "    (conv1): Conv1d(300, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadSelfAttention(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=16000, out_features=256, bias=True)\n",
      "    (bn_dense): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (speech_model): SpeechModel(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=3400, out_features=1024, bias=True)\n",
      "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dense2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dense3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (attention): MultiHeadSelfAttention(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (feed_forward): FeedForward(\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "Combined Model Summary:\n",
      "Model Structure:\n",
      "CombinedModel(\n",
      "  (text_model): TextModel(\n",
      "    (embedding): Embedding(3130, 300)\n",
      "    (conv1): Conv1d(300, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadSelfAttention(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=16000, out_features=256, bias=True)\n",
      "    (bn_dense): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (speech_model): SpeechModel(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=3400, out_features=1024, bias=True)\n",
      "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dense2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dense3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (attention): MultiHeadSelfAttention(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (feed_forward): FeedForward(\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "Input size: [(500,), (100, 34), (200, 189, 1)]\n",
      "text_model: 5,409,336 parameters\n",
      "  embedding: 939,000 parameters\n",
      "  conv1: 230,656 parameters\n",
      "  bn1: 512 parameters\n",
      "  conv2: 98,432 parameters\n",
      "  bn2: 256 parameters\n",
      "  conv3: 24,640 parameters\n",
      "  bn3: 128 parameters\n",
      "  conv4: 6,176 parameters\n",
      "  bn4: 64 parameters\n",
      "  attention: 12,704 parameters\n",
      "  dropout: 0 parameters\n",
      "  flatten: 0 parameters\n",
      "  dense: 4,096,256 parameters\n",
      "  bn_dense: 512 parameters\n",
      "speech_model: 4,142,336 parameters\n",
      "  flatten: 0 parameters\n",
      "  dense1: 3,482,624 parameters\n",
      "  bn1: 2,048 parameters\n",
      "  dense2: 524,800 parameters\n",
      "  bn2: 1,024 parameters\n",
      "  dense3: 131,328 parameters\n",
      "  bn3: 512 parameters\n",
      "  dropout: 0 parameters\n",
      "attention: 3,152,384 parameters\n",
      "  dropout: 0 parameters\n",
      "  query: 262,656 parameters\n",
      "  key: 262,656 parameters\n",
      "  value: 262,656 parameters\n",
      "  fc_out: 262,656 parameters\n",
      "  layer_norm1: 1,024 parameters\n",
      "  layer_norm2: 1,024 parameters\n",
      "  feed_forward: 2,099,712 parameters\n",
      "fc1: 131,328 parameters\n",
      "bn1: 512 parameters\n",
      "fc2: 1,028 parameters\n",
      "\n",
      "Total trainable parameters: 12,836,924\n",
      "\n",
      "Detailed parameter shapes:\n",
      "text_model.embedding.weight: torch.Size([3130, 300])\n",
      "text_model.conv1.weight: torch.Size([256, 300, 3])\n",
      "text_model.conv1.bias: torch.Size([256])\n",
      "text_model.bn1.weight: torch.Size([256])\n",
      "text_model.bn1.bias: torch.Size([256])\n",
      "text_model.conv2.weight: torch.Size([128, 256, 3])\n",
      "text_model.conv2.bias: torch.Size([128])\n",
      "text_model.bn2.weight: torch.Size([128])\n",
      "text_model.bn2.bias: torch.Size([128])\n",
      "text_model.conv3.weight: torch.Size([64, 128, 3])\n",
      "text_model.conv3.bias: torch.Size([64])\n",
      "text_model.bn3.weight: torch.Size([64])\n",
      "text_model.bn3.bias: torch.Size([64])\n",
      "text_model.conv4.weight: torch.Size([32, 64, 3])\n",
      "text_model.conv4.bias: torch.Size([32])\n",
      "text_model.bn4.weight: torch.Size([32])\n",
      "text_model.bn4.bias: torch.Size([32])\n",
      "text_model.attention.query.weight: torch.Size([32, 32])\n",
      "text_model.attention.query.bias: torch.Size([32])\n",
      "text_model.attention.key.weight: torch.Size([32, 32])\n",
      "text_model.attention.key.bias: torch.Size([32])\n",
      "text_model.attention.value.weight: torch.Size([32, 32])\n",
      "text_model.attention.value.bias: torch.Size([32])\n",
      "text_model.attention.fc_out.weight: torch.Size([32, 32])\n",
      "text_model.attention.fc_out.bias: torch.Size([32])\n",
      "text_model.attention.layer_norm1.weight: torch.Size([32])\n",
      "text_model.attention.layer_norm1.bias: torch.Size([32])\n",
      "text_model.attention.layer_norm2.weight: torch.Size([32])\n",
      "text_model.attention.layer_norm2.bias: torch.Size([32])\n",
      "text_model.attention.feed_forward.linear1.weight: torch.Size([128, 32])\n",
      "text_model.attention.feed_forward.linear1.bias: torch.Size([128])\n",
      "text_model.attention.feed_forward.linear2.weight: torch.Size([32, 128])\n",
      "text_model.attention.feed_forward.linear2.bias: torch.Size([32])\n",
      "text_model.dense.weight: torch.Size([256, 16000])\n",
      "text_model.dense.bias: torch.Size([256])\n",
      "text_model.bn_dense.weight: torch.Size([256])\n",
      "text_model.bn_dense.bias: torch.Size([256])\n",
      "speech_model.dense1.weight: torch.Size([1024, 3400])\n",
      "speech_model.dense1.bias: torch.Size([1024])\n",
      "speech_model.bn1.weight: torch.Size([1024])\n",
      "speech_model.bn1.bias: torch.Size([1024])\n",
      "speech_model.dense2.weight: torch.Size([512, 1024])\n",
      "speech_model.dense2.bias: torch.Size([512])\n",
      "speech_model.bn2.weight: torch.Size([512])\n",
      "speech_model.bn2.bias: torch.Size([512])\n",
      "speech_model.dense3.weight: torch.Size([256, 512])\n",
      "speech_model.dense3.bias: torch.Size([256])\n",
      "speech_model.bn3.weight: torch.Size([256])\n",
      "speech_model.bn3.bias: torch.Size([256])\n",
      "attention.query.weight: torch.Size([512, 512])\n",
      "attention.query.bias: torch.Size([512])\n",
      "attention.key.weight: torch.Size([512, 512])\n",
      "attention.key.bias: torch.Size([512])\n",
      "attention.value.weight: torch.Size([512, 512])\n",
      "attention.value.bias: torch.Size([512])\n",
      "attention.fc_out.weight: torch.Size([512, 512])\n",
      "attention.fc_out.bias: torch.Size([512])\n",
      "attention.layer_norm1.weight: torch.Size([512])\n",
      "attention.layer_norm1.bias: torch.Size([512])\n",
      "attention.layer_norm2.weight: torch.Size([512])\n",
      "attention.layer_norm2.bias: torch.Size([512])\n",
      "attention.feed_forward.linear1.weight: torch.Size([2048, 512])\n",
      "attention.feed_forward.linear1.bias: torch.Size([2048])\n",
      "attention.feed_forward.linear2.weight: torch.Size([512, 2048])\n",
      "attention.feed_forward.linear2.bias: torch.Size([512])\n",
      "fc1.weight: torch.Size([256, 512])\n",
      "fc1.bias: torch.Size([256])\n",
      "bn1.weight: torch.Size([256])\n",
      "bn1.bias: torch.Size([256])\n",
      "fc2.weight: torch.Size([4, 256])\n",
      "fc2.bias: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# 设置设备为GPU或CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 检查 class_weights 是否已经在正确的设备上\n",
    "if not isinstance(class_weights, torch.Tensor):\n",
    "    class_weights = torch.FloatTensor(class_weights)\n",
    "\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# 实例化模型、损失函数和优化器\n",
    "model_combined = CombinedModel(nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix)\n",
    "\n",
    "# 打印模型摘要\n",
    "print(model_combined)\n",
    "\n",
    "print(\"Combined Model Summary:\")\n",
    "print_model_summary(model_combined, [(max_sequence_length,), (100, 34), (200, 189, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef9e73fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:21.397028Z",
     "iopub.status.busy": "2024-09-18T17:24:21.396702Z",
     "iopub.status.idle": "2024-09-18T17:24:24.860739Z",
     "shell.execute_reply": "2024-09-18T17:24:24.859636Z"
    },
    "papermill": {
     "duration": 3.488578,
     "end_time": "2024-09-18T17:24:24.862906",
     "exception": false,
     "start_time": "2024-09-18T17:24:21.374328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "torch.Size([128, 4])\n"
     ]
    }
   ],
   "source": [
    "# 使用DataParallel包装模型\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model_combined = nn.DataParallel(model_combined)\n",
    "\n",
    "# 将模型移动到设备上\n",
    "model_combined.to(device)\n",
    "\n",
    "# 假设我们有一个batch\n",
    "batch = next(iter(train_loader))\n",
    "text = batch['text'].to(device).long()\n",
    "speech = batch['speech'].to(device)\n",
    "mocap = batch['mocap'].to(device)\n",
    "\n",
    "# 前向传播\n",
    "outputs = model_combined(text, speech, mocap)\n",
    "\n",
    "# 打印输出形状\n",
    "print(outputs.shape)  # 打印模型输出的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "982c1638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:24:24.909486Z",
     "iopub.status.busy": "2024-09-18T17:24:24.909153Z",
     "iopub.status.idle": "2024-09-18T17:34:09.951264Z",
     "shell.execute_reply": "2024-09-18T17:34:09.950282Z"
    },
    "papermill": {
     "duration": 585.095377,
     "end_time": "2024-09-18T17:34:09.981130",
     "exception": false,
     "start_time": "2024-09-18T17:24:24.885753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], time: 6.92s\n",
      "Train - loss: 1.2308, F1: 0.4019, UW_Acc: 0.4150\n",
      "Val - loss: 1.2428, F1: 0.3883, UW_Acc: 0.4267\n",
      "Epoch [2/80], time: 5.74s\n",
      "Train - loss: 0.9752, F1: 0.5737, UW_Acc: 0.5772\n",
      "Val - loss: 0.9938, F1: 0.5650, UW_Acc: 0.5697\n",
      "Epoch [3/80], time: 5.77s\n",
      "Train - loss: 0.8241, F1: 0.6621, UW_Acc: 0.6621\n",
      "Val - loss: 0.9848, F1: 0.5862, UW_Acc: 0.5923\n",
      "Epoch [4/80], time: 5.84s\n",
      "Train - loss: 0.7002, F1: 0.7193, UW_Acc: 0.7185\n",
      "Val - loss: 1.0538, F1: 0.5584, UW_Acc: 0.5845\n",
      "Epoch [5/80], time: 5.82s\n",
      "Train - loss: 0.6003, F1: 0.7676, UW_Acc: 0.7675\n",
      "Val - loss: 0.9236, F1: 0.6536, UW_Acc: 0.6540\n",
      "Epoch [6/80], time: 5.82s\n",
      "Train - loss: 0.5133, F1: 0.8013, UW_Acc: 0.8010\n",
      "Val - loss: 1.0607, F1: 0.6042, UW_Acc: 0.6081\n",
      "Epoch [7/80], time: 5.84s\n",
      "Train - loss: 0.4510, F1: 0.8277, UW_Acc: 0.8274\n",
      "Val - loss: 1.0844, F1: 0.6067, UW_Acc: 0.6134\n",
      "Epoch [8/80], time: 5.84s\n",
      "Train - loss: 0.3922, F1: 0.8533, UW_Acc: 0.8526\n",
      "Val - loss: 1.1765, F1: 0.5953, UW_Acc: 0.6068\n",
      "Epoch [9/80], time: 5.80s\n",
      "Train - loss: 0.3517, F1: 0.8722, UW_Acc: 0.8716\n",
      "Val - loss: 1.1973, F1: 0.6094, UW_Acc: 0.6134\n",
      "Epoch [10/80], time: 5.86s\n",
      "Train - loss: 0.3172, F1: 0.8797, UW_Acc: 0.8793\n",
      "Val - loss: 1.2464, F1: 0.6009, UW_Acc: 0.6085\n",
      "Epoch 00011: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch [11/80], time: 5.91s\n",
      "Train - loss: 0.2971, F1: 0.8893, UW_Acc: 0.8892\n",
      "Val - loss: 1.1158, F1: 0.6537, UW_Acc: 0.6593\n",
      "Epoch [12/80], time: 5.87s\n",
      "Train - loss: 0.2346, F1: 0.9152, UW_Acc: 0.9149\n",
      "Val - loss: 1.0308, F1: 0.6638, UW_Acc: 0.6649\n",
      "Epoch [13/80], time: 5.79s\n",
      "Train - loss: 0.2058, F1: 0.9269, UW_Acc: 0.9267\n",
      "Val - loss: 1.0345, F1: 0.6736, UW_Acc: 0.6757\n",
      "Epoch [14/80], time: 5.80s\n",
      "Train - loss: 0.1821, F1: 0.9388, UW_Acc: 0.9386\n",
      "Val - loss: 1.0696, F1: 0.6755, UW_Acc: 0.6753\n",
      "Epoch [15/80], time: 5.86s\n",
      "Train - loss: 0.1653, F1: 0.9407, UW_Acc: 0.9404\n",
      "Val - loss: 1.0799, F1: 0.6766, UW_Acc: 0.6790\n",
      "Epoch [16/80], time: 5.89s\n",
      "Train - loss: 0.1757, F1: 0.9370, UW_Acc: 0.9369\n",
      "Val - loss: 1.0851, F1: 0.6820, UW_Acc: 0.6829\n",
      "Epoch [17/80], time: 5.86s\n",
      "Train - loss: 0.1639, F1: 0.9415, UW_Acc: 0.9412\n",
      "Val - loss: 1.0803, F1: 0.6838, UW_Acc: 0.6842\n",
      "Epoch [18/80], time: 5.87s\n",
      "Train - loss: 0.1607, F1: 0.9446, UW_Acc: 0.9445\n",
      "Val - loss: 1.2143, F1: 0.6463, UW_Acc: 0.6493\n",
      "Epoch [19/80], time: 5.96s\n",
      "Train - loss: 0.1507, F1: 0.9460, UW_Acc: 0.9458\n",
      "Val - loss: 1.1928, F1: 0.6530, UW_Acc: 0.6546\n",
      "Epoch [20/80], time: 5.91s\n",
      "Train - loss: 0.1472, F1: 0.9494, UW_Acc: 0.9493\n",
      "Val - loss: 1.2051, F1: 0.6523, UW_Acc: 0.6534\n",
      "Epoch [21/80], time: 5.91s\n",
      "Train - loss: 0.1322, F1: 0.9570, UW_Acc: 0.9568\n",
      "Val - loss: 1.1866, F1: 0.6730, UW_Acc: 0.6747\n",
      "Epoch [22/80], time: 5.94s\n",
      "Train - loss: 0.1288, F1: 0.9555, UW_Acc: 0.9553\n",
      "Val - loss: 1.2896, F1: 0.6418, UW_Acc: 0.6436\n",
      "Epoch 00023: reducing learning rate of group 0 to 9.0000e-06.\n",
      "Epoch [23/80], time: 6.05s\n",
      "Train - loss: 0.1360, F1: 0.9531, UW_Acc: 0.9530\n",
      "Val - loss: 1.2036, F1: 0.6660, UW_Acc: 0.6669\n",
      "Epoch [24/80], time: 5.93s\n",
      "Train - loss: 0.1181, F1: 0.9612, UW_Acc: 0.9611\n",
      "Val - loss: 1.1901, F1: 0.6774, UW_Acc: 0.6780\n",
      "Epoch [25/80], time: 5.92s\n",
      "Train - loss: 0.1117, F1: 0.9615, UW_Acc: 0.9614\n",
      "Val - loss: 1.1866, F1: 0.6761, UW_Acc: 0.6768\n",
      "Epoch [26/80], time: 5.91s\n",
      "Train - loss: 0.1067, F1: 0.9629, UW_Acc: 0.9629\n",
      "Val - loss: 1.2047, F1: 0.6706, UW_Acc: 0.6709\n",
      "Epoch [27/80], time: 5.90s\n",
      "Train - loss: 0.1127, F1: 0.9602, UW_Acc: 0.9600\n",
      "Val - loss: 1.1859, F1: 0.6784, UW_Acc: 0.6786\n",
      "Epoch [28/80], time: 5.92s\n",
      "Train - loss: 0.1100, F1: 0.9619, UW_Acc: 0.9618\n",
      "Val - loss: 1.1809, F1: 0.6765, UW_Acc: 0.6772\n",
      "Epoch 00029: reducing learning rate of group 0 to 2.7000e-06.\n",
      "Epoch [29/80], time: 5.94s\n",
      "Train - loss: 0.1061, F1: 0.9649, UW_Acc: 0.9649\n",
      "Val - loss: 1.2273, F1: 0.6645, UW_Acc: 0.6653\n",
      "Epoch [30/80], time: 5.93s\n",
      "Train - loss: 0.1012, F1: 0.9679, UW_Acc: 0.9677\n",
      "Val - loss: 1.2065, F1: 0.6714, UW_Acc: 0.6717\n",
      "Epoch [31/80], time: 5.92s\n",
      "Train - loss: 0.1036, F1: 0.9667, UW_Acc: 0.9666\n",
      "Val - loss: 1.1891, F1: 0.6793, UW_Acc: 0.6803\n",
      "Epoch [32/80], time: 5.94s\n",
      "Train - loss: 0.0917, F1: 0.9714, UW_Acc: 0.9712\n",
      "Val - loss: 1.2406, F1: 0.6674, UW_Acc: 0.6678\n",
      "Epoch [33/80], time: 5.91s\n",
      "Train - loss: 0.0966, F1: 0.9682, UW_Acc: 0.9682\n",
      "Val - loss: 1.2189, F1: 0.6724, UW_Acc: 0.6731\n",
      "Epoch [34/80], time: 6.00s\n",
      "Train - loss: 0.0984, F1: 0.9693, UW_Acc: 0.9691\n",
      "Val - loss: 1.2201, F1: 0.6737, UW_Acc: 0.6743\n",
      "Epoch 00035: reducing learning rate of group 0 to 8.1000e-07.\n",
      "Epoch [35/80], time: 5.92s\n",
      "Train - loss: 0.0969, F1: 0.9680, UW_Acc: 0.9680\n",
      "Val - loss: 1.2241, F1: 0.6749, UW_Acc: 0.6757\n",
      "Epoch [36/80], time: 5.97s\n",
      "Train - loss: 0.0994, F1: 0.9677, UW_Acc: 0.9676\n",
      "Val - loss: 1.2291, F1: 0.6760, UW_Acc: 0.6766\n",
      "Epoch [37/80], time: 5.91s\n",
      "Train - loss: 0.0946, F1: 0.9668, UW_Acc: 0.9666\n",
      "Val - loss: 1.2217, F1: 0.6751, UW_Acc: 0.6757\n",
      "Epoch [38/80], time: 5.95s\n",
      "Train - loss: 0.0906, F1: 0.9690, UW_Acc: 0.9689\n",
      "Val - loss: 1.2286, F1: 0.6758, UW_Acc: 0.6766\n",
      "Epoch [39/80], time: 5.93s\n",
      "Train - loss: 0.0961, F1: 0.9664, UW_Acc: 0.9663\n",
      "Val - loss: 1.2292, F1: 0.6714, UW_Acc: 0.6723\n",
      "Epoch [40/80], time: 5.97s\n",
      "Train - loss: 0.0891, F1: 0.9725, UW_Acc: 0.9724\n",
      "Val - loss: 1.2223, F1: 0.6730, UW_Acc: 0.6737\n",
      "Epoch 00041: reducing learning rate of group 0 to 2.4300e-07.\n",
      "Epoch [41/80], time: 5.97s\n",
      "Train - loss: 0.1046, F1: 0.9626, UW_Acc: 0.9625\n",
      "Val - loss: 1.2409, F1: 0.6678, UW_Acc: 0.6678\n",
      "Epoch [42/80], time: 5.91s\n",
      "Train - loss: 0.0846, F1: 0.9726, UW_Acc: 0.9726\n",
      "Val - loss: 1.2432, F1: 0.6647, UW_Acc: 0.6649\n",
      "Epoch [43/80], time: 5.90s\n",
      "Train - loss: 0.0915, F1: 0.9694, UW_Acc: 0.9694\n",
      "Val - loss: 1.2305, F1: 0.6737, UW_Acc: 0.6747\n",
      "Epoch [44/80], time: 5.93s\n",
      "Train - loss: 0.0939, F1: 0.9673, UW_Acc: 0.9672\n",
      "Val - loss: 1.2542, F1: 0.6621, UW_Acc: 0.6626\n",
      "Epoch [45/80], time: 5.98s\n",
      "Train - loss: 0.0883, F1: 0.9715, UW_Acc: 0.9714\n",
      "Val - loss: 1.2186, F1: 0.6807, UW_Acc: 0.6815\n",
      "Epoch [46/80], time: 5.91s\n",
      "Train - loss: 0.0873, F1: 0.9721, UW_Acc: 0.9721\n",
      "Val - loss: 1.2424, F1: 0.6633, UW_Acc: 0.6636\n",
      "Epoch 00047: reducing learning rate of group 0 to 7.2900e-08.\n",
      "Epoch [47/80], time: 5.88s\n",
      "Train - loss: 0.0978, F1: 0.9670, UW_Acc: 0.9669\n",
      "Val - loss: 1.2230, F1: 0.6746, UW_Acc: 0.6753\n",
      "Epoch [48/80], time: 5.92s\n",
      "Train - loss: 0.0970, F1: 0.9691, UW_Acc: 0.9690\n",
      "Val - loss: 1.2286, F1: 0.6783, UW_Acc: 0.6786\n",
      "Epoch [49/80], time: 6.05s\n",
      "Train - loss: 0.0925, F1: 0.9696, UW_Acc: 0.9695\n",
      "Val - loss: 1.2303, F1: 0.6713, UW_Acc: 0.6717\n",
      "Epoch [50/80], time: 5.86s\n",
      "Train - loss: 0.0960, F1: 0.9701, UW_Acc: 0.9698\n",
      "Val - loss: 1.2323, F1: 0.6697, UW_Acc: 0.6704\n",
      "Epoch [51/80], time: 5.95s\n",
      "Train - loss: 0.0888, F1: 0.9735, UW_Acc: 0.9734\n",
      "Val - loss: 1.2226, F1: 0.6774, UW_Acc: 0.6780\n",
      "Epoch [52/80], time: 5.90s\n",
      "Train - loss: 0.0968, F1: 0.9674, UW_Acc: 0.9674\n",
      "Val - loss: 1.2230, F1: 0.6700, UW_Acc: 0.6704\n",
      "Epoch 00053: reducing learning rate of group 0 to 2.1870e-08.\n",
      "Epoch [53/80], time: 5.93s\n",
      "Train - loss: 0.0944, F1: 0.9690, UW_Acc: 0.9689\n",
      "Val - loss: 1.2227, F1: 0.6751, UW_Acc: 0.6757\n",
      "Epoch [54/80], time: 6.01s\n",
      "Train - loss: 0.0865, F1: 0.9722, UW_Acc: 0.9722\n",
      "Val - loss: 1.2201, F1: 0.6778, UW_Acc: 0.6786\n",
      "Epoch [55/80], time: 5.93s\n",
      "Train - loss: 0.0920, F1: 0.9692, UW_Acc: 0.9691\n",
      "Val - loss: 1.2330, F1: 0.6695, UW_Acc: 0.6698\n",
      "Epoch [56/80], time: 5.91s\n",
      "Train - loss: 0.0916, F1: 0.9720, UW_Acc: 0.9719\n",
      "Val - loss: 1.2289, F1: 0.6717, UW_Acc: 0.6721\n",
      "Epoch [57/80], time: 5.91s\n",
      "Train - loss: 0.0890, F1: 0.9715, UW_Acc: 0.9714\n",
      "Val - loss: 1.2314, F1: 0.6667, UW_Acc: 0.6672\n",
      "Epoch [58/80], time: 5.97s\n",
      "Train - loss: 0.0971, F1: 0.9652, UW_Acc: 0.9652\n",
      "Val - loss: 1.2417, F1: 0.6682, UW_Acc: 0.6684\n",
      "Epoch 00059: reducing learning rate of group 0 to 6.5610e-09.\n",
      "Epoch [59/80], time: 5.89s\n",
      "Train - loss: 0.0936, F1: 0.9700, UW_Acc: 0.9699\n",
      "Val - loss: 1.2396, F1: 0.6697, UW_Acc: 0.6702\n",
      "Epoch [60/80], time: 5.94s\n",
      "Train - loss: 0.0975, F1: 0.9674, UW_Acc: 0.9674\n",
      "Val - loss: 1.2317, F1: 0.6707, UW_Acc: 0.6714\n",
      "Epoch [61/80], time: 5.89s\n",
      "Train - loss: 0.0955, F1: 0.9700, UW_Acc: 0.9700\n",
      "Val - loss: 1.2275, F1: 0.6712, UW_Acc: 0.6717\n",
      "Epoch [62/80], time: 5.96s\n",
      "Train - loss: 0.0867, F1: 0.9728, UW_Acc: 0.9727\n",
      "Val - loss: 1.2425, F1: 0.6738, UW_Acc: 0.6747\n",
      "Epoch [63/80], time: 5.90s\n",
      "Train - loss: 0.0904, F1: 0.9704, UW_Acc: 0.9703\n",
      "Val - loss: 1.2321, F1: 0.6653, UW_Acc: 0.6655\n",
      "Epoch [64/80], time: 5.89s\n",
      "Train - loss: 0.0958, F1: 0.9679, UW_Acc: 0.9678\n",
      "Val - loss: 1.2167, F1: 0.6802, UW_Acc: 0.6805\n",
      "Epoch [65/80], time: 5.88s\n",
      "Train - loss: 0.0911, F1: 0.9710, UW_Acc: 0.9710\n",
      "Val - loss: 1.2224, F1: 0.6708, UW_Acc: 0.6714\n",
      "Epoch [66/80], time: 5.89s\n",
      "Train - loss: 0.0930, F1: 0.9696, UW_Acc: 0.9695\n",
      "Val - loss: 1.2321, F1: 0.6675, UW_Acc: 0.6678\n",
      "Epoch [67/80], time: 5.99s\n",
      "Train - loss: 0.0918, F1: 0.9687, UW_Acc: 0.9685\n",
      "Val - loss: 1.2264, F1: 0.6743, UW_Acc: 0.6747\n",
      "Epoch [68/80], time: 5.91s\n",
      "Train - loss: 0.0952, F1: 0.9690, UW_Acc: 0.9688\n",
      "Val - loss: 1.2320, F1: 0.6653, UW_Acc: 0.6659\n",
      "Epoch [69/80], time: 5.89s\n",
      "Train - loss: 0.0922, F1: 0.9704, UW_Acc: 0.9703\n",
      "Val - loss: 1.2316, F1: 0.6725, UW_Acc: 0.6727\n",
      "Epoch [70/80], time: 5.94s\n",
      "Train - loss: 0.0900, F1: 0.9733, UW_Acc: 0.9732\n",
      "Val - loss: 1.2301, F1: 0.6707, UW_Acc: 0.6712\n",
      "Epoch [71/80], time: 5.96s\n",
      "Train - loss: 0.0866, F1: 0.9734, UW_Acc: 0.9733\n",
      "Val - loss: 1.2287, F1: 0.6732, UW_Acc: 0.6737\n",
      "Epoch [72/80], time: 5.93s\n",
      "Train - loss: 0.0920, F1: 0.9712, UW_Acc: 0.9712\n",
      "Val - loss: 1.2311, F1: 0.6752, UW_Acc: 0.6760\n",
      "Epoch [73/80], time: 5.95s\n",
      "Train - loss: 0.0908, F1: 0.9708, UW_Acc: 0.9708\n",
      "Val - loss: 1.2344, F1: 0.6703, UW_Acc: 0.6708\n",
      "Epoch [74/80], time: 5.93s\n",
      "Train - loss: 0.0952, F1: 0.9687, UW_Acc: 0.9685\n",
      "Val - loss: 1.2376, F1: 0.6749, UW_Acc: 0.6757\n",
      "Epoch [75/80], time: 6.07s\n",
      "Train - loss: 0.0886, F1: 0.9723, UW_Acc: 0.9722\n",
      "Val - loss: 1.2172, F1: 0.6760, UW_Acc: 0.6762\n",
      "Epoch [76/80], time: 5.93s\n",
      "Train - loss: 0.0964, F1: 0.9699, UW_Acc: 0.9697\n",
      "Val - loss: 1.2187, F1: 0.6731, UW_Acc: 0.6737\n",
      "Epoch [77/80], time: 5.92s\n",
      "Train - loss: 0.0935, F1: 0.9700, UW_Acc: 0.9699\n",
      "Val - loss: 1.2273, F1: 0.6687, UW_Acc: 0.6694\n",
      "Epoch [78/80], time: 5.94s\n",
      "Train - loss: 0.0930, F1: 0.9694, UW_Acc: 0.9693\n",
      "Val - loss: 1.2278, F1: 0.6721, UW_Acc: 0.6731\n",
      "Epoch [79/80], time: 5.94s\n",
      "Train - loss: 0.0926, F1: 0.9689, UW_Acc: 0.9689\n",
      "Val - loss: 1.2380, F1: 0.6635, UW_Acc: 0.6639\n",
      "Epoch [80/80], time: 5.92s\n",
      "Train - loss: 0.0932, F1: 0.9691, UW_Acc: 0.9691\n",
      "Val - loss: 1.2284, F1: 0.6780, UW_Acc: 0.6782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.2308305335599323,\n",
       "  0.9752326427504073,\n",
       "  0.824103923731072,\n",
       "  0.7001542421274407,\n",
       "  0.6002598903899969,\n",
       "  0.5133445962916973,\n",
       "  0.45102210238922474,\n",
       "  0.39216787420039956,\n",
       "  0.3517377179029376,\n",
       "  0.3171789618425591,\n",
       "  0.2970693298550539,\n",
       "  0.23460519279158393,\n",
       "  0.20575914895811745,\n",
       "  0.18209245800971985,\n",
       "  0.16530216260011807,\n",
       "  0.17571557347857675,\n",
       "  0.1639437801962675,\n",
       "  0.16070767942556116,\n",
       "  0.1506528980856718,\n",
       "  0.14721127304919931,\n",
       "  0.1322363128149232,\n",
       "  0.12884535666468533,\n",
       "  0.13599042793692545,\n",
       "  0.11811296485884246,\n",
       "  0.1116602674646433,\n",
       "  0.10665130008791768,\n",
       "  0.11274751691624176,\n",
       "  0.10998728914662849,\n",
       "  0.1061461030743843,\n",
       "  0.10124147544766582,\n",
       "  0.10359856183099192,\n",
       "  0.09171678134521773,\n",
       "  0.09660792965875116,\n",
       "  0.09841575282950733,\n",
       "  0.09690624588104181,\n",
       "  0.09938645406171333,\n",
       "  0.09457190473412358,\n",
       "  0.09064373980427898,\n",
       "  0.09612741191373315,\n",
       "  0.08914554006485052,\n",
       "  0.1046496389736963,\n",
       "  0.0845747735611228,\n",
       "  0.09147951336101044,\n",
       "  0.0939126706920391,\n",
       "  0.0882981515727764,\n",
       "  0.08726495504379272,\n",
       "  0.09776703320270361,\n",
       "  0.096983993122744,\n",
       "  0.09245401915422706,\n",
       "  0.09600279202987981,\n",
       "  0.08884912625301716,\n",
       "  0.09681628315254699,\n",
       "  0.0944150359651377,\n",
       "  0.08646804874026498,\n",
       "  0.09200324993147406,\n",
       "  0.09158421056561691,\n",
       "  0.08903858784672826,\n",
       "  0.09705935661182848,\n",
       "  0.09355850742999898,\n",
       "  0.09752365115076997,\n",
       "  0.09550428841003152,\n",
       "  0.08665462955832481,\n",
       "  0.09044353503647239,\n",
       "  0.09575545302657194,\n",
       "  0.09106492303138555,\n",
       "  0.09300223625330038,\n",
       "  0.09177144071043924,\n",
       "  0.09517497092831967,\n",
       "  0.09223337142273437,\n",
       "  0.0899726715978495,\n",
       "  0.08657946247001026,\n",
       "  0.09204342042983965,\n",
       "  0.09075288708473361,\n",
       "  0.09521541293970374,\n",
       "  0.08855435174218444,\n",
       "  0.09639980973199357,\n",
       "  0.09346244397551515,\n",
       "  0.09304294137414111,\n",
       "  0.092551531746637,\n",
       "  0.09318762627798458],\n",
       " [0.40192428872578506,\n",
       "  0.5737222994995691,\n",
       "  0.662086715343446,\n",
       "  0.7192535089716445,\n",
       "  0.7676237051315339,\n",
       "  0.8013016288969117,\n",
       "  0.8277437845048294,\n",
       "  0.8532561034987207,\n",
       "  0.872158836820016,\n",
       "  0.879712858465357,\n",
       "  0.8893169250238633,\n",
       "  0.9151523301120701,\n",
       "  0.9269164921394482,\n",
       "  0.9387956138516405,\n",
       "  0.9406535968249169,\n",
       "  0.9369842926170132,\n",
       "  0.9414674264155717,\n",
       "  0.9446064437292038,\n",
       "  0.9459684057424013,\n",
       "  0.9494429612100747,\n",
       "  0.9570259816828166,\n",
       "  0.9554772095008506,\n",
       "  0.9530536952813498,\n",
       "  0.9611597749745188,\n",
       "  0.961462247365888,\n",
       "  0.9629328472281454,\n",
       "  0.9601694300943661,\n",
       "  0.9618714791803593,\n",
       "  0.9649430496883571,\n",
       "  0.9678767095392456,\n",
       "  0.966650009689204,\n",
       "  0.9713666853811049,\n",
       "  0.9682412031752055,\n",
       "  0.9692565291608699,\n",
       "  0.9680289694761608,\n",
       "  0.9676987072843248,\n",
       "  0.9667774460591115,\n",
       "  0.9689863055596843,\n",
       "  0.9664061095233832,\n",
       "  0.9724863995226889,\n",
       "  0.9626038384734908,\n",
       "  0.9726202349716295,\n",
       "  0.9694080408949719,\n",
       "  0.9673005445955788,\n",
       "  0.9714935019981688,\n",
       "  0.9721409845109452,\n",
       "  0.9670068636358446,\n",
       "  0.9691001055803498,\n",
       "  0.9696408328581733,\n",
       "  0.9700609335008892,\n",
       "  0.9734519936862,\n",
       "  0.9674320352863974,\n",
       "  0.9690100477285681,\n",
       "  0.9722239291474991,\n",
       "  0.9691747396599366,\n",
       "  0.9719578589557705,\n",
       "  0.971472335341657,\n",
       "  0.96523103791006,\n",
       "  0.9700386502024019,\n",
       "  0.9673876420104559,\n",
       "  0.9700073122251716,\n",
       "  0.9727728226965358,\n",
       "  0.9703770107623437,\n",
       "  0.967869847204957,\n",
       "  0.9710272013509446,\n",
       "  0.9696053836633456,\n",
       "  0.9686850491974631,\n",
       "  0.9689963030707243,\n",
       "  0.9703850865126263,\n",
       "  0.9733103654556348,\n",
       "  0.9734353826636545,\n",
       "  0.9712497181804611,\n",
       "  0.97084279955876,\n",
       "  0.9686757447999516,\n",
       "  0.9723379691756466,\n",
       "  0.9698936198625436,\n",
       "  0.9700356125455284,\n",
       "  0.9694239114067537,\n",
       "  0.9689094892574217,\n",
       "  0.969138707219885],\n",
       " [0.41497093023255816,\n",
       "  0.5771835359408034,\n",
       "  0.6621300211416491,\n",
       "  0.7185187632135306,\n",
       "  0.7674748942917548,\n",
       "  0.8010207452431289,\n",
       "  0.8274147727272727,\n",
       "  0.8526030655391121,\n",
       "  0.8716470665961946,\n",
       "  0.8793439482029598,\n",
       "  0.8892375792811839,\n",
       "  0.9148718287526426,\n",
       "  0.9267474894291755,\n",
       "  0.9385570824524313,\n",
       "  0.9403904598308668,\n",
       "  0.9368888742071882,\n",
       "  0.9411832716701902,\n",
       "  0.9444701374207187,\n",
       "  0.9458080073995773,\n",
       "  0.9492930761099366,\n",
       "  0.9567917547568711,\n",
       "  0.955272198731501,\n",
       "  0.952992864693446,\n",
       "  0.9611357029598309,\n",
       "  0.9613504228329809,\n",
       "  0.9628534619450317,\n",
       "  0.9599795190274842,\n",
       "  0.9618128964059197,\n",
       "  0.9648685253699789,\n",
       "  0.9676929175475687,\n",
       "  0.9665862843551797,\n",
       "  0.9712275369978859,\n",
       "  0.9681553911205074,\n",
       "  0.9691298890063424,\n",
       "  0.9679571881606766,\n",
       "  0.9675938160676534,\n",
       "  0.966602801268499,\n",
       "  0.968882135306554,\n",
       "  0.9662559460887948,\n",
       "  0.9724332716701902,\n",
       "  0.962523123678647,\n",
       "  0.9725984408033826,\n",
       "  0.9694106765327696,\n",
       "  0.9672139270613108,\n",
       "  0.9714092230443975,\n",
       "  0.9721194503171248,\n",
       "  0.9669001057082452,\n",
       "  0.96896471987315,\n",
       "  0.9694932610993657,\n",
       "  0.9697740486257929,\n",
       "  0.9733912526427062,\n",
       "  0.9673625792811839,\n",
       "  0.968882135306554,\n",
       "  0.9721524841437632,\n",
       "  0.9691298890063424,\n",
       "  0.9718716966173361,\n",
       "  0.9713927061310783,\n",
       "  0.9652153805496829,\n",
       "  0.9698731501057082,\n",
       "  0.9673956131078225,\n",
       "  0.9699557346723044,\n",
       "  0.9726645084566597,\n",
       "  0.9703191067653277,\n",
       "  0.9677589852008457,\n",
       "  0.9709797832980972,\n",
       "  0.969509778012685,\n",
       "  0.9684857293868923,\n",
       "  0.9687830338266384,\n",
       "  0.9702530391120507,\n",
       "  0.9732260835095138,\n",
       "  0.9733086680761099,\n",
       "  0.9712275369978859,\n",
       "  0.9707650634249472,\n",
       "  0.9685022463002114,\n",
       "  0.9722350687103594,\n",
       "  0.9696914640591966,\n",
       "  0.9699392177589853,\n",
       "  0.9693115750528541,\n",
       "  0.9688656183932347,\n",
       "  0.9691298890063424],\n",
       " [1.2428120076656342,\n",
       "  0.9937514662742615,\n",
       "  0.9848314374685287,\n",
       "  1.0537779778242111,\n",
       "  0.9235820695757866,\n",
       "  1.060736045241356,\n",
       "  1.0843929573893547,\n",
       "  1.176526516675949,\n",
       "  1.1973314359784126,\n",
       "  1.2464120760560036,\n",
       "  1.1158288717269897,\n",
       "  1.030765876173973,\n",
       "  1.034547172486782,\n",
       "  1.0695747658610344,\n",
       "  1.0799068808555603,\n",
       "  1.0851340293884277,\n",
       "  1.080346740782261,\n",
       "  1.2143369317054749,\n",
       "  1.192768543958664,\n",
       "  1.2051234990358353,\n",
       "  1.186643786728382,\n",
       "  1.2895818501710892,\n",
       "  1.2036310955882072,\n",
       "  1.1901041120290756,\n",
       "  1.186561830341816,\n",
       "  1.204737514257431,\n",
       "  1.185891143977642,\n",
       "  1.1808543354272842,\n",
       "  1.2272642999887466,\n",
       "  1.2064855992794037,\n",
       "  1.1890854388475418,\n",
       "  1.2406390756368637,\n",
       "  1.218850001692772,\n",
       "  1.2200803011655807,\n",
       "  1.224060297012329,\n",
       "  1.229066327214241,\n",
       "  1.221653826534748,\n",
       "  1.2286416888237,\n",
       "  1.229221947491169,\n",
       "  1.222251608967781,\n",
       "  1.2408995181322098,\n",
       "  1.2432432621717453,\n",
       "  1.230532743036747,\n",
       "  1.254205234348774,\n",
       "  1.218612715601921,\n",
       "  1.242410309612751,\n",
       "  1.2230336591601372,\n",
       "  1.2285932302474976,\n",
       "  1.2303285151720047,\n",
       "  1.2322737574577332,\n",
       "  1.2225515022873878,\n",
       "  1.2229816392064095,\n",
       "  1.2227033823728561,\n",
       "  1.220135010778904,\n",
       "  1.2329673916101456,\n",
       "  1.2288961708545685,\n",
       "  1.23138065636158,\n",
       "  1.2416858598589897,\n",
       "  1.2395966053009033,\n",
       "  1.2316699773073196,\n",
       "  1.2275073751807213,\n",
       "  1.2424814254045486,\n",
       "  1.2320910543203354,\n",
       "  1.2166623920202255,\n",
       "  1.2224239110946655,\n",
       "  1.23207575827837,\n",
       "  1.226425364613533,\n",
       "  1.2319751009345055,\n",
       "  1.2315702885389328,\n",
       "  1.230071596801281,\n",
       "  1.2286665886640549,\n",
       "  1.2310905456542969,\n",
       "  1.2344312891364098,\n",
       "  1.2375919818878174,\n",
       "  1.2171798348426819,\n",
       "  1.2187117040157318,\n",
       "  1.2273213490843773,\n",
       "  1.227757841348648,\n",
       "  1.2379802614450455,\n",
       "  1.2284118384122849],\n",
       " [0.38834533639266805,\n",
       "  0.5649506262465336,\n",
       "  0.5861806584882312,\n",
       "  0.558352569662051,\n",
       "  0.6535943652987326,\n",
       "  0.604214640753263,\n",
       "  0.606715122896917,\n",
       "  0.595274010872227,\n",
       "  0.6094473656943443,\n",
       "  0.6008631189443817,\n",
       "  0.6536584469198822,\n",
       "  0.663808286302438,\n",
       "  0.673560351711042,\n",
       "  0.6755430610096625,\n",
       "  0.6766258505066329,\n",
       "  0.6820477414667359,\n",
       "  0.6838341117980828,\n",
       "  0.6463090745458105,\n",
       "  0.6529698697769167,\n",
       "  0.6522516055182029,\n",
       "  0.6729796492915597,\n",
       "  0.6418326424772526,\n",
       "  0.6660435359867652,\n",
       "  0.677397119014839,\n",
       "  0.6761212988492963,\n",
       "  0.6705974269072502,\n",
       "  0.6784150833988378,\n",
       "  0.6764611586527053,\n",
       "  0.6644793397384509,\n",
       "  0.6713743723143776,\n",
       "  0.6793161224664774,\n",
       "  0.6673929936696293,\n",
       "  0.6724270330883213,\n",
       "  0.6736866239099593,\n",
       "  0.6748659612765147,\n",
       "  0.6759844871935592,\n",
       "  0.6751025493880287,\n",
       "  0.6757544261880921,\n",
       "  0.6713576657650303,\n",
       "  0.672957630467512,\n",
       "  0.6677693000100441,\n",
       "  0.6646579603890481,\n",
       "  0.6737201907726014,\n",
       "  0.6620587112769839,\n",
       "  0.6806663868420364,\n",
       "  0.6632580939708147,\n",
       "  0.6746480952342958,\n",
       "  0.6782511513150212,\n",
       "  0.6713349661322466,\n",
       "  0.6696729202582812,\n",
       "  0.6773681083815128,\n",
       "  0.6700038070968997,\n",
       "  0.6751198038297176,\n",
       "  0.6778388068350254,\n",
       "  0.6694765440862593,\n",
       "  0.671732560208442,\n",
       "  0.666683632256624,\n",
       "  0.6681654803696349,\n",
       "  0.6697414177893913,\n",
       "  0.670697045050535,\n",
       "  0.6712407529546873,\n",
       "  0.6737666565158903,\n",
       "  0.665295274326273,\n",
       "  0.6801880544328738,\n",
       "  0.6708027666498001,\n",
       "  0.6674518946006933,\n",
       "  0.6743471699265041,\n",
       "  0.6652862853561058,\n",
       "  0.6724745053507426,\n",
       "  0.6706976754682392,\n",
       "  0.6731914166361438,\n",
       "  0.6752366261514694,\n",
       "  0.670269660504386,\n",
       "  0.6748861337717256,\n",
       "  0.6759965355478195,\n",
       "  0.6731065447546583,\n",
       "  0.6686872668022584,\n",
       "  0.6721430243381543,\n",
       "  0.6634749488891915,\n",
       "  0.6779959233981554],\n",
       " [0.42667289402173914,\n",
       "  0.5696756114130435,\n",
       "  0.5923063858695652,\n",
       "  0.5844938858695652,\n",
       "  0.6539572010869565,\n",
       "  0.6081436820652174,\n",
       "  0.6134086277173914,\n",
       "  0.6067849864130435,\n",
       "  0.6133661684782609,\n",
       "  0.6085258152173914,\n",
       "  0.6592646059782609,\n",
       "  0.6649116847826086,\n",
       "  0.6756538722826086,\n",
       "  0.6752717391304348,\n",
       "  0.6789656929347826,\n",
       "  0.6828719429347826,\n",
       "  0.6842306385869565,\n",
       "  0.6492866847826086,\n",
       "  0.6545940896739131,\n",
       "  0.6534052309782609,\n",
       "  0.6746773097826086,\n",
       "  0.6436396059782609,\n",
       "  0.6668648097826086,\n",
       "  0.6779891304347826,\n",
       "  0.6768002717391304,\n",
       "  0.6709408967391304,\n",
       "  0.6785835597826086,\n",
       "  0.6772248641304348,\n",
       "  0.6652938179347826,\n",
       "  0.6717476222826086,\n",
       "  0.6803243885869565,\n",
       "  0.6678413722826086,\n",
       "  0.6731063179347826,\n",
       "  0.6742951766304348,\n",
       "  0.6756538722826086,\n",
       "  0.6766304347826086,\n",
       "  0.6756538722826086,\n",
       "  0.6766304347826086,\n",
       "  0.6723420516304348,\n",
       "  0.6737007472826086,\n",
       "  0.6678413722826086,\n",
       "  0.6649116847826086,\n",
       "  0.6746773097826086,\n",
       "  0.6625764266304348,\n",
       "  0.6815132472826086,\n",
       "  0.6635529891304348,\n",
       "  0.6752717391304348,\n",
       "  0.6785835597826086,\n",
       "  0.6717476222826086,\n",
       "  0.6703889266304348,\n",
       "  0.6779891304347826,\n",
       "  0.6703889266304348,\n",
       "  0.6756538722826086,\n",
       "  0.6785835597826086,\n",
       "  0.6697944972826086,\n",
       "  0.6721297554347826,\n",
       "  0.6672469429347826,\n",
       "  0.6684358016304348,\n",
       "  0.6701766304347826,\n",
       "  0.6713654891304348,\n",
       "  0.6717476222826086,\n",
       "  0.6746773097826086,\n",
       "  0.6655061141304348,\n",
       "  0.6805366847826086,\n",
       "  0.6713654891304348,\n",
       "  0.6678413722826086,\n",
       "  0.6746773097826086,\n",
       "  0.6658882472826086,\n",
       "  0.6727241847826086,\n",
       "  0.6711531929347826,\n",
       "  0.6737007472826086,\n",
       "  0.6760360054347826,\n",
       "  0.6707710597826086,\n",
       "  0.6756538722826086,\n",
       "  0.6762483016304348,\n",
       "  0.6737007472826086,\n",
       "  0.6694123641304348,\n",
       "  0.6731063179347826,\n",
       "  0.6639351222826086,\n",
       "  0.6782014266304348])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# 优化器\n",
    "optimizer = torch.optim.AdamW(model_combined.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "\n",
    "# 学习率调度器\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.3, patience=5, verbose=True)\n",
    "\n",
    "# 调用训练函数进行训练\n",
    "train_model(model_combined, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=80, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d4f9f",
   "metadata": {
    "papermill": {
     "duration": 0.027951,
     "end_time": "2024-09-18T17:34:10.037325",
     "exception": false,
     "start_time": "2024-09-18T17:34:10.009374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model-6: Multiattention Text+speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a052ad7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:34:10.095526Z",
     "iopub.status.busy": "2024-09-18T17:34:10.095227Z",
     "iopub.status.idle": "2024-09-18T17:34:10.103029Z",
     "shell.execute_reply": "2024-09-18T17:34:10.102119Z"
    },
    "papermill": {
     "duration": 0.039418,
     "end_time": "2024-09-18T17:34:10.104822",
     "exception": false,
     "start_time": "2024-09-18T17:34:10.065404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class MultiHeadSelfAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads):\n",
    "#         super(MultiHeadSelfAttention, self).__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = embed_dim // num_heads\n",
    "\n",
    "#         assert (\n",
    "#             self.head_dim * num_heads == embed_dim\n",
    "#         ), \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "#         self.query = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.key = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.value = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         N, seq_length, embed_dim = x.shape\n",
    "\n",
    "#         # Split the embedding into self.num_heads different pieces\n",
    "#         queries = self.query(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "#         keys = self.key(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "#         values = self.value(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "#         # Transpose the queries, keys, and values\n",
    "#         queries = queries.transpose(1, 2)\n",
    "#         keys = keys.transpose(1, 2)\n",
    "#         values = values.transpose(1, 2)\n",
    "\n",
    "#         # Calculate the energy between the queries and keys\n",
    "#         energy = torch.matmul(queries, keys.transpose(-1, -2)) / (self.head_dim ** (1 / 2))\n",
    "#         attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "#         # Multiply the attention values with the values\n",
    "#         out = torch.matmul(attention, values)\n",
    "#         out = out.transpose(1, 2).contiguous().view(N, seq_length, self.embed_dim)\n",
    "        \n",
    "#         # Apply the final fully connected layer\n",
    "#         out = self.fc_out(out)\n",
    "        \n",
    "#         # Add residual connection and apply layer normalization\n",
    "#         out = self.layer_norm(out + x)\n",
    "\n",
    "#         return out\n",
    "\n",
    "# class TextModel(nn.Module):\n",
    "#     def __init__(self, embedding_dim, max_sequence_length, g_word_embedding_matrix, dropout_rate=0.1, num_heads=num_heads):\n",
    "#         super(TextModel, self).__init__()\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.embedding = nn.Embedding.from_pretrained(g_word_embedding_matrix, freeze=False)  # Make embeddings trainable\n",
    "#         self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=512, batch_first=True, bidirectional=True, num_layers=2, dropout=dropout_rate)\n",
    "#         self.lstm2 = nn.LSTM(input_size=512*2, hidden_size=512, batch_first=True, bidirectional=True, num_layers=2, dropout=dropout_rate)  # Note: 512*2 due to bidirectional\n",
    "#         self.attention = MultiHeadSelfAttention(embed_dim=512*2, num_heads=num_heads)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)  # Dropout layer\n",
    "#         self.dense = nn.Linear(512*2, 256)  # 512*2 due to bidirectional output\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         x, _ = self.lstm1(x)\n",
    "#         x, _ = self.lstm2(x)\n",
    "#         x = self.attention(x)\n",
    "#         x = self.dropout(x[:, -1, :])  # Apply dropout to the output of the last LSTM cell\n",
    "#         x = self.dense(x)  # Only use the output of the last LSTM cell\n",
    "#         return x\n",
    "\n",
    "# class SpeechModel(nn.Module):\n",
    "#     def __init__(self, dropout_rate=0.3, num_heads=num_heads):\n",
    "#         super(SpeechModel, self).__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.dense1 = nn.Linear(100 * 34, 1024)\n",
    "#         self.bn1 = nn.BatchNorm1d(1024)\n",
    "#         self.attention = MultiHeadSelfAttention(embed_dim=1024, num_heads=num_heads)\n",
    "#         self.dense2 = nn.Linear(1024, 256)\n",
    "#         self.bn2 = nn.BatchNorm1d(256)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         x = F.relu(self.bn1(self.dense1(x)))\n",
    "#         x = self.attention(x.unsqueeze(1)).squeeze(1)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.bn2(self.dense2(x))\n",
    "#         return x\n",
    "\n",
    "# class CombinedModel(nn.Module):\n",
    "#     def __init__(self, embedding_dim, max_sequence_length, g_word_embedding_matrix, dropout_rate=0.3, num_heads=num_heads):\n",
    "#         super(CombinedModel, self).__init__()\n",
    "#         self.text_model = TextModel(embedding_dim, max_sequence_length, g_word_embedding_matrix, dropout_rate, num_heads)\n",
    "#         self.speech_model = SpeechModel(dropout_rate, num_heads)\n",
    "#         self.fc1 = nn.Linear(256 * 2, 256)\n",
    "#         self.bn1 = nn.BatchNorm1d(256)\n",
    "#         self.fc2 = nn.Linear(256, 4)\n",
    "    \n",
    "#     def forward(self, text, speech, mocap=None):\n",
    "#         text_out = self.text_model(text)\n",
    "#         speech_out = self.speech_model(speech)\n",
    "#         combined = torch.cat((text_out, speech_out), dim=1)\n",
    "#         combined = F.relu(self.bn1(self.fc1(combined)))\n",
    "#         combined = self.fc2(combined)\n",
    "#         return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b9b3148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:34:10.163186Z",
     "iopub.status.busy": "2024-09-18T17:34:10.162908Z",
     "iopub.status.idle": "2024-09-18T17:34:10.167180Z",
     "shell.execute_reply": "2024-09-18T17:34:10.166328Z"
    },
    "papermill": {
     "duration": 0.035218,
     "end_time": "2024-09-18T17:34:10.168989",
     "exception": false,
     "start_time": "2024-09-18T17:34:10.133771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 设置设备为GPU或CPU\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # 检查 class_weights 是否已经在正确的设备上\n",
    "# if not isinstance(class_weights, torch.Tensor):\n",
    "#     class_weights = torch.FloatTensor(class_weights)\n",
    "\n",
    "# class_weights = class_weights.to(device)\n",
    "\n",
    "# # 实例化模型、损失函数和优化器\n",
    "# model_combined = CombinedModel(nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix)\n",
    "\n",
    "# # 打印模型摘要\n",
    "# print(model_combined)\n",
    "\n",
    "# print(\"Combined Model Summary:\")\n",
    "# print_model_summary(model_combined, [(max_sequence_length,), (100, 34), (200, 189, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fd82030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:34:10.227092Z",
     "iopub.status.busy": "2024-09-18T17:34:10.226679Z",
     "iopub.status.idle": "2024-09-18T17:34:10.230857Z",
     "shell.execute_reply": "2024-09-18T17:34:10.230012Z"
    },
    "papermill": {
     "duration": 0.035546,
     "end_time": "2024-09-18T17:34:10.232708",
     "exception": false,
     "start_time": "2024-09-18T17:34:10.197162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 使用DataParallel包装模型\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "#     model_combined = nn.DataParallel(model_combined)\n",
    "\n",
    "# # 将模型移动到设备上\n",
    "# model_combined.to(device)\n",
    "\n",
    "# # 假设我们有一个batch\n",
    "# batch = next(iter(train_loader))\n",
    "# text = batch['text'].to(device).long()\n",
    "# speech = batch['speech'].to(device)\n",
    "# mocap = batch['mocap'].to(device)\n",
    "\n",
    "# # 前向传播\n",
    "# outputs = model_combined(text, speech, mocap)\n",
    "\n",
    "# # 打印输出形状\n",
    "# print(outputs.shape)  # 打印模型输出的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d9588b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:34:10.290453Z",
     "iopub.status.busy": "2024-09-18T17:34:10.290184Z",
     "iopub.status.idle": "2024-09-18T17:34:10.294505Z",
     "shell.execute_reply": "2024-09-18T17:34:10.293711Z"
    },
    "papermill": {
     "duration": 0.035202,
     "end_time": "2024-09-18T17:34:10.296412",
     "exception": false,
     "start_time": "2024-09-18T17:34:10.261210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# # 优化器\n",
    "# optimizer = torch.optim.AdamW(model_combined.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "\n",
    "# # 学习率调度器\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.3, patience=5, verbose=True)\n",
    "\n",
    "# # 调用训练函数进行训练\n",
    "# train_model(model_combined, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=80, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87aca9",
   "metadata": {
    "papermill": {
     "duration": 0.028572,
     "end_time": "2024-09-18T17:34:10.354705",
     "exception": false,
     "start_time": "2024-09-18T17:34:10.326133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model-7: Multi-Attention Text+speech+mocap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84c78ddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:34:10.413778Z",
     "iopub.status.busy": "2024-09-18T17:34:10.413519Z",
     "iopub.status.idle": "2024-09-18T17:34:10.454523Z",
     "shell.execute_reply": "2024-09-18T17:34:10.453840Z"
    },
    "papermill": {
     "duration": 0.072974,
     "end_time": "2024-09-18T17:34:10.456452",
     "exception": false,
     "start_time": "2024-09-18T17:34:10.383478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_dim\n",
    "        ), \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = FeedForward(embed_dim, 4 * embed_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, embed_dim = x.shape\n",
    "\n",
    "        queries = self.query(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "        keys = self.key(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "        values = self.value(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        energy = torch.matmul(queries, keys.transpose(-1, -2)) / (self.head_dim ** (1 / 2))\n",
    "        attention = self.dropout(torch.softmax(energy, dim=-1))  # 添加dropout\n",
    "\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.transpose(1, 2).contiguous().view(N, seq_length, self.embed_dim)\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        out = self.dropout(out)  # 添加dropout\n",
    "        out = self.layer_norm1(out + x)\n",
    "        ff_out = self.feed_forward(out)\n",
    "        out = self.layer_norm2(ff_out + out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class TextModel(nn.Module):\n",
    "    def __init__(self, nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads=num_heads):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(nb_words, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(g_word_embedding_matrix.clone().detach())\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.conv2 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim=32, num_heads=num_heads)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(32 * max_sequence_length, 256)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.bn5(self.dense(x))\n",
    "        return x\n",
    "\n",
    "class SpeechModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.1, num_heads=num_heads):\n",
    "        super(SpeechModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(100 * 34, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim=1024, num_heads=num_heads)\n",
    "        self.dense2 = nn.Linear(1024, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.bn1(self.dense1(x)))\n",
    "        x = self.attention(x.unsqueeze(1)).squeeze(1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn2(self.dense2(x))\n",
    "        return x\n",
    "\n",
    "class MocapModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MocapModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(256 * 7 * 6, 256)\n",
    "        self.bn8 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.bn8(self.dense(x))\n",
    "        return x\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads=num_heads):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.text_model = TextModel(nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads=num_heads)\n",
    "        self.speech_model = SpeechModel(num_heads=num_heads)\n",
    "        self.mocap_model = MocapModel()\n",
    "        self.fc1 = nn.Linear(256 * 3, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 4)\n",
    "    \n",
    "    def forward(self, text, speech, mocap):\n",
    "        text_out = self.text_model(text)\n",
    "        speech_out = self.speech_model(speech)\n",
    "        mocap_out = self.mocap_model(mocap)\n",
    "        combined = torch.cat((text_out, speech_out, mocap_out), dim=1)\n",
    "        combined = F.relu(self.bn1(self.fc1(combined)))\n",
    "        combined = self.fc2(combined)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca8de5cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:34:10.513933Z",
     "iopub.status.busy": "2024-09-18T17:34:10.513634Z",
     "iopub.status.idle": "2024-09-18T17:34:10.771642Z",
     "shell.execute_reply": "2024-09-18T17:34:10.770695Z"
    },
    "papermill": {
     "duration": 0.289796,
     "end_time": "2024-09-18T17:34:10.774317",
     "exception": false,
     "start_time": "2024-09-18T17:34:10.484521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CombinedModel(\n",
      "  (text_model): TextModel(\n",
      "    (embedding): Embedding(3130, 300)\n",
      "    (conv1): Conv1d(300, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadSelfAttention(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=16000, out_features=256, bias=True)\n",
      "    (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (speech_model): SpeechModel(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=3400, out_features=1024, bias=True)\n",
      "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadSelfAttention(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (mocap_model): MocapModel(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=10752, out_features=256, bias=True)\n",
      "    (bn8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "Combined Model Summary:\n",
      "Model Structure:\n",
      "CombinedModel(\n",
      "  (text_model): TextModel(\n",
      "    (embedding): Embedding(3130, 300)\n",
      "    (conv1): Conv1d(300, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadSelfAttention(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=16000, out_features=256, bias=True)\n",
      "    (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (speech_model): SpeechModel(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=3400, out_features=1024, bias=True)\n",
      "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadSelfAttention(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (mocap_model): MocapModel(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=10752, out_features=256, bias=True)\n",
      "    (bn8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "Input size: [(500,), (100, 34), (200, 189, 1)]\n",
      "text_model: 5,409,336 parameters\n",
      "  embedding: 939,000 parameters\n",
      "  conv1: 230,656 parameters\n",
      "  bn1: 512 parameters\n",
      "  conv2: 98,432 parameters\n",
      "  bn2: 256 parameters\n",
      "  conv3: 24,640 parameters\n",
      "  bn3: 128 parameters\n",
      "  conv4: 6,176 parameters\n",
      "  bn4: 64 parameters\n",
      "  attention: 12,704 parameters\n",
      "  dropout: 0 parameters\n",
      "  flatten: 0 parameters\n",
      "  dense: 4,096,256 parameters\n",
      "  bn5: 512 parameters\n",
      "speech_model: 16,343,808 parameters\n",
      "  flatten: 0 parameters\n",
      "  dense1: 3,482,624 parameters\n",
      "  bn1: 2,048 parameters\n",
      "  attention: 12,596,224 parameters\n",
      "  dense2: 262,400 parameters\n",
      "  bn2: 512 parameters\n",
      "  dropout: 0 parameters\n",
      "mocap_model: 3,179,136 parameters\n",
      "  conv1: 320 parameters\n",
      "  bn1: 64 parameters\n",
      "  conv2: 18,496 parameters\n",
      "  bn2: 128 parameters\n",
      "  conv3: 36,928 parameters\n",
      "  bn3: 128 parameters\n",
      "  conv4: 73,856 parameters\n",
      "  bn4: 256 parameters\n",
      "  conv5: 295,168 parameters\n",
      "  bn5: 512 parameters\n",
      "  dropout: 0 parameters\n",
      "  flatten: 0 parameters\n",
      "  dense: 2,752,768 parameters\n",
      "  bn8: 512 parameters\n",
      "fc1: 196,864 parameters\n",
      "bn1: 512 parameters\n",
      "fc2: 1,028 parameters\n",
      "\n",
      "Total trainable parameters: 25,130,684\n",
      "\n",
      "Detailed parameter shapes:\n",
      "text_model.embedding.weight: torch.Size([3130, 300])\n",
      "text_model.conv1.weight: torch.Size([256, 300, 3])\n",
      "text_model.conv1.bias: torch.Size([256])\n",
      "text_model.bn1.weight: torch.Size([256])\n",
      "text_model.bn1.bias: torch.Size([256])\n",
      "text_model.conv2.weight: torch.Size([128, 256, 3])\n",
      "text_model.conv2.bias: torch.Size([128])\n",
      "text_model.bn2.weight: torch.Size([128])\n",
      "text_model.bn2.bias: torch.Size([128])\n",
      "text_model.conv3.weight: torch.Size([64, 128, 3])\n",
      "text_model.conv3.bias: torch.Size([64])\n",
      "text_model.bn3.weight: torch.Size([64])\n",
      "text_model.bn3.bias: torch.Size([64])\n",
      "text_model.conv4.weight: torch.Size([32, 64, 3])\n",
      "text_model.conv4.bias: torch.Size([32])\n",
      "text_model.bn4.weight: torch.Size([32])\n",
      "text_model.bn4.bias: torch.Size([32])\n",
      "text_model.attention.query.weight: torch.Size([32, 32])\n",
      "text_model.attention.query.bias: torch.Size([32])\n",
      "text_model.attention.key.weight: torch.Size([32, 32])\n",
      "text_model.attention.key.bias: torch.Size([32])\n",
      "text_model.attention.value.weight: torch.Size([32, 32])\n",
      "text_model.attention.value.bias: torch.Size([32])\n",
      "text_model.attention.fc_out.weight: torch.Size([32, 32])\n",
      "text_model.attention.fc_out.bias: torch.Size([32])\n",
      "text_model.attention.layer_norm1.weight: torch.Size([32])\n",
      "text_model.attention.layer_norm1.bias: torch.Size([32])\n",
      "text_model.attention.layer_norm2.weight: torch.Size([32])\n",
      "text_model.attention.layer_norm2.bias: torch.Size([32])\n",
      "text_model.attention.feed_forward.linear1.weight: torch.Size([128, 32])\n",
      "text_model.attention.feed_forward.linear1.bias: torch.Size([128])\n",
      "text_model.attention.feed_forward.linear2.weight: torch.Size([32, 128])\n",
      "text_model.attention.feed_forward.linear2.bias: torch.Size([32])\n",
      "text_model.dense.weight: torch.Size([256, 16000])\n",
      "text_model.dense.bias: torch.Size([256])\n",
      "text_model.bn5.weight: torch.Size([256])\n",
      "text_model.bn5.bias: torch.Size([256])\n",
      "speech_model.dense1.weight: torch.Size([1024, 3400])\n",
      "speech_model.dense1.bias: torch.Size([1024])\n",
      "speech_model.bn1.weight: torch.Size([1024])\n",
      "speech_model.bn1.bias: torch.Size([1024])\n",
      "speech_model.attention.query.weight: torch.Size([1024, 1024])\n",
      "speech_model.attention.query.bias: torch.Size([1024])\n",
      "speech_model.attention.key.weight: torch.Size([1024, 1024])\n",
      "speech_model.attention.key.bias: torch.Size([1024])\n",
      "speech_model.attention.value.weight: torch.Size([1024, 1024])\n",
      "speech_model.attention.value.bias: torch.Size([1024])\n",
      "speech_model.attention.fc_out.weight: torch.Size([1024, 1024])\n",
      "speech_model.attention.fc_out.bias: torch.Size([1024])\n",
      "speech_model.attention.layer_norm1.weight: torch.Size([1024])\n",
      "speech_model.attention.layer_norm1.bias: torch.Size([1024])\n",
      "speech_model.attention.layer_norm2.weight: torch.Size([1024])\n",
      "speech_model.attention.layer_norm2.bias: torch.Size([1024])\n",
      "speech_model.attention.feed_forward.linear1.weight: torch.Size([4096, 1024])\n",
      "speech_model.attention.feed_forward.linear1.bias: torch.Size([4096])\n",
      "speech_model.attention.feed_forward.linear2.weight: torch.Size([1024, 4096])\n",
      "speech_model.attention.feed_forward.linear2.bias: torch.Size([1024])\n",
      "speech_model.dense2.weight: torch.Size([256, 1024])\n",
      "speech_model.dense2.bias: torch.Size([256])\n",
      "speech_model.bn2.weight: torch.Size([256])\n",
      "speech_model.bn2.bias: torch.Size([256])\n",
      "mocap_model.conv1.weight: torch.Size([32, 1, 3, 3])\n",
      "mocap_model.conv1.bias: torch.Size([32])\n",
      "mocap_model.bn1.weight: torch.Size([32])\n",
      "mocap_model.bn1.bias: torch.Size([32])\n",
      "mocap_model.conv2.weight: torch.Size([64, 32, 3, 3])\n",
      "mocap_model.conv2.bias: torch.Size([64])\n",
      "mocap_model.bn2.weight: torch.Size([64])\n",
      "mocap_model.bn2.bias: torch.Size([64])\n",
      "mocap_model.conv3.weight: torch.Size([64, 64, 3, 3])\n",
      "mocap_model.conv3.bias: torch.Size([64])\n",
      "mocap_model.bn3.weight: torch.Size([64])\n",
      "mocap_model.bn3.bias: torch.Size([64])\n",
      "mocap_model.conv4.weight: torch.Size([128, 64, 3, 3])\n",
      "mocap_model.conv4.bias: torch.Size([128])\n",
      "mocap_model.bn4.weight: torch.Size([128])\n",
      "mocap_model.bn4.bias: torch.Size([128])\n",
      "mocap_model.conv5.weight: torch.Size([256, 128, 3, 3])\n",
      "mocap_model.conv5.bias: torch.Size([256])\n",
      "mocap_model.bn5.weight: torch.Size([256])\n",
      "mocap_model.bn5.bias: torch.Size([256])\n",
      "mocap_model.dense.weight: torch.Size([256, 10752])\n",
      "mocap_model.dense.bias: torch.Size([256])\n",
      "mocap_model.bn8.weight: torch.Size([256])\n",
      "mocap_model.bn8.bias: torch.Size([256])\n",
      "fc1.weight: torch.Size([256, 768])\n",
      "fc1.bias: torch.Size([256])\n",
      "bn1.weight: torch.Size([256])\n",
      "bn1.bias: torch.Size([256])\n",
      "fc2.weight: torch.Size([4, 256])\n",
      "fc2.bias: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# 设置设备为GPU或CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 检查 class_weights 是否已经在正确的设备上\n",
    "if not isinstance(class_weights, torch.Tensor):\n",
    "    class_weights = torch.FloatTensor(class_weights)\n",
    "\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# 模型实例化\n",
    "model_combined = CombinedModel(nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads)\n",
    "\n",
    "# 打印模型摘要\n",
    "print(model_combined)\n",
    "\n",
    "print(\"Combined Model Summary:\")\n",
    "print_model_summary(model_combined, [(max_sequence_length,), (100, 34), (200, 189, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "524a568c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:34:10.834124Z",
     "iopub.status.busy": "2024-09-18T17:34:10.833789Z",
     "iopub.status.idle": "2024-09-18T17:34:12.049938Z",
     "shell.execute_reply": "2024-09-18T17:34:12.048638Z"
    },
    "papermill": {
     "duration": 1.248532,
     "end_time": "2024-09-18T17:34:12.052235",
     "exception": false,
     "start_time": "2024-09-18T17:34:10.803703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "torch.Size([128, 4])\n"
     ]
    }
   ],
   "source": [
    "# 使用DataParallel包装模型\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model_combined = nn.DataParallel(model_combined)\n",
    "\n",
    "# 将模型移动到设备上\n",
    "model_combined.to(device)\n",
    "\n",
    "# 假设我们有一个batch\n",
    "batch = next(iter(train_loader))\n",
    "text = batch['text'].to(device).long()\n",
    "speech = batch['speech'].to(device)\n",
    "mocap = batch['mocap'].to(device)\n",
    "\n",
    "# 前向传播\n",
    "outputs = model_combined(text, speech, mocap)\n",
    "\n",
    "# 打印输出形状\n",
    "print(outputs.shape)  # 打印模型输出的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fe2a87f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:34:12.112772Z",
     "iopub.status.busy": "2024-09-18T17:34:12.112425Z",
     "iopub.status.idle": "2024-09-18T17:46:41.944042Z",
     "shell.execute_reply": "2024-09-18T17:46:41.942943Z"
    },
    "papermill": {
     "duration": 749.864031,
     "end_time": "2024-09-18T17:46:41.946274",
     "exception": false,
     "start_time": "2024-09-18T17:34:12.082243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], time: 8.09s\n",
      "Train - loss: 1.1401, F1: 0.4806, UW_Acc: 0.4936\n",
      "Val - loss: 1.2778, F1: 0.4386, UW_Acc: 0.4581\n",
      "Epoch [2/80], time: 7.87s\n",
      "Train - loss: 0.8184, F1: 0.6622, UW_Acc: 0.6654\n",
      "Val - loss: 0.9675, F1: 0.5730, UW_Acc: 0.5837\n",
      "Epoch [3/80], time: 7.85s\n",
      "Train - loss: 0.6374, F1: 0.7499, UW_Acc: 0.7497\n",
      "Val - loss: 0.9889, F1: 0.6106, UW_Acc: 0.6167\n",
      "Epoch [4/80], time: 7.84s\n",
      "Train - loss: 0.5260, F1: 0.8007, UW_Acc: 0.8009\n",
      "Val - loss: 0.9681, F1: 0.6285, UW_Acc: 0.6417\n",
      "Epoch [5/80], time: 7.82s\n",
      "Train - loss: 0.4377, F1: 0.8416, UW_Acc: 0.8411\n",
      "Val - loss: 0.9710, F1: 0.6369, UW_Acc: 0.6526\n",
      "Epoch [6/80], time: 7.98s\n",
      "Train - loss: 0.3716, F1: 0.8644, UW_Acc: 0.8642\n",
      "Val - loss: 0.8444, F1: 0.6798, UW_Acc: 0.6858\n",
      "Epoch [7/80], time: 7.80s\n",
      "Train - loss: 0.3140, F1: 0.8885, UW_Acc: 0.8880\n",
      "Val - loss: 1.1308, F1: 0.6330, UW_Acc: 0.6436\n",
      "Epoch [8/80], time: 7.81s\n",
      "Train - loss: 0.2715, F1: 0.9070, UW_Acc: 0.9066\n",
      "Val - loss: 0.8907, F1: 0.6975, UW_Acc: 0.7022\n",
      "Epoch [9/80], time: 7.80s\n",
      "Train - loss: 0.2247, F1: 0.9206, UW_Acc: 0.9205\n",
      "Val - loss: 1.0555, F1: 0.6676, UW_Acc: 0.6780\n",
      "Epoch [10/80], time: 7.84s\n",
      "Train - loss: 0.1981, F1: 0.9366, UW_Acc: 0.9364\n",
      "Val - loss: 0.9912, F1: 0.6848, UW_Acc: 0.6883\n",
      "Epoch [11/80], time: 7.80s\n",
      "Train - loss: 0.1672, F1: 0.9471, UW_Acc: 0.9469\n",
      "Val - loss: 1.1110, F1: 0.6412, UW_Acc: 0.6509\n",
      "Epoch [12/80], time: 7.84s\n",
      "Train - loss: 0.1619, F1: 0.9474, UW_Acc: 0.9472\n",
      "Val - loss: 1.0847, F1: 0.6642, UW_Acc: 0.6720\n",
      "Epoch [13/80], time: 7.90s\n",
      "Train - loss: 0.1457, F1: 0.9505, UW_Acc: 0.9503\n",
      "Val - loss: 1.1507, F1: 0.6541, UW_Acc: 0.6671\n",
      "Epoch [14/80], time: 7.83s\n",
      "Train - loss: 0.1235, F1: 0.9625, UW_Acc: 0.9625\n",
      "Val - loss: 0.9558, F1: 0.7136, UW_Acc: 0.7164\n",
      "Epoch [15/80], time: 7.80s\n",
      "Train - loss: 0.1132, F1: 0.9641, UW_Acc: 0.9641\n",
      "Val - loss: 1.0007, F1: 0.7126, UW_Acc: 0.7157\n",
      "Epoch [16/80], time: 7.81s\n",
      "Train - loss: 0.1101, F1: 0.9620, UW_Acc: 0.9621\n",
      "Val - loss: 1.0230, F1: 0.7070, UW_Acc: 0.7090\n",
      "Epoch [17/80], time: 7.85s\n",
      "Train - loss: 0.0942, F1: 0.9692, UW_Acc: 0.9691\n",
      "Val - loss: 1.0644, F1: 0.7053, UW_Acc: 0.7092\n",
      "Epoch [18/80], time: 7.84s\n",
      "Train - loss: 0.0957, F1: 0.9696, UW_Acc: 0.9697\n",
      "Val - loss: 1.3828, F1: 0.6472, UW_Acc: 0.6554\n",
      "Epoch [19/80], time: 7.84s\n",
      "Train - loss: 0.0853, F1: 0.9743, UW_Acc: 0.9741\n",
      "Val - loss: 1.0953, F1: 0.6881, UW_Acc: 0.6954\n",
      "Epoch [20/80], time: 7.88s\n",
      "Train - loss: 0.0738, F1: 0.9767, UW_Acc: 0.9767\n",
      "Val - loss: 1.1005, F1: 0.7183, UW_Acc: 0.7217\n",
      "Epoch [21/80], time: 7.85s\n",
      "Train - loss: 0.0756, F1: 0.9781, UW_Acc: 0.9780\n",
      "Val - loss: 1.2814, F1: 0.6875, UW_Acc: 0.6873\n",
      "Epoch [22/80], time: 7.82s\n",
      "Train - loss: 0.0642, F1: 0.9811, UW_Acc: 0.9811\n",
      "Val - loss: 1.4772, F1: 0.6371, UW_Acc: 0.6521\n",
      "Epoch [23/80], time: 7.94s\n",
      "Train - loss: 0.0638, F1: 0.9813, UW_Acc: 0.9813\n",
      "Val - loss: 1.1326, F1: 0.7015, UW_Acc: 0.7092\n",
      "Epoch [24/80], time: 7.82s\n",
      "Train - loss: 0.0640, F1: 0.9805, UW_Acc: 0.9805\n",
      "Val - loss: 1.1063, F1: 0.7090, UW_Acc: 0.7127\n",
      "Epoch [25/80], time: 7.86s\n",
      "Train - loss: 0.0576, F1: 0.9821, UW_Acc: 0.9820\n",
      "Val - loss: 1.2460, F1: 0.7036, UW_Acc: 0.7051\n",
      "Epoch 00026: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch [26/80], time: 7.89s\n",
      "Train - loss: 0.0613, F1: 0.9794, UW_Acc: 0.9794\n",
      "Val - loss: 1.2388, F1: 0.6875, UW_Acc: 0.6909\n",
      "Epoch [27/80], time: 7.93s\n",
      "Train - loss: 0.0378, F1: 0.9901, UW_Acc: 0.9901\n",
      "Val - loss: 1.2693, F1: 0.6744, UW_Acc: 0.6847\n",
      "Epoch [28/80], time: 7.84s\n",
      "Train - loss: 0.0248, F1: 0.9949, UW_Acc: 0.9949\n",
      "Val - loss: 1.2562, F1: 0.6940, UW_Acc: 0.7004\n",
      "Epoch [29/80], time: 7.85s\n",
      "Train - loss: 0.0205, F1: 0.9954, UW_Acc: 0.9954\n",
      "Val - loss: 1.1833, F1: 0.7066, UW_Acc: 0.7121\n",
      "Epoch [30/80], time: 7.82s\n",
      "Train - loss: 0.0213, F1: 0.9952, UW_Acc: 0.9952\n",
      "Val - loss: 1.1408, F1: 0.7296, UW_Acc: 0.7328\n",
      "Epoch [31/80], time: 7.83s\n",
      "Train - loss: 0.0183, F1: 0.9967, UW_Acc: 0.9967\n",
      "Val - loss: 1.1703, F1: 0.7200, UW_Acc: 0.7242\n",
      "Epoch [32/80], time: 7.81s\n",
      "Train - loss: 0.0194, F1: 0.9955, UW_Acc: 0.9955\n",
      "Val - loss: 1.2873, F1: 0.6955, UW_Acc: 0.7041\n",
      "Epoch [33/80], time: 7.81s\n",
      "Train - loss: 0.0169, F1: 0.9956, UW_Acc: 0.9956\n",
      "Val - loss: 1.2011, F1: 0.7130, UW_Acc: 0.7176\n",
      "Epoch [34/80], time: 7.83s\n",
      "Train - loss: 0.0135, F1: 0.9984, UW_Acc: 0.9984\n",
      "Val - loss: 1.2125, F1: 0.7270, UW_Acc: 0.7297\n",
      "Epoch [35/80], time: 7.85s\n",
      "Train - loss: 0.0166, F1: 0.9955, UW_Acc: 0.9955\n",
      "Val - loss: 1.3054, F1: 0.7106, UW_Acc: 0.7155\n",
      "Epoch 00036: reducing learning rate of group 0 to 9.0000e-06.\n",
      "Epoch [36/80], time: 7.83s\n",
      "Train - loss: 0.0181, F1: 0.9954, UW_Acc: 0.9954\n",
      "Val - loss: 1.2353, F1: 0.7099, UW_Acc: 0.7137\n",
      "Epoch [37/80], time: 7.86s\n",
      "Train - loss: 0.0144, F1: 0.9965, UW_Acc: 0.9965\n",
      "Val - loss: 1.2561, F1: 0.7298, UW_Acc: 0.7330\n",
      "Epoch [38/80], time: 7.82s\n",
      "Train - loss: 0.0115, F1: 0.9983, UW_Acc: 0.9983\n",
      "Val - loss: 1.2473, F1: 0.7213, UW_Acc: 0.7262\n",
      "Epoch [39/80], time: 7.82s\n",
      "Train - loss: 0.0102, F1: 0.9985, UW_Acc: 0.9985\n",
      "Val - loss: 1.2535, F1: 0.7175, UW_Acc: 0.7223\n",
      "Epoch [40/80], time: 7.81s\n",
      "Train - loss: 0.0081, F1: 0.9989, UW_Acc: 0.9989\n",
      "Val - loss: 1.2355, F1: 0.7172, UW_Acc: 0.7213\n",
      "Epoch [41/80], time: 7.78s\n",
      "Train - loss: 0.0088, F1: 0.9984, UW_Acc: 0.9984\n",
      "Val - loss: 1.2375, F1: 0.7122, UW_Acc: 0.7164\n",
      "Epoch [42/80], time: 7.81s\n",
      "Train - loss: 0.0090, F1: 0.9985, UW_Acc: 0.9985\n",
      "Val - loss: 1.2666, F1: 0.7110, UW_Acc: 0.7162\n",
      "Epoch 00043: reducing learning rate of group 0 to 2.7000e-06.\n",
      "Epoch [43/80], time: 7.80s\n",
      "Train - loss: 0.0088, F1: 0.9987, UW_Acc: 0.9987\n",
      "Val - loss: 1.2621, F1: 0.7181, UW_Acc: 0.7219\n",
      "Epoch [44/80], time: 7.89s\n",
      "Train - loss: 0.0088, F1: 0.9989, UW_Acc: 0.9989\n",
      "Val - loss: 1.2728, F1: 0.7161, UW_Acc: 0.7205\n",
      "Epoch [45/80], time: 7.86s\n",
      "Train - loss: 0.0075, F1: 0.9993, UW_Acc: 0.9993\n",
      "Val - loss: 1.2299, F1: 0.7228, UW_Acc: 0.7258\n",
      "Epoch [46/80], time: 7.83s\n",
      "Train - loss: 0.0074, F1: 0.9992, UW_Acc: 0.9992\n",
      "Val - loss: 1.2474, F1: 0.7163, UW_Acc: 0.7199\n",
      "Epoch [47/80], time: 7.82s\n",
      "Train - loss: 0.0063, F1: 0.9993, UW_Acc: 0.9993\n",
      "Val - loss: 1.2519, F1: 0.7220, UW_Acc: 0.7256\n",
      "Epoch [48/80], time: 7.82s\n",
      "Train - loss: 0.0098, F1: 0.9978, UW_Acc: 0.9978\n",
      "Val - loss: 1.2733, F1: 0.7102, UW_Acc: 0.7151\n",
      "Epoch 00049: reducing learning rate of group 0 to 8.1000e-07.\n",
      "Epoch [49/80], time: 7.86s\n",
      "Train - loss: 0.0057, F1: 1.0000, UW_Acc: 1.0000\n",
      "Val - loss: 1.2647, F1: 0.7150, UW_Acc: 0.7199\n",
      "Epoch [50/80], time: 7.85s\n",
      "Train - loss: 0.0075, F1: 0.9989, UW_Acc: 0.9989\n",
      "Val - loss: 1.2737, F1: 0.7118, UW_Acc: 0.7159\n",
      "Epoch [51/80], time: 7.86s\n",
      "Train - loss: 0.0075, F1: 0.9991, UW_Acc: 0.9991\n",
      "Val - loss: 1.2391, F1: 0.7193, UW_Acc: 0.7233\n",
      "Epoch [52/80], time: 7.82s\n",
      "Train - loss: 0.0069, F1: 0.9993, UW_Acc: 0.9993\n",
      "Val - loss: 1.2504, F1: 0.7200, UW_Acc: 0.7233\n",
      "Epoch [53/80], time: 7.82s\n",
      "Train - loss: 0.0074, F1: 0.9988, UW_Acc: 0.9988\n",
      "Val - loss: 1.2327, F1: 0.7161, UW_Acc: 0.7199\n",
      "Epoch [54/80], time: 7.86s\n",
      "Train - loss: 0.0070, F1: 0.9993, UW_Acc: 0.9993\n",
      "Val - loss: 1.2755, F1: 0.7069, UW_Acc: 0.7121\n",
      "Epoch 00055: reducing learning rate of group 0 to 2.4300e-07.\n",
      "Epoch [55/80], time: 7.82s\n",
      "Train - loss: 0.0073, F1: 0.9995, UW_Acc: 0.9995\n",
      "Val - loss: 1.2348, F1: 0.7231, UW_Acc: 0.7266\n",
      "Epoch [56/80], time: 7.81s\n",
      "Train - loss: 0.0074, F1: 0.9987, UW_Acc: 0.9987\n",
      "Val - loss: 1.2465, F1: 0.7143, UW_Acc: 0.7190\n",
      "Epoch [57/80], time: 7.82s\n",
      "Train - loss: 0.0069, F1: 0.9991, UW_Acc: 0.9991\n",
      "Val - loss: 1.2879, F1: 0.7070, UW_Acc: 0.7120\n",
      "Epoch [58/80], time: 7.84s\n",
      "Train - loss: 0.0059, F1: 0.9996, UW_Acc: 0.9996\n",
      "Val - loss: 1.2353, F1: 0.7182, UW_Acc: 0.7223\n",
      "Epoch [59/80], time: 7.84s\n",
      "Train - loss: 0.0075, F1: 0.9986, UW_Acc: 0.9985\n",
      "Val - loss: 1.2481, F1: 0.7186, UW_Acc: 0.7227\n",
      "Epoch [60/80], time: 7.85s\n",
      "Train - loss: 0.0069, F1: 0.9989, UW_Acc: 0.9989\n",
      "Val - loss: 1.2318, F1: 0.7158, UW_Acc: 0.7202\n",
      "Epoch 00061: reducing learning rate of group 0 to 7.2900e-08.\n",
      "Epoch [61/80], time: 7.92s\n",
      "Train - loss: 0.0083, F1: 0.9984, UW_Acc: 0.9984\n",
      "Val - loss: 1.2601, F1: 0.7204, UW_Acc: 0.7246\n",
      "Epoch [62/80], time: 7.81s\n",
      "Train - loss: 0.0070, F1: 0.9993, UW_Acc: 0.9993\n",
      "Val - loss: 1.2407, F1: 0.7177, UW_Acc: 0.7219\n",
      "Epoch [63/80], time: 7.81s\n",
      "Train - loss: 0.0071, F1: 0.9986, UW_Acc: 0.9985\n",
      "Val - loss: 1.2526, F1: 0.7158, UW_Acc: 0.7203\n",
      "Epoch [64/80], time: 7.87s\n",
      "Train - loss: 0.0064, F1: 0.9994, UW_Acc: 0.9994\n",
      "Val - loss: 1.2401, F1: 0.7209, UW_Acc: 0.7252\n",
      "Epoch [65/80], time: 7.86s\n",
      "Train - loss: 0.0071, F1: 0.9993, UW_Acc: 0.9993\n",
      "Val - loss: 1.2447, F1: 0.7215, UW_Acc: 0.7256\n",
      "Epoch [66/80], time: 9.97s\n",
      "Train - loss: 0.0064, F1: 0.9995, UW_Acc: 0.9995\n",
      "Val - loss: 1.2330, F1: 0.7168, UW_Acc: 0.7209\n",
      "Epoch 00067: reducing learning rate of group 0 to 2.1870e-08.\n",
      "Epoch [67/80], time: 7.85s\n",
      "Train - loss: 0.0081, F1: 0.9986, UW_Acc: 0.9985\n",
      "Val - loss: 1.2659, F1: 0.7105, UW_Acc: 0.7151\n",
      "Epoch [68/80], time: 7.84s\n",
      "Train - loss: 0.0063, F1: 0.9996, UW_Acc: 0.9996\n",
      "Val - loss: 1.2460, F1: 0.7142, UW_Acc: 0.7186\n",
      "Epoch [69/80], time: 7.87s\n",
      "Train - loss: 0.0075, F1: 0.9987, UW_Acc: 0.9987\n",
      "Val - loss: 1.2262, F1: 0.7199, UW_Acc: 0.7238\n",
      "Epoch [70/80], time: 7.84s\n",
      "Train - loss: 0.0068, F1: 0.9991, UW_Acc: 0.9991\n",
      "Val - loss: 1.2595, F1: 0.7203, UW_Acc: 0.7242\n",
      "Epoch [71/80], time: 7.94s\n",
      "Train - loss: 0.0055, F1: 0.9998, UW_Acc: 0.9998\n",
      "Val - loss: 1.2684, F1: 0.7151, UW_Acc: 0.7193\n",
      "Epoch [72/80], time: 7.89s\n",
      "Train - loss: 0.0063, F1: 0.9989, UW_Acc: 0.9989\n",
      "Val - loss: 1.2365, F1: 0.7149, UW_Acc: 0.7190\n",
      "Epoch 00073: reducing learning rate of group 0 to 6.5610e-09.\n",
      "Epoch [73/80], time: 7.84s\n",
      "Train - loss: 0.0059, F1: 0.9996, UW_Acc: 0.9996\n",
      "Val - loss: 1.2394, F1: 0.7160, UW_Acc: 0.7199\n",
      "Epoch [74/80], time: 7.88s\n",
      "Train - loss: 0.0074, F1: 0.9991, UW_Acc: 0.9991\n",
      "Val - loss: 1.2082, F1: 0.7254, UW_Acc: 0.7285\n",
      "Epoch [75/80], time: 7.84s\n",
      "Train - loss: 0.0062, F1: 0.9998, UW_Acc: 0.9998\n",
      "Val - loss: 1.2678, F1: 0.7145, UW_Acc: 0.7190\n",
      "Epoch [76/80], time: 7.86s\n",
      "Train - loss: 0.0063, F1: 0.9995, UW_Acc: 0.9995\n",
      "Val - loss: 1.2340, F1: 0.7212, UW_Acc: 0.7252\n",
      "Epoch [77/80], time: 7.98s\n",
      "Train - loss: 0.0060, F1: 0.9993, UW_Acc: 0.9993\n",
      "Val - loss: 1.2310, F1: 0.7206, UW_Acc: 0.7252\n",
      "Epoch [78/80], time: 7.84s\n",
      "Train - loss: 0.0061, F1: 0.9996, UW_Acc: 0.9996\n",
      "Val - loss: 1.2196, F1: 0.7250, UW_Acc: 0.7281\n",
      "Epoch [79/80], time: 7.84s\n",
      "Train - loss: 0.0066, F1: 0.9989, UW_Acc: 0.9989\n",
      "Val - loss: 1.2688, F1: 0.7073, UW_Acc: 0.7123\n",
      "Epoch [80/80], time: 7.85s\n",
      "Train - loss: 0.0068, F1: 0.9993, UW_Acc: 0.9993\n",
      "Val - loss: 1.2065, F1: 0.7292, UW_Acc: 0.7320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.1400883280953695,\n",
       "  0.8183701468068499,\n",
       "  0.6373799207598664,\n",
       "  0.5260107877642609,\n",
       "  0.4377241952474727,\n",
       "  0.3715539713238561,\n",
       "  0.3140091622291609,\n",
       "  0.27146527586981306,\n",
       "  0.2247300373260365,\n",
       "  0.19808672299218733,\n",
       "  0.16724539322908535,\n",
       "  0.16186780343915141,\n",
       "  0.14570256495891615,\n",
       "  0.12352123665948246,\n",
       "  0.11319833370142204,\n",
       "  0.1101403577729713,\n",
       "  0.09423800774438437,\n",
       "  0.09566593456060388,\n",
       "  0.08530494042260703,\n",
       "  0.07384864487793556,\n",
       "  0.07560433590308178,\n",
       "  0.06419770086054193,\n",
       "  0.0638206223415774,\n",
       "  0.0639672267229058,\n",
       "  0.05755404842107795,\n",
       "  0.061320294431129165,\n",
       "  0.037792531990034635,\n",
       "  0.02482133384707362,\n",
       "  0.020490199500738188,\n",
       "  0.02129561782186461,\n",
       "  0.018252627869938,\n",
       "  0.019371400782188703,\n",
       "  0.016898191923838714,\n",
       "  0.013487059276464373,\n",
       "  0.016634339206787044,\n",
       "  0.01811978286998563,\n",
       "  0.01435749194324883,\n",
       "  0.011512130764204749,\n",
       "  0.01021599393272989,\n",
       "  0.00807534152273695,\n",
       "  0.008785394345258558,\n",
       "  0.00903115774299083,\n",
       "  0.008789011699602355,\n",
       "  0.008768752379732769,\n",
       "  0.007533725675975167,\n",
       "  0.007350918100497057,\n",
       "  0.006333152866424169,\n",
       "  0.009753081157023823,\n",
       "  0.005671816314895486,\n",
       "  0.0074824865116889395,\n",
       "  0.007515660682043364,\n",
       "  0.006878900227981598,\n",
       "  0.007363266354903232,\n",
       "  0.006960286661376094,\n",
       "  0.007304151320483449,\n",
       "  0.007411639447064074,\n",
       "  0.006852120437204491,\n",
       "  0.005894593033549744,\n",
       "  0.007454748124663913,\n",
       "  0.006949537558221194,\n",
       "  0.008272475416793726,\n",
       "  0.00700658249538825,\n",
       "  0.007089724022418607,\n",
       "  0.006414688276235274,\n",
       "  0.007106218094461013,\n",
       "  0.006381240667948543,\n",
       "  0.008080134533757215,\n",
       "  0.006275164063608404,\n",
       "  0.007544732419773936,\n",
       "  0.00683255007403881,\n",
       "  0.005516720096429073,\n",
       "  0.006317835547528122,\n",
       "  0.005875405438547564,\n",
       "  0.007423650191793608,\n",
       "  0.0062458683130179726,\n",
       "  0.006286902927122143,\n",
       "  0.006045978391802934,\n",
       "  0.006137987753094802,\n",
       "  0.006597059491891847,\n",
       "  0.006786179319998726],\n",
       " [0.4806268145755823,\n",
       "  0.6622117558367204,\n",
       "  0.749925168987789,\n",
       "  0.8006736880450589,\n",
       "  0.8416298554074196,\n",
       "  0.8644193313119207,\n",
       "  0.8885404223890658,\n",
       "  0.9069529958377388,\n",
       "  0.9206473836031402,\n",
       "  0.9366437648307584,\n",
       "  0.9470656526195322,\n",
       "  0.9474174689621581,\n",
       "  0.9504937429987873,\n",
       "  0.9625362070126005,\n",
       "  0.9641317256987519,\n",
       "  0.9619737961968131,\n",
       "  0.9691840947719298,\n",
       "  0.9696047441709003,\n",
       "  0.9742772470552864,\n",
       "  0.9766798121252752,\n",
       "  0.978093102373002,\n",
       "  0.981110693143667,\n",
       "  0.981309795899164,\n",
       "  0.9805034601139629,\n",
       "  0.9820599580437183,\n",
       "  0.9794263869006563,\n",
       "  0.9901215960054972,\n",
       "  0.9949152653850709,\n",
       "  0.995383371808724,\n",
       "  0.9952103322479908,\n",
       "  0.9967376561751544,\n",
       "  0.9954785700026232,\n",
       "  0.9955616811706441,\n",
       "  0.9983677566179763,\n",
       "  0.995468254670861,\n",
       "  0.9953918150903454,\n",
       "  0.9964771974049179,\n",
       "  0.9982911726683907,\n",
       "  0.9985477952093869,\n",
       "  0.998912669757744,\n",
       "  0.9983689471993221,\n",
       "  0.9984718529146847,\n",
       "  0.998728998655256,\n",
       "  0.998912832279947,\n",
       "  0.9992724174670734,\n",
       "  0.9991932058222528,\n",
       "  0.9992790439989612,\n",
       "  0.9978205256203548,\n",
       "  1.0,\n",
       "  0.9989130774882967,\n",
       "  0.9990968016174616,\n",
       "  0.9992793150323411,\n",
       "  0.9988289435761037,\n",
       "  0.9992790661702363,\n",
       "  0.9994568592380945,\n",
       "  0.9987334093094984,\n",
       "  0.9990937039197619,\n",
       "  0.9996368465818019,\n",
       "  0.9985509894422655,\n",
       "  0.9989120801686774,\n",
       "  0.9983684192840329,\n",
       "  0.9992749961003738,\n",
       "  0.9985515740259037,\n",
       "  0.9993727827380121,\n",
       "  0.9992768438228922,\n",
       "  0.9994555315694702,\n",
       "  0.9985502593657014,\n",
       "  0.9996370177075486,\n",
       "  0.9987299277479099,\n",
       "  0.9990941508453131,\n",
       "  0.9998187858652975,\n",
       "  0.9989108365832601,\n",
       "  0.9996377620100366,\n",
       "  0.9990930815125163,\n",
       "  0.9998183139534884,\n",
       "  0.9994546221519314,\n",
       "  0.9992755826295122,\n",
       "  0.9996380004827865,\n",
       "  0.9989114955815144,\n",
       "  0.9992753052292396],\n",
       " [0.4936079545454546,\n",
       "  0.665449920718816,\n",
       "  0.7496696617336152,\n",
       "  0.8008720930232558,\n",
       "  0.8411403276955604,\n",
       "  0.8641649048625792,\n",
       "  0.8879822938689217,\n",
       "  0.906596855179704,\n",
       "  0.9205040961945031,\n",
       "  0.9363768498942918,\n",
       "  0.9469311575052854,\n",
       "  0.9471954281183932,\n",
       "  0.9502510570824524,\n",
       "  0.9625066067653277,\n",
       "  0.9640757135306554,\n",
       "  0.9620606501057082,\n",
       "  0.9691133720930233,\n",
       "  0.9696584302325582,\n",
       "  0.9741345137420719,\n",
       "  0.9766781183932347,\n",
       "  0.9780490221987315,\n",
       "  0.9811211680761099,\n",
       "  0.9813028541226215,\n",
       "  0.9804770084566597,\n",
       "  0.9820295983086681,\n",
       "  0.9793868921775899,\n",
       "  0.990106368921776,\n",
       "  0.9949127906976745,\n",
       "  0.9953752642706132,\n",
       "  0.9951935782241016,\n",
       "  0.9967296511627907,\n",
       "  0.9954578488372093,\n",
       "  0.9955569503171248,\n",
       "  0.9983648255813954,\n",
       "  0.9954578488372093,\n",
       "  0.9953752642706132,\n",
       "  0.9964653805496829,\n",
       "  0.9982822410147992,\n",
       "  0.998546511627907,\n",
       "  0.9989098837209303,\n",
       "  0.9983648255813954,\n",
       "  0.9984639270613108,\n",
       "  0.9987281976744186,\n",
       "  0.9989098837209303,\n",
       "  0.9992732558139535,\n",
       "  0.9991906712473574,\n",
       "  0.9992732558139535,\n",
       "  0.9978197674418605,\n",
       "  1.0,\n",
       "  0.9989098837209303,\n",
       "  0.9990915697674418,\n",
       "  0.9992732558139535,\n",
       "  0.9988272991543341,\n",
       "  0.9992732558139535,\n",
       "  0.9994549418604651,\n",
       "  0.9987281976744186,\n",
       "  0.9990915697674418,\n",
       "  0.9996366279069767,\n",
       "  0.998546511627907,\n",
       "  0.9989098837209303,\n",
       "  0.9983648255813954,\n",
       "  0.9992732558139535,\n",
       "  0.998546511627907,\n",
       "  0.999372357293869,\n",
       "  0.9992732558139535,\n",
       "  0.9994549418604651,\n",
       "  0.998546511627907,\n",
       "  0.9996366279069767,\n",
       "  0.9987281976744186,\n",
       "  0.9990915697674418,\n",
       "  0.9998183139534884,\n",
       "  0.9989098837209303,\n",
       "  0.9996366279069767,\n",
       "  0.9990915697674418,\n",
       "  0.9998183139534884,\n",
       "  0.9994549418604651,\n",
       "  0.9992732558139535,\n",
       "  0.9996366279069767,\n",
       "  0.9989098837209303,\n",
       "  0.9992732558139535],\n",
       " [1.277798816561699,\n",
       "  0.967506505548954,\n",
       "  0.9888699799776077,\n",
       "  0.9680721312761307,\n",
       "  0.9709985256195068,\n",
       "  0.8443527892231941,\n",
       "  1.1307820975780487,\n",
       "  0.8906800299882889,\n",
       "  1.0555331483483315,\n",
       "  0.9911539852619171,\n",
       "  1.110959142446518,\n",
       "  1.0846791714429855,\n",
       "  1.1506534069776535,\n",
       "  0.9557577595114708,\n",
       "  1.0006589069962502,\n",
       "  1.0230493023991585,\n",
       "  1.0644300356507301,\n",
       "  1.382818564772606,\n",
       "  1.0953354015946388,\n",
       "  1.1005071625113487,\n",
       "  1.281387060880661,\n",
       "  1.4771538227796555,\n",
       "  1.1326123476028442,\n",
       "  1.1062997877597809,\n",
       "  1.2460183054208755,\n",
       "  1.2387733086943626,\n",
       "  1.2693493068218231,\n",
       "  1.2561554536223412,\n",
       "  1.1833492070436478,\n",
       "  1.140761837363243,\n",
       "  1.170280858874321,\n",
       "  1.2873128205537796,\n",
       "  1.2011091262102127,\n",
       "  1.2125270813703537,\n",
       "  1.3054053783416748,\n",
       "  1.2352580353617668,\n",
       "  1.256139114499092,\n",
       "  1.247316412627697,\n",
       "  1.253475770354271,\n",
       "  1.235549807548523,\n",
       "  1.2375455275177956,\n",
       "  1.266648456454277,\n",
       "  1.2620965242385864,\n",
       "  1.272826299071312,\n",
       "  1.2298750951886177,\n",
       "  1.247400313615799,\n",
       "  1.2518573254346848,\n",
       "  1.273305520415306,\n",
       "  1.2646871879696846,\n",
       "  1.2737073078751564,\n",
       "  1.239109568297863,\n",
       "  1.2503802999854088,\n",
       "  1.2326534688472748,\n",
       "  1.2754617631435394,\n",
       "  1.2348377853631973,\n",
       "  1.246485896408558,\n",
       "  1.2878768518567085,\n",
       "  1.2352981567382812,\n",
       "  1.2480559423565865,\n",
       "  1.2317909747362137,\n",
       "  1.2600807920098305,\n",
       "  1.24071204662323,\n",
       "  1.2526025250554085,\n",
       "  1.2400886714458466,\n",
       "  1.2446817085146904,\n",
       "  1.2329671829938889,\n",
       "  1.2658725157380104,\n",
       "  1.2459994480013847,\n",
       "  1.226197600364685,\n",
       "  1.259495109319687,\n",
       "  1.2684382870793343,\n",
       "  1.236491322517395,\n",
       "  1.2393827885389328,\n",
       "  1.2082132324576378,\n",
       "  1.2677665576338768,\n",
       "  1.234014853835106,\n",
       "  1.2309677675366402,\n",
       "  1.2196232602000237,\n",
       "  1.2688074931502342,\n",
       "  1.206509180366993],\n",
       " [0.4385807090988794,\n",
       "  0.5729664451130061,\n",
       "  0.6105588227536886,\n",
       "  0.6284883616660252,\n",
       "  0.6368947634105143,\n",
       "  0.6797712608358344,\n",
       "  0.6330243198848535,\n",
       "  0.6974694902097693,\n",
       "  0.6675621855605444,\n",
       "  0.6847831619872659,\n",
       "  0.6411501005386032,\n",
       "  0.6642064879406783,\n",
       "  0.6541466591994433,\n",
       "  0.7136130755533944,\n",
       "  0.7126224781048726,\n",
       "  0.7069705785488358,\n",
       "  0.7052648710661503,\n",
       "  0.6471660859583167,\n",
       "  0.688095693302202,\n",
       "  0.7183397670355812,\n",
       "  0.68754704225567,\n",
       "  0.6371472679532609,\n",
       "  0.7015143596761244,\n",
       "  0.7089839892494325,\n",
       "  0.7035981929033261,\n",
       "  0.6874549308141904,\n",
       "  0.6744412455116388,\n",
       "  0.6939679130575823,\n",
       "  0.7065773572859101,\n",
       "  0.7296423799821192,\n",
       "  0.7200170238435009,\n",
       "  0.6954947639019996,\n",
       "  0.712960975962976,\n",
       "  0.7269507174888746,\n",
       "  0.7105554904803889,\n",
       "  0.7099328253346905,\n",
       "  0.7298460868355037,\n",
       "  0.7212655185259306,\n",
       "  0.7175263184349844,\n",
       "  0.7172408099222187,\n",
       "  0.712181936990699,\n",
       "  0.711048387583219,\n",
       "  0.7181298223003026,\n",
       "  0.7161179994822864,\n",
       "  0.7228401282920435,\n",
       "  0.716346113539365,\n",
       "  0.7220062767868594,\n",
       "  0.7102426280304561,\n",
       "  0.7149574572161144,\n",
       "  0.7117953647649281,\n",
       "  0.7193020337717122,\n",
       "  0.7200118660361544,\n",
       "  0.7161180486951219,\n",
       "  0.7068824523029468,\n",
       "  0.7231328892344941,\n",
       "  0.714342051675862,\n",
       "  0.7069893574099686,\n",
       "  0.7181760693627633,\n",
       "  0.7186140221192757,\n",
       "  0.7157557137732029,\n",
       "  0.7204346681045535,\n",
       "  0.717715630282699,\n",
       "  0.7158042729648593,\n",
       "  0.7209071432714509,\n",
       "  0.7214770930061299,\n",
       "  0.7168276523521241,\n",
       "  0.7105157416281156,\n",
       "  0.7142229762103534,\n",
       "  0.7199331424873097,\n",
       "  0.7202707703457553,\n",
       "  0.7151390856769398,\n",
       "  0.7148993872059614,\n",
       "  0.7159938136126064,\n",
       "  0.7253789082470171,\n",
       "  0.7145036839838949,\n",
       "  0.7211918689068483,\n",
       "  0.7206446279963871,\n",
       "  0.724982938638643,\n",
       "  0.7072765749111471,\n",
       "  0.7291720048682352],\n",
       " [0.45805027173913043,\n",
       "  0.5837296195652174,\n",
       "  0.6167204483695652,\n",
       "  0.6416864809782609,\n",
       "  0.6525985054347826,\n",
       "  0.6858016304347826,\n",
       "  0.6436396059782609,\n",
       "  0.7021908967391304,\n",
       "  0.6779891304347826,\n",
       "  0.6883491847826086,\n",
       "  0.6508576766304348,\n",
       "  0.6719599184782609,\n",
       "  0.6670771059782609,\n",
       "  0.716414741847826,\n",
       "  0.7156504755434783,\n",
       "  0.708984375,\n",
       "  0.7091966711956522,\n",
       "  0.6553583559782609,\n",
       "  0.6953549592391304,\n",
       "  0.7216796875,\n",
       "  0.6873301630434783,\n",
       "  0.6520889945652174,\n",
       "  0.7091966711956522,\n",
       "  0.7127207880434783,\n",
       "  0.7051205842391304,\n",
       "  0.6908967391304348,\n",
       "  0.6846552309782609,\n",
       "  0.7004076086956522,\n",
       "  0.7121263586956522,\n",
       "  0.732804008152174,\n",
       "  0.724227241847826,\n",
       "  0.7041440217391304,\n",
       "  0.7176036005434783,\n",
       "  0.7297044836956522,\n",
       "  0.7154806385869565,\n",
       "  0.7136973505434783,\n",
       "  0.733016304347826,\n",
       "  0.726180366847826,\n",
       "  0.722274116847826,\n",
       "  0.721297554347826,\n",
       "  0.716414741847826,\n",
       "  0.7162449048913043,\n",
       "  0.7218919836956522,\n",
       "  0.7205332880434783,\n",
       "  0.7257982336956522,\n",
       "  0.7199388586956522,\n",
       "  0.7255859375,\n",
       "  0.7150560461956522,\n",
       "  0.7199388586956522,\n",
       "  0.7158627717391304,\n",
       "  0.723250679347826,\n",
       "  0.723250679347826,\n",
       "  0.7199388586956522,\n",
       "  0.7121263586956522,\n",
       "  0.7265625,\n",
       "  0.7189622961956522,\n",
       "  0.7119565217391304,\n",
       "  0.722274116847826,\n",
       "  0.72265625,\n",
       "  0.7201511548913043,\n",
       "  0.724609375,\n",
       "  0.7218919836956522,\n",
       "  0.720320991847826,\n",
       "  0.725203804347826,\n",
       "  0.7255859375,\n",
       "  0.7209154211956522,\n",
       "  0.7150560461956522,\n",
       "  0.7185801630434783,\n",
       "  0.7238451086956522,\n",
       "  0.724227241847826,\n",
       "  0.719344429347826,\n",
       "  0.7189622961956522,\n",
       "  0.7199388586956522,\n",
       "  0.728515625,\n",
       "  0.7189622961956522,\n",
       "  0.725203804347826,\n",
       "  0.725203804347826,\n",
       "  0.728133491847826,\n",
       "  0.7123386548913043,\n",
       "  0.732039741847826])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# 优化器\n",
    "optimizer = torch.optim.AdamW(model_combined.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "\n",
    "# 学习率调度器\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.3, patience=5, verbose=True)\n",
    "\n",
    "# 调用训练函数进行训练\n",
    "train_model(model_combined, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=80, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac90288",
   "metadata": {
    "papermill": {
     "duration": 0.036713,
     "end_time": "2024-09-18T17:46:42.019373",
     "exception": false,
     "start_time": "2024-09-18T17:46:41.982660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model-8: Self-cross attention Text+Speech+Mocap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86a01313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:46:42.092603Z",
     "iopub.status.busy": "2024-09-18T17:46:42.092271Z",
     "iopub.status.idle": "2024-09-18T17:46:42.146147Z",
     "shell.execute_reply": "2024-09-18T17:46:42.145273Z"
    },
    "papermill": {
     "duration": 0.092858,
     "end_time": "2024-09-18T17:46:42.148045",
     "exception": false,
     "start_time": "2024-09-18T17:46:42.055187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadCrossAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = query_dim // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == query_dim\n",
    "        ), \"Query dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.query = nn.Linear(query_dim, query_dim)\n",
    "        self.key = nn.Linear(key_dim, query_dim)\n",
    "        self.value = nn.Linear(key_dim, query_dim)\n",
    "        self.fc_out = nn.Linear(query_dim, query_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(query_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(query_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.feed_forward = FeedForward(query_dim, query_dim * 4, dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        N, query_len, _ = query.shape\n",
    "        _, key_len, _ = key.shape\n",
    "\n",
    "        residual = query\n",
    "\n",
    "        queries = self.query(query).view(N, query_len, self.num_heads, self.head_dim)\n",
    "        keys = self.key(key).view(N, key_len, self.num_heads, self.head_dim)\n",
    "        values = self.value(value).view(N, key_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        energy = torch.matmul(queries, keys.transpose(-1, -2)) / (self.head_dim ** (1 / 2))\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.transpose(1, 2).contiguous().view(N, query_len, -1)\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer_norm1(out + residual)\n",
    "\n",
    "        ff_out = self.feed_forward(out)\n",
    "        out = self.layer_norm2(ff_out + out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_dim\n",
    "        ), \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.feed_forward = FeedForward(embed_dim, embed_dim * 4, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_length, embed_dim = x.shape\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        queries = self.query(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "        keys = self.key(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "        values = self.value(x).view(N, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        energy = torch.matmul(queries, keys.transpose(-1, -2)) / (self.head_dim ** (1 / 2))\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.transpose(1, 2).contiguous().view(N, seq_length, self.embed_dim)\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer_norm1(out + residual)\n",
    "\n",
    "        ff_out = self.feed_forward(out)\n",
    "        out = self.layer_norm2(ff_out + out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class TextModel(nn.Module):\n",
    "    def __init__(self, nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads=num_heads):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(nb_words, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(g_word_embedding_matrix.clone().detach())\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.conv2 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim=32, num_heads=num_heads)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(32 * max_sequence_length, 256)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.bn5(self.dense(x))\n",
    "        return x\n",
    "\n",
    "class SpeechModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.1, num_heads=num_heads):\n",
    "        super(SpeechModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(100 * 34, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim=1024, num_heads=num_heads)\n",
    "        self.dense2 = nn.Linear(1024, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.bn1(self.dense1(x)))\n",
    "        x = self.attention(x.unsqueeze(1)).squeeze(1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn2(self.dense2(x))\n",
    "        return x\n",
    "\n",
    "class MocapModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MocapModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(256 * 7 * 6, 256)\n",
    "        self.bn8 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.bn8(self.dense(x))\n",
    "        return x\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads=num_heads):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.text_model = TextModel(nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads=num_heads)\n",
    "        self.speech_model = SpeechModel(num_heads=num_heads)\n",
    "        self.mocap_model = MocapModel()\n",
    "        \n",
    "        self.text_speech_attention = MultiHeadCrossAttention(256, 256, num_heads)\n",
    "        self.text_mocap_attention = MultiHeadCrossAttention(256, 256, num_heads)\n",
    "        \n",
    "        self.final_attention = MultiHeadSelfAttention(256, num_heads)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 4)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, text, speech, mocap):\n",
    "        text_out = self.text_model(text)\n",
    "        speech_out = self.speech_model(speech)\n",
    "        mocap_out = self.mocap_model(mocap)\n",
    "        \n",
    "        text_speech_attended = self.text_speech_attention(text_out.unsqueeze(1), speech_out.unsqueeze(1), speech_out.unsqueeze(1)).squeeze(1)\n",
    "        text_mocap_attended = self.text_mocap_attention(text_out.unsqueeze(1), mocap_out.unsqueeze(1), mocap_out.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        combined = torch.stack([text_speech_attended, text_mocap_attended, text_out], dim=1)\n",
    "        \n",
    "        combined = self.final_attention(combined)\n",
    "        combined = combined.mean(dim=1)  # Average pooling across the sequence dimension\n",
    "        \n",
    "        combined = F.relu(self.bn1(self.fc1(combined)))\n",
    "        combined = self.dropout(combined)\n",
    "        combined = self.fc2(combined)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7353e2ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:46:42.219876Z",
     "iopub.status.busy": "2024-09-18T17:46:42.219553Z",
     "iopub.status.idle": "2024-09-18T17:46:42.492701Z",
     "shell.execute_reply": "2024-09-18T17:46:42.491757Z"
    },
    "papermill": {
     "duration": 0.312743,
     "end_time": "2024-09-18T17:46:42.496221",
     "exception": false,
     "start_time": "2024-09-18T17:46:42.183478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CombinedModel(\n",
      "  (text_model): TextModel(\n",
      "    (embedding): Embedding(3130, 300)\n",
      "    (conv1): Conv1d(300, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadSelfAttention(\n",
      "      (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=16000, out_features=256, bias=True)\n",
      "    (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (speech_model): SpeechModel(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=3400, out_features=1024, bias=True)\n",
      "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (mocap_model): MocapModel(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=10752, out_features=256, bias=True)\n",
      "    (bn8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (text_speech_attention): MultiHeadCrossAttention(\n",
      "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (feed_forward): FeedForward(\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (text_mocap_attention): MultiHeadCrossAttention(\n",
      "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (feed_forward): FeedForward(\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_attention): MultiHeadSelfAttention(\n",
      "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (feed_forward): FeedForward(\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Combined Model Summary:\n",
      "Model Structure:\n",
      "CombinedModel(\n",
      "  (text_model): TextModel(\n",
      "    (embedding): Embedding(3130, 300)\n",
      "    (conv1): Conv1d(300, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadSelfAttention(\n",
      "      (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (fc_out): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=16000, out_features=256, bias=True)\n",
      "    (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (speech_model): SpeechModel(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=3400, out_features=1024, bias=True)\n",
      "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (mocap_model): MocapModel(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=10752, out_features=256, bias=True)\n",
      "    (bn8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (text_speech_attention): MultiHeadCrossAttention(\n",
      "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (feed_forward): FeedForward(\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (text_mocap_attention): MultiHeadCrossAttention(\n",
      "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (feed_forward): FeedForward(\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_attention): MultiHeadSelfAttention(\n",
      "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (feed_forward): FeedForward(\n",
      "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Input size: [(500,), (100, 34), (200, 189, 1)]\n",
      "text_model: 5,409,336 parameters\n",
      "  embedding: 939,000 parameters\n",
      "  conv1: 230,656 parameters\n",
      "  bn1: 512 parameters\n",
      "  conv2: 98,432 parameters\n",
      "  bn2: 256 parameters\n",
      "  conv3: 24,640 parameters\n",
      "  bn3: 128 parameters\n",
      "  conv4: 6,176 parameters\n",
      "  bn4: 64 parameters\n",
      "  attention: 12,704 parameters\n",
      "  dropout: 0 parameters\n",
      "  flatten: 0 parameters\n",
      "  dense: 4,096,256 parameters\n",
      "  bn5: 512 parameters\n",
      "speech_model: 16,343,808 parameters\n",
      "  flatten: 0 parameters\n",
      "  dense1: 3,482,624 parameters\n",
      "  bn1: 2,048 parameters\n",
      "  attention: 12,596,224 parameters\n",
      "  dense2: 262,400 parameters\n",
      "  bn2: 512 parameters\n",
      "  dropout: 0 parameters\n",
      "mocap_model: 3,179,136 parameters\n",
      "  conv1: 320 parameters\n",
      "  bn1: 64 parameters\n",
      "  conv2: 18,496 parameters\n",
      "  bn2: 128 parameters\n",
      "  conv3: 36,928 parameters\n",
      "  bn3: 128 parameters\n",
      "  conv4: 73,856 parameters\n",
      "  bn4: 256 parameters\n",
      "  conv5: 295,168 parameters\n",
      "  bn5: 512 parameters\n",
      "  dropout: 0 parameters\n",
      "  flatten: 0 parameters\n",
      "  dense: 2,752,768 parameters\n",
      "  bn8: 512 parameters\n",
      "text_speech_attention: 789,760 parameters\n",
      "  query: 65,792 parameters\n",
      "  key: 65,792 parameters\n",
      "  value: 65,792 parameters\n",
      "  fc_out: 65,792 parameters\n",
      "  layer_norm1: 512 parameters\n",
      "  layer_norm2: 512 parameters\n",
      "  dropout: 0 parameters\n",
      "  feed_forward: 525,568 parameters\n",
      "text_mocap_attention: 789,760 parameters\n",
      "  query: 65,792 parameters\n",
      "  key: 65,792 parameters\n",
      "  value: 65,792 parameters\n",
      "  fc_out: 65,792 parameters\n",
      "  layer_norm1: 512 parameters\n",
      "  layer_norm2: 512 parameters\n",
      "  dropout: 0 parameters\n",
      "  feed_forward: 525,568 parameters\n",
      "final_attention: 789,760 parameters\n",
      "  query: 65,792 parameters\n",
      "  key: 65,792 parameters\n",
      "  value: 65,792 parameters\n",
      "  fc_out: 65,792 parameters\n",
      "  layer_norm1: 512 parameters\n",
      "  layer_norm2: 512 parameters\n",
      "  dropout: 0 parameters\n",
      "  feed_forward: 525,568 parameters\n",
      "fc1: 32,896 parameters\n",
      "bn1: 256 parameters\n",
      "fc2: 516 parameters\n",
      "dropout: 0 parameters\n",
      "\n",
      "Total trainable parameters: 27,335,228\n",
      "\n",
      "Detailed parameter shapes:\n",
      "text_model.embedding.weight: torch.Size([3130, 300])\n",
      "text_model.conv1.weight: torch.Size([256, 300, 3])\n",
      "text_model.conv1.bias: torch.Size([256])\n",
      "text_model.bn1.weight: torch.Size([256])\n",
      "text_model.bn1.bias: torch.Size([256])\n",
      "text_model.conv2.weight: torch.Size([128, 256, 3])\n",
      "text_model.conv2.bias: torch.Size([128])\n",
      "text_model.bn2.weight: torch.Size([128])\n",
      "text_model.bn2.bias: torch.Size([128])\n",
      "text_model.conv3.weight: torch.Size([64, 128, 3])\n",
      "text_model.conv3.bias: torch.Size([64])\n",
      "text_model.bn3.weight: torch.Size([64])\n",
      "text_model.bn3.bias: torch.Size([64])\n",
      "text_model.conv4.weight: torch.Size([32, 64, 3])\n",
      "text_model.conv4.bias: torch.Size([32])\n",
      "text_model.bn4.weight: torch.Size([32])\n",
      "text_model.bn4.bias: torch.Size([32])\n",
      "text_model.attention.query.weight: torch.Size([32, 32])\n",
      "text_model.attention.query.bias: torch.Size([32])\n",
      "text_model.attention.key.weight: torch.Size([32, 32])\n",
      "text_model.attention.key.bias: torch.Size([32])\n",
      "text_model.attention.value.weight: torch.Size([32, 32])\n",
      "text_model.attention.value.bias: torch.Size([32])\n",
      "text_model.attention.fc_out.weight: torch.Size([32, 32])\n",
      "text_model.attention.fc_out.bias: torch.Size([32])\n",
      "text_model.attention.layer_norm1.weight: torch.Size([32])\n",
      "text_model.attention.layer_norm1.bias: torch.Size([32])\n",
      "text_model.attention.layer_norm2.weight: torch.Size([32])\n",
      "text_model.attention.layer_norm2.bias: torch.Size([32])\n",
      "text_model.attention.feed_forward.linear1.weight: torch.Size([128, 32])\n",
      "text_model.attention.feed_forward.linear1.bias: torch.Size([128])\n",
      "text_model.attention.feed_forward.linear2.weight: torch.Size([32, 128])\n",
      "text_model.attention.feed_forward.linear2.bias: torch.Size([32])\n",
      "text_model.dense.weight: torch.Size([256, 16000])\n",
      "text_model.dense.bias: torch.Size([256])\n",
      "text_model.bn5.weight: torch.Size([256])\n",
      "text_model.bn5.bias: torch.Size([256])\n",
      "speech_model.dense1.weight: torch.Size([1024, 3400])\n",
      "speech_model.dense1.bias: torch.Size([1024])\n",
      "speech_model.bn1.weight: torch.Size([1024])\n",
      "speech_model.bn1.bias: torch.Size([1024])\n",
      "speech_model.attention.query.weight: torch.Size([1024, 1024])\n",
      "speech_model.attention.query.bias: torch.Size([1024])\n",
      "speech_model.attention.key.weight: torch.Size([1024, 1024])\n",
      "speech_model.attention.key.bias: torch.Size([1024])\n",
      "speech_model.attention.value.weight: torch.Size([1024, 1024])\n",
      "speech_model.attention.value.bias: torch.Size([1024])\n",
      "speech_model.attention.fc_out.weight: torch.Size([1024, 1024])\n",
      "speech_model.attention.fc_out.bias: torch.Size([1024])\n",
      "speech_model.attention.layer_norm1.weight: torch.Size([1024])\n",
      "speech_model.attention.layer_norm1.bias: torch.Size([1024])\n",
      "speech_model.attention.layer_norm2.weight: torch.Size([1024])\n",
      "speech_model.attention.layer_norm2.bias: torch.Size([1024])\n",
      "speech_model.attention.feed_forward.linear1.weight: torch.Size([4096, 1024])\n",
      "speech_model.attention.feed_forward.linear1.bias: torch.Size([4096])\n",
      "speech_model.attention.feed_forward.linear2.weight: torch.Size([1024, 4096])\n",
      "speech_model.attention.feed_forward.linear2.bias: torch.Size([1024])\n",
      "speech_model.dense2.weight: torch.Size([256, 1024])\n",
      "speech_model.dense2.bias: torch.Size([256])\n",
      "speech_model.bn2.weight: torch.Size([256])\n",
      "speech_model.bn2.bias: torch.Size([256])\n",
      "mocap_model.conv1.weight: torch.Size([32, 1, 3, 3])\n",
      "mocap_model.conv1.bias: torch.Size([32])\n",
      "mocap_model.bn1.weight: torch.Size([32])\n",
      "mocap_model.bn1.bias: torch.Size([32])\n",
      "mocap_model.conv2.weight: torch.Size([64, 32, 3, 3])\n",
      "mocap_model.conv2.bias: torch.Size([64])\n",
      "mocap_model.bn2.weight: torch.Size([64])\n",
      "mocap_model.bn2.bias: torch.Size([64])\n",
      "mocap_model.conv3.weight: torch.Size([64, 64, 3, 3])\n",
      "mocap_model.conv3.bias: torch.Size([64])\n",
      "mocap_model.bn3.weight: torch.Size([64])\n",
      "mocap_model.bn3.bias: torch.Size([64])\n",
      "mocap_model.conv4.weight: torch.Size([128, 64, 3, 3])\n",
      "mocap_model.conv4.bias: torch.Size([128])\n",
      "mocap_model.bn4.weight: torch.Size([128])\n",
      "mocap_model.bn4.bias: torch.Size([128])\n",
      "mocap_model.conv5.weight: torch.Size([256, 128, 3, 3])\n",
      "mocap_model.conv5.bias: torch.Size([256])\n",
      "mocap_model.bn5.weight: torch.Size([256])\n",
      "mocap_model.bn5.bias: torch.Size([256])\n",
      "mocap_model.dense.weight: torch.Size([256, 10752])\n",
      "mocap_model.dense.bias: torch.Size([256])\n",
      "mocap_model.bn8.weight: torch.Size([256])\n",
      "mocap_model.bn8.bias: torch.Size([256])\n",
      "text_speech_attention.query.weight: torch.Size([256, 256])\n",
      "text_speech_attention.query.bias: torch.Size([256])\n",
      "text_speech_attention.key.weight: torch.Size([256, 256])\n",
      "text_speech_attention.key.bias: torch.Size([256])\n",
      "text_speech_attention.value.weight: torch.Size([256, 256])\n",
      "text_speech_attention.value.bias: torch.Size([256])\n",
      "text_speech_attention.fc_out.weight: torch.Size([256, 256])\n",
      "text_speech_attention.fc_out.bias: torch.Size([256])\n",
      "text_speech_attention.layer_norm1.weight: torch.Size([256])\n",
      "text_speech_attention.layer_norm1.bias: torch.Size([256])\n",
      "text_speech_attention.layer_norm2.weight: torch.Size([256])\n",
      "text_speech_attention.layer_norm2.bias: torch.Size([256])\n",
      "text_speech_attention.feed_forward.linear1.weight: torch.Size([1024, 256])\n",
      "text_speech_attention.feed_forward.linear1.bias: torch.Size([1024])\n",
      "text_speech_attention.feed_forward.linear2.weight: torch.Size([256, 1024])\n",
      "text_speech_attention.feed_forward.linear2.bias: torch.Size([256])\n",
      "text_mocap_attention.query.weight: torch.Size([256, 256])\n",
      "text_mocap_attention.query.bias: torch.Size([256])\n",
      "text_mocap_attention.key.weight: torch.Size([256, 256])\n",
      "text_mocap_attention.key.bias: torch.Size([256])\n",
      "text_mocap_attention.value.weight: torch.Size([256, 256])\n",
      "text_mocap_attention.value.bias: torch.Size([256])\n",
      "text_mocap_attention.fc_out.weight: torch.Size([256, 256])\n",
      "text_mocap_attention.fc_out.bias: torch.Size([256])\n",
      "text_mocap_attention.layer_norm1.weight: torch.Size([256])\n",
      "text_mocap_attention.layer_norm1.bias: torch.Size([256])\n",
      "text_mocap_attention.layer_norm2.weight: torch.Size([256])\n",
      "text_mocap_attention.layer_norm2.bias: torch.Size([256])\n",
      "text_mocap_attention.feed_forward.linear1.weight: torch.Size([1024, 256])\n",
      "text_mocap_attention.feed_forward.linear1.bias: torch.Size([1024])\n",
      "text_mocap_attention.feed_forward.linear2.weight: torch.Size([256, 1024])\n",
      "text_mocap_attention.feed_forward.linear2.bias: torch.Size([256])\n",
      "final_attention.query.weight: torch.Size([256, 256])\n",
      "final_attention.query.bias: torch.Size([256])\n",
      "final_attention.key.weight: torch.Size([256, 256])\n",
      "final_attention.key.bias: torch.Size([256])\n",
      "final_attention.value.weight: torch.Size([256, 256])\n",
      "final_attention.value.bias: torch.Size([256])\n",
      "final_attention.fc_out.weight: torch.Size([256, 256])\n",
      "final_attention.fc_out.bias: torch.Size([256])\n",
      "final_attention.layer_norm1.weight: torch.Size([256])\n",
      "final_attention.layer_norm1.bias: torch.Size([256])\n",
      "final_attention.layer_norm2.weight: torch.Size([256])\n",
      "final_attention.layer_norm2.bias: torch.Size([256])\n",
      "final_attention.feed_forward.linear1.weight: torch.Size([1024, 256])\n",
      "final_attention.feed_forward.linear1.bias: torch.Size([1024])\n",
      "final_attention.feed_forward.linear2.weight: torch.Size([256, 1024])\n",
      "final_attention.feed_forward.linear2.bias: torch.Size([256])\n",
      "fc1.weight: torch.Size([128, 256])\n",
      "fc1.bias: torch.Size([128])\n",
      "bn1.weight: torch.Size([128])\n",
      "bn1.bias: torch.Size([128])\n",
      "fc2.weight: torch.Size([4, 128])\n",
      "fc2.bias: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# 设置设备为GPU或CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 检查 class_weights 是否已经在正确的设备上\n",
    "if not isinstance(class_weights, torch.Tensor):\n",
    "    class_weights = torch.FloatTensor(class_weights)\n",
    "\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# 模型实例化\n",
    "model_combined = CombinedModel(nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads)\n",
    "\n",
    "# 打印模型摘要\n",
    "print(model_combined)\n",
    "\n",
    "print(\"Combined Model Summary:\")\n",
    "print_model_summary(model_combined, [(max_sequence_length,), (100, 34), (200, 189, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5784bf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:46:42.572716Z",
     "iopub.status.busy": "2024-09-18T17:46:42.572410Z",
     "iopub.status.idle": "2024-09-18T17:46:44.018749Z",
     "shell.execute_reply": "2024-09-18T17:46:44.017583Z"
    },
    "papermill": {
     "duration": 1.487739,
     "end_time": "2024-09-18T17:46:44.021105",
     "exception": false,
     "start_time": "2024-09-18T17:46:42.533366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "torch.Size([128, 4])\n"
     ]
    }
   ],
   "source": [
    "# 使用DataParallel包装模型\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model_combined = nn.DataParallel(model_combined)\n",
    "\n",
    "# 将模型移动到设备上\n",
    "model_combined.to(device)\n",
    "\n",
    "# 假设我们有一个batch\n",
    "batch = next(iter(train_loader))\n",
    "text = batch['text'].to(device).long()\n",
    "speech = batch['speech'].to(device)\n",
    "mocap = batch['mocap'].to(device)\n",
    "\n",
    "# 前向传播\n",
    "outputs = model_combined(text, speech, mocap)\n",
    "\n",
    "# 打印输出形状\n",
    "print(outputs.shape)  # 打印模型输出的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "331864ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:46:44.101300Z",
     "iopub.status.busy": "2024-09-18T17:46:44.100960Z",
     "iopub.status.idle": "2024-09-18T17:59:13.724722Z",
     "shell.execute_reply": "2024-09-18T17:59:13.723569Z"
    },
    "papermill": {
     "duration": 749.706237,
     "end_time": "2024-09-18T17:59:13.769001",
     "exception": false,
     "start_time": "2024-09-18T17:46:44.062764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], time: 7.81s\n",
      "Train - loss: 1.2983, F1: 0.3625, UW_Acc: 0.3890\n",
      "Val - loss: 1.1388, F1: 0.5254, UW_Acc: 0.5262\n",
      "Epoch [2/80], time: 7.82s\n",
      "Train - loss: 0.9086, F1: 0.6355, UW_Acc: 0.6417\n",
      "Val - loss: 1.0061, F1: 0.5722, UW_Acc: 0.5918\n",
      "Epoch [3/80], time: 7.79s\n",
      "Train - loss: 0.6949, F1: 0.7342, UW_Acc: 0.7333\n",
      "Val - loss: 0.9920, F1: 0.5926, UW_Acc: 0.5980\n",
      "Epoch [4/80], time: 7.85s\n",
      "Train - loss: 0.5886, F1: 0.7775, UW_Acc: 0.7775\n",
      "Val - loss: 1.1453, F1: 0.5475, UW_Acc: 0.5792\n",
      "Epoch [5/80], time: 7.77s\n",
      "Train - loss: 0.4979, F1: 0.8244, UW_Acc: 0.8238\n",
      "Val - loss: 1.1919, F1: 0.5478, UW_Acc: 0.5818\n",
      "Epoch [6/80], time: 7.78s\n",
      "Train - loss: 0.4142, F1: 0.8528, UW_Acc: 0.8525\n",
      "Val - loss: 0.9200, F1: 0.6418, UW_Acc: 0.6546\n",
      "Epoch [7/80], time: 7.94s\n",
      "Train - loss: 0.3698, F1: 0.8674, UW_Acc: 0.8673\n",
      "Val - loss: 1.0046, F1: 0.6301, UW_Acc: 0.6394\n",
      "Epoch [8/80], time: 7.80s\n",
      "Train - loss: 0.3273, F1: 0.8885, UW_Acc: 0.8882\n",
      "Val - loss: 1.0805, F1: 0.6114, UW_Acc: 0.6282\n",
      "Epoch [9/80], time: 7.82s\n",
      "Train - loss: 0.2950, F1: 0.8993, UW_Acc: 0.8991\n",
      "Val - loss: 1.1863, F1: 0.5739, UW_Acc: 0.5951\n",
      "Epoch [10/80], time: 7.77s\n",
      "Train - loss: 0.2638, F1: 0.9116, UW_Acc: 0.9114\n",
      "Val - loss: 1.0551, F1: 0.6315, UW_Acc: 0.6444\n",
      "Epoch [11/80], time: 7.87s\n",
      "Train - loss: 0.2417, F1: 0.9202, UW_Acc: 0.9203\n",
      "Val - loss: 1.0448, F1: 0.6475, UW_Acc: 0.6526\n",
      "Epoch [12/80], time: 7.78s\n",
      "Train - loss: 0.2122, F1: 0.9305, UW_Acc: 0.9304\n",
      "Val - loss: 1.1778, F1: 0.6245, UW_Acc: 0.6302\n",
      "Epoch [13/80], time: 7.81s\n",
      "Train - loss: 0.2152, F1: 0.9249, UW_Acc: 0.9248\n",
      "Val - loss: 0.8925, F1: 0.7078, UW_Acc: 0.7112\n",
      "Epoch [14/80], time: 7.85s\n",
      "Train - loss: 0.1956, F1: 0.9330, UW_Acc: 0.9327\n",
      "Val - loss: 0.9329, F1: 0.7035, UW_Acc: 0.7043\n",
      "Epoch [15/80], time: 7.76s\n",
      "Train - loss: 0.1673, F1: 0.9417, UW_Acc: 0.9415\n",
      "Val - loss: 1.1679, F1: 0.6445, UW_Acc: 0.6619\n",
      "Epoch [16/80], time: 7.81s\n",
      "Train - loss: 0.1722, F1: 0.9461, UW_Acc: 0.9460\n",
      "Val - loss: 1.0620, F1: 0.6664, UW_Acc: 0.6721\n",
      "Epoch [17/80], time: 7.79s\n",
      "Train - loss: 0.1485, F1: 0.9509, UW_Acc: 0.9507\n",
      "Val - loss: 0.9966, F1: 0.6972, UW_Acc: 0.7026\n",
      "Epoch [18/80], time: 7.81s\n",
      "Train - loss: 0.1424, F1: 0.9524, UW_Acc: 0.9523\n",
      "Val - loss: 1.0819, F1: 0.6669, UW_Acc: 0.6815\n",
      "Epoch 00019: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch [19/80], time: 7.80s\n",
      "Train - loss: 0.1381, F1: 0.9538, UW_Acc: 0.9538\n",
      "Val - loss: 1.0932, F1: 0.6831, UW_Acc: 0.6891\n",
      "Epoch [20/80], time: 7.81s\n",
      "Train - loss: 0.1040, F1: 0.9679, UW_Acc: 0.9678\n",
      "Val - loss: 0.9879, F1: 0.7146, UW_Acc: 0.7209\n",
      "Epoch [21/80], time: 7.80s\n",
      "Train - loss: 0.0872, F1: 0.9738, UW_Acc: 0.9737\n",
      "Val - loss: 1.1934, F1: 0.6573, UW_Acc: 0.6665\n",
      "Epoch [22/80], time: 7.81s\n",
      "Train - loss: 0.0828, F1: 0.9753, UW_Acc: 0.9752\n",
      "Val - loss: 1.1490, F1: 0.6716, UW_Acc: 0.6858\n",
      "Epoch [23/80], time: 7.76s\n",
      "Train - loss: 0.0754, F1: 0.9763, UW_Acc: 0.9763\n",
      "Val - loss: 1.0552, F1: 0.7013, UW_Acc: 0.7071\n",
      "Epoch [24/80], time: 7.86s\n",
      "Train - loss: 0.0734, F1: 0.9782, UW_Acc: 0.9781\n",
      "Val - loss: 1.1133, F1: 0.6868, UW_Acc: 0.6934\n",
      "Epoch [25/80], time: 7.81s\n",
      "Train - loss: 0.0635, F1: 0.9805, UW_Acc: 0.9805\n",
      "Val - loss: 1.1024, F1: 0.6892, UW_Acc: 0.6961\n",
      "Epoch 00026: reducing learning rate of group 0 to 9.0000e-06.\n",
      "Epoch [26/80], time: 7.79s\n",
      "Train - loss: 0.0559, F1: 0.9854, UW_Acc: 0.9854\n",
      "Val - loss: 1.1985, F1: 0.6714, UW_Acc: 0.6802\n",
      "Epoch [27/80], time: 7.81s\n",
      "Train - loss: 0.0562, F1: 0.9842, UW_Acc: 0.9841\n",
      "Val - loss: 1.0662, F1: 0.7050, UW_Acc: 0.7096\n",
      "Epoch [28/80], time: 7.86s\n",
      "Train - loss: 0.0500, F1: 0.9886, UW_Acc: 0.9886\n",
      "Val - loss: 1.0474, F1: 0.7231, UW_Acc: 0.7260\n",
      "Epoch [29/80], time: 7.81s\n",
      "Train - loss: 0.0447, F1: 0.9883, UW_Acc: 0.9883\n",
      "Val - loss: 1.1002, F1: 0.7055, UW_Acc: 0.7106\n",
      "Epoch [30/80], time: 7.76s\n",
      "Train - loss: 0.0459, F1: 0.9889, UW_Acc: 0.9889\n",
      "Val - loss: 1.1276, F1: 0.7004, UW_Acc: 0.7047\n",
      "Epoch [31/80], time: 7.82s\n",
      "Train - loss: 0.0428, F1: 0.9903, UW_Acc: 0.9903\n",
      "Val - loss: 1.0903, F1: 0.7023, UW_Acc: 0.7065\n",
      "Epoch [32/80], time: 7.81s\n",
      "Train - loss: 0.0447, F1: 0.9889, UW_Acc: 0.9889\n",
      "Val - loss: 1.1518, F1: 0.6995, UW_Acc: 0.7035\n",
      "Epoch [33/80], time: 7.77s\n",
      "Train - loss: 0.0432, F1: 0.9878, UW_Acc: 0.9877\n",
      "Val - loss: 1.1006, F1: 0.7098, UW_Acc: 0.7137\n",
      "Epoch 00034: reducing learning rate of group 0 to 2.7000e-06.\n",
      "Epoch [34/80], time: 7.90s\n",
      "Train - loss: 0.0366, F1: 0.9916, UW_Acc: 0.9916\n",
      "Val - loss: 1.1319, F1: 0.6996, UW_Acc: 0.7069\n",
      "Epoch [35/80], time: 7.79s\n",
      "Train - loss: 0.0371, F1: 0.9912, UW_Acc: 0.9912\n",
      "Val - loss: 1.1206, F1: 0.7042, UW_Acc: 0.7094\n",
      "Epoch [36/80], time: 7.76s\n",
      "Train - loss: 0.0349, F1: 0.9923, UW_Acc: 0.9923\n",
      "Val - loss: 1.1061, F1: 0.7130, UW_Acc: 0.7166\n",
      "Epoch [37/80], time: 7.78s\n",
      "Train - loss: 0.0339, F1: 0.9922, UW_Acc: 0.9922\n",
      "Val - loss: 1.1063, F1: 0.7068, UW_Acc: 0.7114\n",
      "Epoch [38/80], time: 7.81s\n",
      "Train - loss: 0.0405, F1: 0.9906, UW_Acc: 0.9906\n",
      "Val - loss: 1.0952, F1: 0.7169, UW_Acc: 0.7209\n",
      "Epoch [39/80], time: 7.77s\n",
      "Train - loss: 0.0348, F1: 0.9910, UW_Acc: 0.9910\n",
      "Val - loss: 1.1461, F1: 0.6993, UW_Acc: 0.7045\n",
      "Epoch 00040: reducing learning rate of group 0 to 8.1000e-07.\n",
      "Epoch [40/80], time: 7.78s\n",
      "Train - loss: 0.0333, F1: 0.9923, UW_Acc: 0.9923\n",
      "Val - loss: 1.1373, F1: 0.7009, UW_Acc: 0.7055\n",
      "Epoch [41/80], time: 7.85s\n",
      "Train - loss: 0.0368, F1: 0.9912, UW_Acc: 0.9911\n",
      "Val - loss: 1.1739, F1: 0.6903, UW_Acc: 0.6969\n",
      "Epoch [42/80], time: 7.77s\n",
      "Train - loss: 0.0346, F1: 0.9929, UW_Acc: 0.9929\n",
      "Val - loss: 1.1736, F1: 0.6874, UW_Acc: 0.6946\n",
      "Epoch [43/80], time: 7.77s\n",
      "Train - loss: 0.0326, F1: 0.9926, UW_Acc: 0.9926\n",
      "Val - loss: 1.1347, F1: 0.7042, UW_Acc: 0.7094\n",
      "Epoch [44/80], time: 7.76s\n",
      "Train - loss: 0.0379, F1: 0.9902, UW_Acc: 0.9902\n",
      "Val - loss: 1.1320, F1: 0.6978, UW_Acc: 0.7022\n",
      "Epoch [45/80], time: 7.87s\n",
      "Train - loss: 0.0368, F1: 0.9900, UW_Acc: 0.9900\n",
      "Val - loss: 1.1208, F1: 0.7090, UW_Acc: 0.7127\n",
      "Epoch 00046: reducing learning rate of group 0 to 2.4300e-07.\n",
      "Epoch [46/80], time: 7.79s\n",
      "Train - loss: 0.0369, F1: 0.9927, UW_Acc: 0.9927\n",
      "Val - loss: 1.1533, F1: 0.7001, UW_Acc: 0.7041\n",
      "Epoch [47/80], time: 7.83s\n",
      "Train - loss: 0.0351, F1: 0.9916, UW_Acc: 0.9916\n",
      "Val - loss: 1.1419, F1: 0.6993, UW_Acc: 0.7041\n",
      "Epoch [48/80], time: 7.82s\n",
      "Train - loss: 0.0391, F1: 0.9900, UW_Acc: 0.9899\n",
      "Val - loss: 1.1543, F1: 0.6978, UW_Acc: 0.7035\n",
      "Epoch [49/80], time: 7.77s\n",
      "Train - loss: 0.0350, F1: 0.9916, UW_Acc: 0.9916\n",
      "Val - loss: 1.1734, F1: 0.6973, UW_Acc: 0.7051\n",
      "Epoch [50/80], time: 7.77s\n",
      "Train - loss: 0.0418, F1: 0.9884, UW_Acc: 0.9884\n",
      "Val - loss: 1.1339, F1: 0.7047, UW_Acc: 0.7100\n",
      "Epoch [51/80], time: 8.04s\n",
      "Train - loss: 0.0362, F1: 0.9915, UW_Acc: 0.9915\n",
      "Val - loss: 1.1454, F1: 0.7061, UW_Acc: 0.7120\n",
      "Epoch 00052: reducing learning rate of group 0 to 7.2900e-08.\n",
      "Epoch [52/80], time: 7.78s\n",
      "Train - loss: 0.0343, F1: 0.9926, UW_Acc: 0.9926\n",
      "Val - loss: 1.1548, F1: 0.7015, UW_Acc: 0.7061\n",
      "Epoch [53/80], time: 7.75s\n",
      "Train - loss: 0.0302, F1: 0.9944, UW_Acc: 0.9944\n",
      "Val - loss: 1.1381, F1: 0.7065, UW_Acc: 0.7114\n",
      "Epoch [54/80], time: 7.78s\n",
      "Train - loss: 0.0295, F1: 0.9949, UW_Acc: 0.9948\n",
      "Val - loss: 1.1596, F1: 0.6974, UW_Acc: 0.7039\n",
      "Epoch [55/80], time: 7.87s\n",
      "Train - loss: 0.0380, F1: 0.9911, UW_Acc: 0.9910\n",
      "Val - loss: 1.1720, F1: 0.6983, UW_Acc: 0.7045\n",
      "Epoch [56/80], time: 7.79s\n",
      "Train - loss: 0.0400, F1: 0.9903, UW_Acc: 0.9903\n",
      "Val - loss: 1.1881, F1: 0.6975, UW_Acc: 0.7038\n",
      "Epoch [57/80], time: 7.78s\n",
      "Train - loss: 0.0338, F1: 0.9937, UW_Acc: 0.9937\n",
      "Val - loss: 1.1691, F1: 0.6963, UW_Acc: 0.7018\n",
      "Epoch 00058: reducing learning rate of group 0 to 2.1870e-08.\n",
      "Epoch [58/80], time: 7.88s\n",
      "Train - loss: 0.0360, F1: 0.9920, UW_Acc: 0.9920\n",
      "Val - loss: 1.1534, F1: 0.6982, UW_Acc: 0.7045\n",
      "Epoch [59/80], time: 7.76s\n",
      "Train - loss: 0.0371, F1: 0.9910, UW_Acc: 0.9909\n",
      "Val - loss: 1.1361, F1: 0.6987, UW_Acc: 0.7035\n",
      "Epoch [60/80], time: 7.79s\n",
      "Train - loss: 0.0375, F1: 0.9920, UW_Acc: 0.9920\n",
      "Val - loss: 1.1593, F1: 0.6976, UW_Acc: 0.7032\n",
      "Epoch [61/80], time: 7.77s\n",
      "Train - loss: 0.0364, F1: 0.9902, UW_Acc: 0.9901\n",
      "Val - loss: 1.1542, F1: 0.7034, UW_Acc: 0.7090\n",
      "Epoch [62/80], time: 7.80s\n",
      "Train - loss: 0.0350, F1: 0.9924, UW_Acc: 0.9923\n",
      "Val - loss: 1.1553, F1: 0.7012, UW_Acc: 0.7065\n",
      "Epoch [63/80], time: 7.77s\n",
      "Train - loss: 0.0337, F1: 0.9934, UW_Acc: 0.9934\n",
      "Val - loss: 1.1678, F1: 0.6991, UW_Acc: 0.7051\n",
      "Epoch 00064: reducing learning rate of group 0 to 6.5610e-09.\n",
      "Epoch [64/80], time: 7.76s\n",
      "Train - loss: 0.0318, F1: 0.9923, UW_Acc: 0.9923\n",
      "Val - loss: 1.1474, F1: 0.6992, UW_Acc: 0.7035\n",
      "Epoch [65/80], time: 7.84s\n",
      "Train - loss: 0.0410, F1: 0.9904, UW_Acc: 0.9903\n",
      "Val - loss: 1.2061, F1: 0.6896, UW_Acc: 0.6967\n",
      "Epoch [66/80], time: 7.80s\n",
      "Train - loss: 0.0287, F1: 0.9939, UW_Acc: 0.9939\n",
      "Val - loss: 1.1562, F1: 0.6990, UW_Acc: 0.7041\n",
      "Epoch [67/80], time: 7.80s\n",
      "Train - loss: 0.0323, F1: 0.9930, UW_Acc: 0.9930\n",
      "Val - loss: 1.1512, F1: 0.6991, UW_Acc: 0.7051\n",
      "Epoch [68/80], time: 7.98s\n",
      "Train - loss: 0.0354, F1: 0.9919, UW_Acc: 0.9918\n",
      "Val - loss: 1.1233, F1: 0.7099, UW_Acc: 0.7143\n",
      "Epoch [69/80], time: 7.90s\n",
      "Train - loss: 0.0344, F1: 0.9920, UW_Acc: 0.9920\n",
      "Val - loss: 1.1500, F1: 0.7008, UW_Acc: 0.7061\n",
      "Epoch [70/80], time: 7.82s\n",
      "Train - loss: 0.0378, F1: 0.9924, UW_Acc: 0.9924\n",
      "Val - loss: 1.1619, F1: 0.6978, UW_Acc: 0.7032\n",
      "Epoch [71/80], time: 7.81s\n",
      "Train - loss: 0.0340, F1: 0.9920, UW_Acc: 0.9920\n",
      "Val - loss: 1.1737, F1: 0.6962, UW_Acc: 0.7026\n",
      "Epoch [72/80], time: 7.94s\n",
      "Train - loss: 0.0345, F1: 0.9917, UW_Acc: 0.9917\n",
      "Val - loss: 1.1300, F1: 0.7080, UW_Acc: 0.7129\n",
      "Epoch [73/80], time: 7.84s\n",
      "Train - loss: 0.0365, F1: 0.9904, UW_Acc: 0.9904\n",
      "Val - loss: 1.1666, F1: 0.6959, UW_Acc: 0.7012\n",
      "Epoch [74/80], time: 7.80s\n",
      "Train - loss: 0.0429, F1: 0.9891, UW_Acc: 0.9890\n",
      "Val - loss: 1.1660, F1: 0.6976, UW_Acc: 0.7032\n",
      "Epoch [75/80], time: 7.86s\n",
      "Train - loss: 0.0355, F1: 0.9918, UW_Acc: 0.9917\n",
      "Val - loss: 1.1671, F1: 0.6981, UW_Acc: 0.7041\n",
      "Epoch [76/80], time: 7.79s\n",
      "Train - loss: 0.0410, F1: 0.9894, UW_Acc: 0.9894\n",
      "Val - loss: 1.1730, F1: 0.6957, UW_Acc: 0.7008\n",
      "Epoch [77/80], time: 7.85s\n",
      "Train - loss: 0.0338, F1: 0.9933, UW_Acc: 0.9933\n",
      "Val - loss: 1.1323, F1: 0.7049, UW_Acc: 0.7094\n",
      "Epoch [78/80], time: 7.78s\n",
      "Train - loss: 0.0363, F1: 0.9912, UW_Acc: 0.9912\n",
      "Val - loss: 1.1447, F1: 0.6968, UW_Acc: 0.7035\n",
      "Epoch [79/80], time: 7.78s\n",
      "Train - loss: 0.0294, F1: 0.9937, UW_Acc: 0.9937\n",
      "Val - loss: 1.1379, F1: 0.7053, UW_Acc: 0.7098\n",
      "Epoch [80/80], time: 7.84s\n",
      "Train - loss: 0.0320, F1: 0.9925, UW_Acc: 0.9925\n",
      "Val - loss: 1.1244, F1: 0.7049, UW_Acc: 0.7098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.2982983810957087,\n",
       "  0.9085727431053339,\n",
       "  0.6948946354001068,\n",
       "  0.5885889959889788,\n",
       "  0.4978982480459435,\n",
       "  0.41418074036753455,\n",
       "  0.36979189584421557,\n",
       "  0.3272529743438543,\n",
       "  0.2950351886277975,\n",
       "  0.2637899926928587,\n",
       "  0.24173882915530093,\n",
       "  0.21217315886602844,\n",
       "  0.21515137165091758,\n",
       "  0.19557491464670315,\n",
       "  0.1673129202668057,\n",
       "  0.17216877258101174,\n",
       "  0.14846423268318176,\n",
       "  0.1423806565445523,\n",
       "  0.1381496660584627,\n",
       "  0.1040314812819625,\n",
       "  0.08720562355809433,\n",
       "  0.0827846571283285,\n",
       "  0.07540996164776557,\n",
       "  0.07339802925843139,\n",
       "  0.06354606480792512,\n",
       "  0.05589027659491051,\n",
       "  0.056165609811974125,\n",
       "  0.04997726118322029,\n",
       "  0.04474440156373867,\n",
       "  0.04593914929170941,\n",
       "  0.04278192781778269,\n",
       "  0.04471057116292244,\n",
       "  0.04320821694509928,\n",
       "  0.03655503837521686,\n",
       "  0.037088304909682554,\n",
       "  0.034897514553957204,\n",
       "  0.033937419699721555,\n",
       "  0.04049589660365221,\n",
       "  0.034805150139470435,\n",
       "  0.03326475206589283,\n",
       "  0.03683922366174155,\n",
       "  0.0346474964171648,\n",
       "  0.03255828251239172,\n",
       "  0.03791276733715867,\n",
       "  0.036768283001905266,\n",
       "  0.03689062530391438,\n",
       "  0.03509762255085069,\n",
       "  0.03912608392623275,\n",
       "  0.03499873164434766,\n",
       "  0.041775071430344916,\n",
       "  0.03622809431493976,\n",
       "  0.0343378564819347,\n",
       "  0.03022584979703953,\n",
       "  0.029507654350857403,\n",
       "  0.037959465063935105,\n",
       "  0.04001845295952503,\n",
       "  0.03383908509601687,\n",
       "  0.036008996672408526,\n",
       "  0.03711794975192048,\n",
       "  0.03747424751866695,\n",
       "  0.0363776556237839,\n",
       "  0.0350014453883781,\n",
       "  0.03366261553885632,\n",
       "  0.03175495120934969,\n",
       "  0.0409893529966127,\n",
       "  0.028659715728704318,\n",
       "  0.032265970938254236,\n",
       "  0.03538109104387289,\n",
       "  0.0343860627780127,\n",
       "  0.037816492559085056,\n",
       "  0.033961180619202384,\n",
       "  0.03454833042396362,\n",
       "  0.03651745091084131,\n",
       "  0.04287080936740304,\n",
       "  0.0354593914911844,\n",
       "  0.04098510133579027,\n",
       "  0.03380404133349657,\n",
       "  0.036275540868383506,\n",
       "  0.029425127258480982,\n",
       "  0.031986121066607705],\n",
       " [0.3625390628601457,\n",
       "  0.6355315028747166,\n",
       "  0.7341550576224128,\n",
       "  0.777454086347298,\n",
       "  0.8244004764225047,\n",
       "  0.8528474428177838,\n",
       "  0.8674465004359327,\n",
       "  0.8884914283923891,\n",
       "  0.8993058669896888,\n",
       "  0.9116140862444224,\n",
       "  0.9201945874201445,\n",
       "  0.9305406610337711,\n",
       "  0.9249269440493187,\n",
       "  0.9330271086186019,\n",
       "  0.9416550658319877,\n",
       "  0.946137872645865,\n",
       "  0.9509084727282215,\n",
       "  0.952417417991276,\n",
       "  0.953824151007735,\n",
       "  0.9678764750047605,\n",
       "  0.9737681434717089,\n",
       "  0.9752783507677767,\n",
       "  0.9763397880420378,\n",
       "  0.9781714829131399,\n",
       "  0.9805110990145267,\n",
       "  0.9854099601686542,\n",
       "  0.9841678395385938,\n",
       "  0.9885918608024785,\n",
       "  0.988311845562232,\n",
       "  0.9889497416195084,\n",
       "  0.9903180945541902,\n",
       "  0.9889207606810987,\n",
       "  0.987762710362589,\n",
       "  0.9916468310300638,\n",
       "  0.9912113899149451,\n",
       "  0.9923309402045488,\n",
       "  0.9922009360944279,\n",
       "  0.9906129609545306,\n",
       "  0.9910160787055424,\n",
       "  0.9923278986783337,\n",
       "  0.9911566158561964,\n",
       "  0.9929165992008555,\n",
       "  0.9925653519145778,\n",
       "  0.9902019741770346,\n",
       "  0.9900265840223555,\n",
       "  0.9927485008106338,\n",
       "  0.9915590240404603,\n",
       "  0.9899651491930122,\n",
       "  0.9916330970730914,\n",
       "  0.9884361766610247,\n",
       "  0.9914656508239524,\n",
       "  0.9925904705129169,\n",
       "  0.9943830930425441,\n",
       "  0.9948524132056589,\n",
       "  0.9910607870672106,\n",
       "  0.9903384611049418,\n",
       "  0.9937464571597916,\n",
       "  0.9920438169985828,\n",
       "  0.9909737810488572,\n",
       "  0.9920245541981815,\n",
       "  0.9901579727152505,\n",
       "  0.9924018388471794,\n",
       "  0.9933955126473356,\n",
       "  0.9923200123052293,\n",
       "  0.9903698341078775,\n",
       "  0.9939418244444456,\n",
       "  0.993035575806548,\n",
       "  0.9918814890939697,\n",
       "  0.992009153000231,\n",
       "  0.9924473035185533,\n",
       "  0.9920356631647043,\n",
       "  0.9916917636431541,\n",
       "  0.9904245829722046,\n",
       "  0.989095698988168,\n",
       "  0.9917624887668053,\n",
       "  0.9894351991241717,\n",
       "  0.9932973390752501,\n",
       "  0.9911930510416564,\n",
       "  0.9936755608152563,\n",
       "  0.9924661156107469],\n",
       " [0.3889567917547569,\n",
       "  0.6416820824524313,\n",
       "  0.7332848837209303,\n",
       "  0.7775336945031713,\n",
       "  0.8238471194503171,\n",
       "  0.8525039640591966,\n",
       "  0.8672700845665962,\n",
       "  0.8882465644820297,\n",
       "  0.8991146934460889,\n",
       "  0.9114197938689217,\n",
       "  0.9203058932346723,\n",
       "  0.9303977272727273,\n",
       "  0.9247984936575052,\n",
       "  0.9326770613107822,\n",
       "  0.9415301268498942,\n",
       "  0.9459896934460889,\n",
       "  0.9506639799154333,\n",
       "  0.9523487050739958,\n",
       "  0.9538021934460889,\n",
       "  0.967775502114165,\n",
       "  0.9736885570824524,\n",
       "  0.9752081131078225,\n",
       "  0.9762982293868923,\n",
       "  0.9781150898520086,\n",
       "  0.9804770084566597,\n",
       "  0.9853825317124737,\n",
       "  0.9841107293868923,\n",
       "  0.9885537790697675,\n",
       "  0.9882895084566597,\n",
       "  0.9889336680761099,\n",
       "  0.9902880549682876,\n",
       "  0.9889171511627907,\n",
       "  0.9877444503171248,\n",
       "  0.9916424418604651,\n",
       "  0.9911964852008457,\n",
       "  0.9923031183932347,\n",
       "  0.9921875,\n",
       "  0.9905688424947146,\n",
       "  0.9910147991543341,\n",
       "  0.9922866014799155,\n",
       "  0.9911304175475687,\n",
       "  0.9929142441860465,\n",
       "  0.9925508720930233,\n",
       "  0.9901889534883721,\n",
       "  0.9900072674418605,\n",
       "  0.9927325581395349,\n",
       "  0.991559857293869,\n",
       "  0.9899246828752644,\n",
       "  0.9915763742071882,\n",
       "  0.9884051268498942,\n",
       "  0.9914607558139535,\n",
       "  0.9925673890063424,\n",
       "  0.9943677325581395,\n",
       "  0.9948302061310783,\n",
       "  0.9910313160676534,\n",
       "  0.9902880549682876,\n",
       "  0.9937400898520086,\n",
       "  0.9920223308668076,\n",
       "  0.990948731501057,\n",
       "  0.9920058139534884,\n",
       "  0.990106368921776,\n",
       "  0.9923031183932347,\n",
       "  0.9933767177589853,\n",
       "  0.9923031183932347,\n",
       "  0.9903210887949261,\n",
       "  0.9939217758985202,\n",
       "  0.993013345665962,\n",
       "  0.991840644820296,\n",
       "  0.9920058139534884,\n",
       "  0.9923857029598309,\n",
       "  0.9920058139534884,\n",
       "  0.9916589587737843,\n",
       "  0.9903706395348837,\n",
       "  0.9890327695560255,\n",
       "  0.9917415433403807,\n",
       "  0.9893796247357295,\n",
       "  0.9932776162790697,\n",
       "  0.9911964852008457,\n",
       "  0.9936575052854123,\n",
       "  0.9924682875264271],\n",
       " [1.1387992650270462,\n",
       "  1.0060971975326538,\n",
       "  0.9920433610677719,\n",
       "  1.1452737748622894,\n",
       "  1.191851407289505,\n",
       "  0.9199609830975533,\n",
       "  1.004636101424694,\n",
       "  1.0805442556738853,\n",
       "  1.1863184124231339,\n",
       "  1.0550771206617355,\n",
       "  1.0447759926319122,\n",
       "  1.1778499633073807,\n",
       "  0.8925290033221245,\n",
       "  0.9329134300351143,\n",
       "  1.1679206788539886,\n",
       "  1.0619912296533585,\n",
       "  0.9965897276997566,\n",
       "  1.0819133818149567,\n",
       "  1.0932230949401855,\n",
       "  0.9879470765590668,\n",
       "  1.193429097533226,\n",
       "  1.1489587724208832,\n",
       "  1.055151954293251,\n",
       "  1.1133339330554008,\n",
       "  1.1023847311735153,\n",
       "  1.1984845846891403,\n",
       "  1.0661676079034805,\n",
       "  1.0473825559020042,\n",
       "  1.1002267748117447,\n",
       "  1.1275737807154655,\n",
       "  1.0903396978974342,\n",
       "  1.1517841666936874,\n",
       "  1.1005582362413406,\n",
       "  1.13188137114048,\n",
       "  1.120581641793251,\n",
       "  1.106076419353485,\n",
       "  1.1063242852687836,\n",
       "  1.0951520428061485,\n",
       "  1.146087795495987,\n",
       "  1.1372972130775452,\n",
       "  1.1738868579268456,\n",
       "  1.1736286282539368,\n",
       "  1.1347018703818321,\n",
       "  1.131973385810852,\n",
       "  1.1208078414201736,\n",
       "  1.1532858088612556,\n",
       "  1.141908124089241,\n",
       "  1.1542794406414032,\n",
       "  1.173434540629387,\n",
       "  1.1338968947529793,\n",
       "  1.1454397961497307,\n",
       "  1.1547876521945,\n",
       "  1.1380872130393982,\n",
       "  1.159577988088131,\n",
       "  1.1720042154192924,\n",
       "  1.188088484108448,\n",
       "  1.1691455617547035,\n",
       "  1.153385080397129,\n",
       "  1.136090211570263,\n",
       "  1.1593426242470741,\n",
       "  1.1542427241802216,\n",
       "  1.1552503779530525,\n",
       "  1.1678036898374557,\n",
       "  1.147350862622261,\n",
       "  1.206122636795044,\n",
       "  1.1561619713902473,\n",
       "  1.1512492001056671,\n",
       "  1.1232855096459389,\n",
       "  1.1500492468476295,\n",
       "  1.161885291337967,\n",
       "  1.1736550778150558,\n",
       "  1.1299914345145226,\n",
       "  1.166555292904377,\n",
       "  1.166039764881134,\n",
       "  1.1671088561415672,\n",
       "  1.17301394790411,\n",
       "  1.1323055177927017,\n",
       "  1.1446754187345505,\n",
       "  1.1378829330205917,\n",
       "  1.1243996918201447],\n",
       " [0.5254107911681947,\n",
       "  0.5722408816927083,\n",
       "  0.5926192531960248,\n",
       "  0.5474769180364827,\n",
       "  0.5478110833324611,\n",
       "  0.6418168308204886,\n",
       "  0.6300645939621714,\n",
       "  0.6114412308610145,\n",
       "  0.5738633750648215,\n",
       "  0.6314787935188028,\n",
       "  0.6474786615527345,\n",
       "  0.6244606936879415,\n",
       "  0.7077808441850016,\n",
       "  0.7035198395720199,\n",
       "  0.6444525340043623,\n",
       "  0.6664312493852396,\n",
       "  0.6971520714680858,\n",
       "  0.6668590613717584,\n",
       "  0.6830896690309822,\n",
       "  0.7146083476956786,\n",
       "  0.657309174564487,\n",
       "  0.6716273944307911,\n",
       "  0.7013107466702645,\n",
       "  0.6867873638532506,\n",
       "  0.6891738158175447,\n",
       "  0.6714207475183938,\n",
       "  0.705026748399096,\n",
       "  0.7231134199131858,\n",
       "  0.7055022200953559,\n",
       "  0.7003551098387202,\n",
       "  0.7022616153401238,\n",
       "  0.6995026260349232,\n",
       "  0.7098261830564125,\n",
       "  0.6996198445404945,\n",
       "  0.7042162865507678,\n",
       "  0.712975942397889,\n",
       "  0.706840381521784,\n",
       "  0.7168597738166993,\n",
       "  0.6993085420735313,\n",
       "  0.7008840743499839,\n",
       "  0.6902740133768628,\n",
       "  0.6873680152460862,\n",
       "  0.70415454397895,\n",
       "  0.697837678126128,\n",
       "  0.7089997427247916,\n",
       "  0.700059899578265,\n",
       "  0.6993409177098933,\n",
       "  0.6977611284443309,\n",
       "  0.6973199562713113,\n",
       "  0.7047118157757557,\n",
       "  0.7060936655791509,\n",
       "  0.70154541330081,\n",
       "  0.7064845219039145,\n",
       "  0.69736474703015,\n",
       "  0.6983283109993857,\n",
       "  0.6975349292213465,\n",
       "  0.6963361601803831,\n",
       "  0.6981877010928413,\n",
       "  0.6987025565059981,\n",
       "  0.6975962790212068,\n",
       "  0.7034408286068815,\n",
       "  0.7011976290614841,\n",
       "  0.6991381464023526,\n",
       "  0.6992320110091967,\n",
       "  0.6895705185251514,\n",
       "  0.6989587539115605,\n",
       "  0.6991324940036621,\n",
       "  0.7099000008607177,\n",
       "  0.7007732669193669,\n",
       "  0.6977869779059003,\n",
       "  0.6962062327016301,\n",
       "  0.7079793424374241,\n",
       "  0.69590472468365,\n",
       "  0.697554609663779,\n",
       "  0.6980838265731822,\n",
       "  0.69573425524339,\n",
       "  0.7049136301884659,\n",
       "  0.6968181279551683,\n",
       "  0.7052656018150477,\n",
       "  0.7048869204135881],\n",
       " [0.5261548913043478,\n",
       "  0.5917544157608696,\n",
       "  0.5979534646739131,\n",
       "  0.5792289402173914,\n",
       "  0.5817764945652174,\n",
       "  0.6545516304347826,\n",
       "  0.6393512228260869,\n",
       "  0.6282269021739131,\n",
       "  0.5950662364130435,\n",
       "  0.6444463315217391,\n",
       "  0.6525985054347826,\n",
       "  0.6301800271739131,\n",
       "  0.7111922554347826,\n",
       "  0.7043138586956522,\n",
       "  0.6618546195652174,\n",
       "  0.6721297554347826,\n",
       "  0.7025730298913043,\n",
       "  0.6815132472826086,\n",
       "  0.6891134510869565,\n",
       "  0.7209154211956522,\n",
       "  0.6664826766304348,\n",
       "  0.6858440896739131,\n",
       "  0.7070737092391304,\n",
       "  0.6934018342391304,\n",
       "  0.6961192255434783,\n",
       "  0.6801545516304348,\n",
       "  0.7096212635869565,\n",
       "  0.7260105298913043,\n",
       "  0.7105978260869565,\n",
       "  0.7047384510869565,\n",
       "  0.7064792798913043,\n",
       "  0.7035495923913043,\n",
       "  0.7136973505434783,\n",
       "  0.7069038722826086,\n",
       "  0.7094089673913043,\n",
       "  0.7166270380434783,\n",
       "  0.7113620923913043,\n",
       "  0.7209154211956522,\n",
       "  0.7045261548913043,\n",
       "  0.7055027173913043,\n",
       "  0.6969259510869565,\n",
       "  0.6945906929347826,\n",
       "  0.7094089673913043,\n",
       "  0.7021908967391304,\n",
       "  0.7127207880434783,\n",
       "  0.7041440217391304,\n",
       "  0.7041440217391304,\n",
       "  0.7035495923913043,\n",
       "  0.7051205842391304,\n",
       "  0.7100033967391304,\n",
       "  0.7119565217391304,\n",
       "  0.7060971467391304,\n",
       "  0.7113620923913043,\n",
       "  0.7039317255434783,\n",
       "  0.7045261548913043,\n",
       "  0.7037618885869565,\n",
       "  0.7018087635869565,\n",
       "  0.7045261548913043,\n",
       "  0.7035495923913043,\n",
       "  0.7031674592391304,\n",
       "  0.7090268342391304,\n",
       "  0.7064792798913043,\n",
       "  0.7051205842391304,\n",
       "  0.7035495923913043,\n",
       "  0.6967136548913043,\n",
       "  0.7041440217391304,\n",
       "  0.7051205842391304,\n",
       "  0.7142917798913043,\n",
       "  0.7060971467391304,\n",
       "  0.7031674592391304,\n",
       "  0.7025730298913043,\n",
       "  0.7129330842391304,\n",
       "  0.7012143342391304,\n",
       "  0.7031674592391304,\n",
       "  0.7041440217391304,\n",
       "  0.7008322010869565,\n",
       "  0.7094089673913043,\n",
       "  0.7035495923913043,\n",
       "  0.7097911005434783,\n",
       "  0.7097911005434783])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# 优化器\n",
    "optimizer = torch.optim.AdamW(model_combined.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "\n",
    "# 学习率调度器\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.3, patience=5, verbose=True)\n",
    "\n",
    "# 调用训练函数进行训练\n",
    "train_model(model_combined, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=80, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac99b1",
   "metadata": {
    "papermill": {
     "duration": 0.042749,
     "end_time": "2024-09-18T17:59:13.854745",
     "exception": false,
     "start_time": "2024-09-18T17:59:13.811996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model-9 Mixed Attention 3 modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "166cb6f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:59:13.943123Z",
     "iopub.status.busy": "2024-09-18T17:59:13.942261Z",
     "iopub.status.idle": "2024-09-18T17:59:13.949479Z",
     "shell.execute_reply": "2024-09-18T17:59:13.948607Z"
    },
    "papermill": {
     "duration": 0.05384,
     "end_time": "2024-09-18T17:59:13.951403",
     "exception": false,
     "start_time": "2024-09-18T17:59:13.897563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "#     history = {\n",
    "#         'train_loss': [], 'train_f1': [], 'train_uw_acc': [],\n",
    "#         'val_loss': [], 'val_f1': [], 'val_uw_acc': [],\n",
    "#         'train_weights': []\n",
    "#     }\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         epoch_results = {'train_loss': 0, 'train_f1': 0, 'train_uw_acc': 0, 'train_weights': torch.zeros(3).to(device)}\n",
    "        \n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         for batch in train_loader:\n",
    "#             text = batch['text'].to(device)\n",
    "#             speech = batch['speech'].to(device)\n",
    "#             mocap = batch['mocap'].to(device)\n",
    "#             labels = batch['labels'].to(device).long()\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs, weights = model(text, speech, mocap)\n",
    "            \n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             epoch_results['train_loss'] += loss.item()\n",
    "#             f1, uw_acc = calculate_metrics(outputs, labels)\n",
    "#             epoch_results['train_f1'] += f1\n",
    "#             epoch_results['train_uw_acc'] += uw_acc\n",
    "#             epoch_results['train_weights'] += weights.sum(dim=0)\n",
    "        \n",
    "#         for key in ['train_loss', 'train_f1', 'train_uw_acc']:\n",
    "#             epoch_results[key] /= len(train_loader)\n",
    "#         epoch_results['train_weights'] /= len(train_loader.dataset)\n",
    "        \n",
    "#         val_results = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "#         train_time = time.time() - start_time\n",
    "        \n",
    "#         for key in epoch_results:\n",
    "#             history[key].append(epoch_results[key])\n",
    "#         for key in val_results:\n",
    "#             history[key].append(val_results[key])\n",
    "        \n",
    "#         print_log(epoch, train_time, epoch_results, val_results, epochs=num_epochs)\n",
    "\n",
    "#     return history\n",
    "\n",
    "# def validate(model, val_loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     results = {'val_loss': 0, 'val_f1': 0, 'val_uw_acc': 0}\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             text = batch['text'].to(device)\n",
    "#             speech = batch['speech'].to(device)\n",
    "#             mocap = batch['mocap'].to(device)\n",
    "#             labels = batch['labels'].to(device).long()\n",
    "            \n",
    "#             outputs, _ = model(text, speech, mocap)  # 我们在验证时不需要weights\n",
    "            \n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             results['val_loss'] += loss.item()\n",
    "#             f1, uw_acc = calculate_metrics(outputs, labels)\n",
    "#             results['val_f1'] += f1\n",
    "#             results['val_uw_acc'] += uw_acc\n",
    "    \n",
    "#     for key in ['val_loss', 'val_f1', 'val_uw_acc']:\n",
    "#         results[key] /= len(val_loader)\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ae50a24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:59:14.038774Z",
     "iopub.status.busy": "2024-09-18T17:59:14.038510Z",
     "iopub.status.idle": "2024-09-18T17:59:14.059142Z",
     "shell.execute_reply": "2024-09-18T17:59:14.058372Z"
    },
    "papermill": {
     "duration": 0.066553,
     "end_time": "2024-09-18T17:59:14.060917",
     "exception": false,
     "start_time": "2024-09-18T17:59:13.994364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
    "    history = {\n",
    "        'train_loss': [], 'train_f1': [], 'train_uw_acc': [],\n",
    "        'val_loss': [], 'val_f1': [], 'val_uw_acc': [],\n",
    "        'train_weights': [], 'val_weights': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_results = {'train_loss': 0, 'train_f1': 0, 'train_uw_acc': 0, 'train_weights': torch.zeros(3).to(device)}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            text = batch['text'].to(device)\n",
    "            speech = batch['speech'].to(device)\n",
    "            mocap = batch['mocap'].to(device)\n",
    "            labels = batch['labels'].to(device).long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, weights = model(text, speech, mocap)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Add diversity loss\n",
    "            diversity_loss = -torch.std(weights, dim=-1).mean()\n",
    "            loss += 0.1 * diversity_loss\n",
    "            \n",
    "            # L2 regularization on attention weights\n",
    "            l2_reg = 0.001 * weights.pow(2).sum()\n",
    "            loss += l2_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_results['train_loss'] += loss.item()\n",
    "            f1, uw_acc = calculate_metrics(outputs, labels)\n",
    "            epoch_results['train_f1'] += f1\n",
    "            epoch_results['train_uw_acc'] += uw_acc\n",
    "            epoch_results['train_weights'] += weights.sum(dim=0)\n",
    "        \n",
    "        for key in ['train_loss', 'train_f1', 'train_uw_acc']:\n",
    "            epoch_results[key] /= len(train_loader)\n",
    "        epoch_results['train_weights'] /= len(train_loader.dataset)\n",
    "        \n",
    "        val_results = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        for key in epoch_results:\n",
    "            history[key].append(epoch_results[key])\n",
    "        for key in val_results:\n",
    "            history[key].append(val_results[key])\n",
    "        \n",
    "        print_log(epoch, train_time, epoch_results, val_results, epochs=num_epochs)\n",
    "        \n",
    "        scheduler.step(val_results['val_f1'])\n",
    "\n",
    "    return history\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    results = {'val_loss': 0, 'val_f1': 0, 'val_uw_acc': 0, 'val_weights': torch.zeros(3).to(device)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            text = batch['text'].to(device)\n",
    "            speech = batch['speech'].to(device)\n",
    "            mocap = batch['mocap'].to(device)\n",
    "            labels = batch['labels'].to(device).long()\n",
    "            \n",
    "            outputs, weights = model(text, speech, mocap)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            results['val_loss'] += loss.item()\n",
    "            f1, uw_acc = calculate_metrics(outputs, labels)\n",
    "            results['val_f1'] += f1\n",
    "            results['val_uw_acc'] += uw_acc\n",
    "            results['val_weights'] += weights.sum(dim=0)\n",
    "    \n",
    "    for key in ['val_loss', 'val_f1', 'val_uw_acc']:\n",
    "        results[key] /= len(val_loader)\n",
    "    results['val_weights'] /= len(val_loader.dataset)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_log(epoch, train_time, train_results, val_results, epochs=100):\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], time: {train_time:.2f}s\")\n",
    "    print(f\"Train - loss: {train_results['train_loss']:.4f}, F1: {train_results['train_f1']:.4f}, UW_Acc: {train_results['train_uw_acc']:.4f}\")\n",
    "    print(f\"Train Weights - Text: {train_results['train_weights'][0]:.4f}, Speech: {train_results['train_weights'][1]:.4f}, Mocap: {train_results['train_weights'][2]:.4f}\")\n",
    "    print(f\"Val - loss: {val_results['val_loss']:.4f}, F1: {val_results['val_f1']:.4f}, UW_Acc: {val_results['val_uw_acc']:.4f}\")\n",
    "    print(f\"Val Weights - Text: {val_results['val_weights'][0]:.4f}, Speech: {val_results['val_weights'][1]:.4f}, Mocap: {val_results['val_weights'][2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51857a81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:59:14.148816Z",
     "iopub.status.busy": "2024-09-18T17:59:14.148536Z",
     "iopub.status.idle": "2024-09-18T17:59:14.152562Z",
     "shell.execute_reply": "2024-09-18T17:59:14.151707Z"
    },
    "papermill": {
     "duration": 0.050381,
     "end_time": "2024-09-18T17:59:14.154562",
     "exception": false,
     "start_time": "2024-09-18T17:59:14.104181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def print_log(epoch, train_time, train_results, val_results, epochs=100):\n",
    "#     print(f\"Epoch [{epoch+1}/{epochs}], time: {train_time:.2f}s\")\n",
    "#     print(f\"Train - loss: {train_results['train_loss']:.4f}, F1: {train_results['train_f1']:.4f}, UW_Acc: {train_results['train_uw_acc']:.4f}\")\n",
    "#     print(f\"Train Weights - Text: {train_results['train_weights'][0]:.4f}, Speech: {train_results['train_weights'][1]:.4f}, Mocap: {train_results['train_weights'][2]:.4f}\")\n",
    "#     print(f\"Val - loss: {val_results['val_loss']:.4f}, F1: {val_results['val_f1']:.4f}, UW_Acc: {val_results['val_uw_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee86a6d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:59:14.243842Z",
     "iopub.status.busy": "2024-09-18T17:59:14.243506Z",
     "iopub.status.idle": "2024-09-18T17:59:14.291333Z",
     "shell.execute_reply": "2024-09-18T17:59:14.290631Z"
    },
    "papermill": {
     "duration": 0.094569,
     "end_time": "2024-09-18T17:59:14.293260",
     "exception": false,
     "start_time": "2024-09-18T17:59:14.198691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        qkv = self.qkv(x).reshape(x.size(0), x.size(1), 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            dots = dots.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(dots, dim=-1)\n",
    "        attn = self.att_drop(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v).transpose(1, 2).reshape(x.size(0), x.size(1), self.d_model)\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "class ModalityEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DynamicQuerySelection(nn.Module):\n",
    "    def __init__(self, modal_dim, num_modalities):\n",
    "        super().__init__()\n",
    "        self.modal_encoders = nn.ModuleList([\n",
    "            ModalityEncoder(modal_dim, 128, modal_dim) for _ in range(num_modalities)\n",
    "        ])\n",
    "        self.selector = nn.Sequential(\n",
    "            nn.Linear(modal_dim * num_modalities, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_modalities)\n",
    "        )\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 0.5)\n",
    "        \n",
    "    def forward(self, modalities):\n",
    "        encoded = [encoder(modality) for encoder, modality in zip(self.modal_encoders, modalities)]\n",
    "        combined = torch.cat(encoded, dim=-1)\n",
    "        logits = self.selector(combined)\n",
    "        weights = F.softmax(logits / self.temperature, dim=-1)\n",
    "        query = sum(w.unsqueeze(1) * m for w, m in zip(weights.unbind(dim=-1), encoded))\n",
    "        return query, weights\n",
    "\n",
    "class MixedAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_modalities, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_modalities = num_modalities\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model * num_modalities, num_heads)\n",
    "        \n",
    "    def forward(self, modalities):\n",
    "        # Concatenate all modalities\n",
    "        combined = torch.cat(modalities, dim=-1)  # Shape: [batch_size, d_model * num_modalities]\n",
    "        combined = combined.unsqueeze(0)  # Add sequence dimension: [1, batch_size, d_model * num_modalities]\n",
    "        \n",
    "        attn_output, _ = self.multihead_attn(combined, combined, combined)\n",
    "        return attn_output.squeeze(0)  # Remove sequence dimension: [batch_size, d_model * num_modalities]\n",
    "\n",
    "\n",
    "class TextModel(nn.Module):\n",
    "    def __init__(self, nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(nb_words, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(g_word_embedding_matrix.clone().detach())\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.conv2 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.attention = MultiHeadAttention(32, num_heads)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(32 * max_sequence_length, 256)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.bn5(self.dense(x))\n",
    "        return x\n",
    "\n",
    "class SpeechModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.1, num_heads=8):\n",
    "        super(SpeechModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(100 * 34, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.attention = MultiHeadAttention(1024, num_heads)\n",
    "        self.dense2 = nn.Linear(1024, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.bn1(self.dense1(x)))\n",
    "        x = self.attention(x.unsqueeze(1)).squeeze(1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn2(self.dense2(x))\n",
    "        return x\n",
    "\n",
    "class MocapModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MocapModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Linear(256 * 7 * 6, 256)\n",
    "        self.bn8 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.bn8(self.dense(x))\n",
    "        return x\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.text_model = TextModel(nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads)\n",
    "        self.speech_model = SpeechModel(num_heads=num_heads)\n",
    "        self.mocap_model = MocapModel()\n",
    "        \n",
    "        self.dynamic_query = DynamicQuerySelection(256, 3)  # 3 modalities, 256 is the output dim of each model\n",
    "        self.mixed_attention = MixedAttention(256, 3, num_heads)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * 4, 128)  # 256 * 4 because we concatenate the dynamic query\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 4)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, text, speech, mocap):\n",
    "        text_out = self.text_model(text)\n",
    "        speech_out = self.speech_model(speech)\n",
    "        mocap_out = self.mocap_model(mocap)\n",
    "        \n",
    "        query, weights = self.dynamic_query([text_out, speech_out, mocap_out])\n",
    "        combined = self.mixed_attention([text_out, speech_out, mocap_out])\n",
    "        \n",
    "        x = torch.cat([combined, query], dim=-1)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c350c5b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:59:14.381548Z",
     "iopub.status.busy": "2024-09-18T17:59:14.380744Z",
     "iopub.status.idle": "2024-09-18T17:59:14.578718Z",
     "shell.execute_reply": "2024-09-18T17:59:14.577849Z"
    },
    "papermill": {
     "duration": 0.244198,
     "end_time": "2024-09-18T17:59:14.581280",
     "exception": false,
     "start_time": "2024-09-18T17:59:14.337082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CombinedModel(\n",
      "  (text_model): TextModel(\n",
      "    (embedding): Embedding(3130, 300)\n",
      "    (conv1): Conv1d(300, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadAttention(\n",
      "      (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "      (att_drop): Dropout(p=0.1, inplace=False)\n",
      "      (projection): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=16000, out_features=256, bias=True)\n",
      "    (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (speech_model): SpeechModel(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=3400, out_features=1024, bias=True)\n",
      "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadAttention(\n",
      "      (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "      (att_drop): Dropout(p=0.1, inplace=False)\n",
      "      (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (mocap_model): MocapModel(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=10752, out_features=256, bias=True)\n",
      "    (bn8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (dynamic_query): DynamicQuerySelection(\n",
      "    (modal_encoders): ModuleList(\n",
      "      (0-2): 3 x ModalityEncoder(\n",
      "        (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (selector): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (mixed_attention): MixedAttention(\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Combined Model Summary:\n",
      "Model Structure:\n",
      "CombinedModel(\n",
      "  (text_model): TextModel(\n",
      "    (embedding): Embedding(3130, 300)\n",
      "    (conv1): Conv1d(300, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadAttention(\n",
      "      (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "      (att_drop): Dropout(p=0.1, inplace=False)\n",
      "      (projection): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=16000, out_features=256, bias=True)\n",
      "    (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (speech_model): SpeechModel(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense1): Linear(in_features=3400, out_features=1024, bias=True)\n",
      "    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (attention): MultiHeadAttention(\n",
      "      (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "      (att_drop): Dropout(p=0.1, inplace=False)\n",
      "      (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (mocap_model): MocapModel(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (dense): Linear(in_features=10752, out_features=256, bias=True)\n",
      "    (bn8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (dynamic_query): DynamicQuerySelection(\n",
      "    (modal_encoders): ModuleList(\n",
      "      (0-2): 3 x ModalityEncoder(\n",
      "        (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (selector): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (mixed_attention): MixedAttention(\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Input size: [(500,), (100, 34), (200, 189, 1)]\n",
      "text_model: 5,400,856 parameters\n",
      "  embedding: 939,000 parameters\n",
      "  conv1: 230,656 parameters\n",
      "  bn1: 512 parameters\n",
      "  conv2: 98,432 parameters\n",
      "  bn2: 256 parameters\n",
      "  conv3: 24,640 parameters\n",
      "  bn3: 128 parameters\n",
      "  conv4: 6,176 parameters\n",
      "  bn4: 64 parameters\n",
      "  attention: 4,224 parameters\n",
      "  dropout: 0 parameters\n",
      "  flatten: 0 parameters\n",
      "  dense: 4,096,256 parameters\n",
      "  bn5: 512 parameters\n",
      "speech_model: 7,945,984 parameters\n",
      "  flatten: 0 parameters\n",
      "  dense1: 3,482,624 parameters\n",
      "  bn1: 2,048 parameters\n",
      "  attention: 4,198,400 parameters\n",
      "  dense2: 262,400 parameters\n",
      "  bn2: 512 parameters\n",
      "  dropout: 0 parameters\n",
      "mocap_model: 3,179,136 parameters\n",
      "  conv1: 320 parameters\n",
      "  bn1: 64 parameters\n",
      "  conv2: 18,496 parameters\n",
      "  bn2: 128 parameters\n",
      "  conv3: 36,928 parameters\n",
      "  bn3: 128 parameters\n",
      "  conv4: 73,856 parameters\n",
      "  bn4: 256 parameters\n",
      "  conv5: 295,168 parameters\n",
      "  bn5: 512 parameters\n",
      "  dropout: 0 parameters\n",
      "  flatten: 0 parameters\n",
      "  dense: 2,752,768 parameters\n",
      "  bn8: 512 parameters\n",
      "dynamic_query: 429,444 parameters\n",
      "  modal_encoders: 199,296 parameters\n",
      "  selector: 230,147 parameters\n",
      "mixed_attention: 2,362,368 parameters\n",
      "  multihead_attn: 2,362,368 parameters\n",
      "fc1: 131,200 parameters\n",
      "bn1: 256 parameters\n",
      "fc2: 516 parameters\n",
      "dropout: 0 parameters\n",
      "\n",
      "Total trainable parameters: 19,449,760\n",
      "\n",
      "Detailed parameter shapes:\n",
      "text_model.embedding.weight: torch.Size([3130, 300])\n",
      "text_model.conv1.weight: torch.Size([256, 300, 3])\n",
      "text_model.conv1.bias: torch.Size([256])\n",
      "text_model.bn1.weight: torch.Size([256])\n",
      "text_model.bn1.bias: torch.Size([256])\n",
      "text_model.conv2.weight: torch.Size([128, 256, 3])\n",
      "text_model.conv2.bias: torch.Size([128])\n",
      "text_model.bn2.weight: torch.Size([128])\n",
      "text_model.bn2.bias: torch.Size([128])\n",
      "text_model.conv3.weight: torch.Size([64, 128, 3])\n",
      "text_model.conv3.bias: torch.Size([64])\n",
      "text_model.bn3.weight: torch.Size([64])\n",
      "text_model.bn3.bias: torch.Size([64])\n",
      "text_model.conv4.weight: torch.Size([32, 64, 3])\n",
      "text_model.conv4.bias: torch.Size([32])\n",
      "text_model.bn4.weight: torch.Size([32])\n",
      "text_model.bn4.bias: torch.Size([32])\n",
      "text_model.attention.qkv.weight: torch.Size([96, 32])\n",
      "text_model.attention.qkv.bias: torch.Size([96])\n",
      "text_model.attention.projection.weight: torch.Size([32, 32])\n",
      "text_model.attention.projection.bias: torch.Size([32])\n",
      "text_model.dense.weight: torch.Size([256, 16000])\n",
      "text_model.dense.bias: torch.Size([256])\n",
      "text_model.bn5.weight: torch.Size([256])\n",
      "text_model.bn5.bias: torch.Size([256])\n",
      "speech_model.dense1.weight: torch.Size([1024, 3400])\n",
      "speech_model.dense1.bias: torch.Size([1024])\n",
      "speech_model.bn1.weight: torch.Size([1024])\n",
      "speech_model.bn1.bias: torch.Size([1024])\n",
      "speech_model.attention.qkv.weight: torch.Size([3072, 1024])\n",
      "speech_model.attention.qkv.bias: torch.Size([3072])\n",
      "speech_model.attention.projection.weight: torch.Size([1024, 1024])\n",
      "speech_model.attention.projection.bias: torch.Size([1024])\n",
      "speech_model.dense2.weight: torch.Size([256, 1024])\n",
      "speech_model.dense2.bias: torch.Size([256])\n",
      "speech_model.bn2.weight: torch.Size([256])\n",
      "speech_model.bn2.bias: torch.Size([256])\n",
      "mocap_model.conv1.weight: torch.Size([32, 1, 3, 3])\n",
      "mocap_model.conv1.bias: torch.Size([32])\n",
      "mocap_model.bn1.weight: torch.Size([32])\n",
      "mocap_model.bn1.bias: torch.Size([32])\n",
      "mocap_model.conv2.weight: torch.Size([64, 32, 3, 3])\n",
      "mocap_model.conv2.bias: torch.Size([64])\n",
      "mocap_model.bn2.weight: torch.Size([64])\n",
      "mocap_model.bn2.bias: torch.Size([64])\n",
      "mocap_model.conv3.weight: torch.Size([64, 64, 3, 3])\n",
      "mocap_model.conv3.bias: torch.Size([64])\n",
      "mocap_model.bn3.weight: torch.Size([64])\n",
      "mocap_model.bn3.bias: torch.Size([64])\n",
      "mocap_model.conv4.weight: torch.Size([128, 64, 3, 3])\n",
      "mocap_model.conv4.bias: torch.Size([128])\n",
      "mocap_model.bn4.weight: torch.Size([128])\n",
      "mocap_model.bn4.bias: torch.Size([128])\n",
      "mocap_model.conv5.weight: torch.Size([256, 128, 3, 3])\n",
      "mocap_model.conv5.bias: torch.Size([256])\n",
      "mocap_model.bn5.weight: torch.Size([256])\n",
      "mocap_model.bn5.bias: torch.Size([256])\n",
      "mocap_model.dense.weight: torch.Size([256, 10752])\n",
      "mocap_model.dense.bias: torch.Size([256])\n",
      "mocap_model.bn8.weight: torch.Size([256])\n",
      "mocap_model.bn8.bias: torch.Size([256])\n",
      "dynamic_query.temperature: torch.Size([1])\n",
      "dynamic_query.modal_encoders.0.fc1.weight: torch.Size([128, 256])\n",
      "dynamic_query.modal_encoders.0.fc1.bias: torch.Size([128])\n",
      "dynamic_query.modal_encoders.0.fc2.weight: torch.Size([256, 128])\n",
      "dynamic_query.modal_encoders.0.fc2.bias: torch.Size([256])\n",
      "dynamic_query.modal_encoders.0.norm.weight: torch.Size([256])\n",
      "dynamic_query.modal_encoders.0.norm.bias: torch.Size([256])\n",
      "dynamic_query.modal_encoders.1.fc1.weight: torch.Size([128, 256])\n",
      "dynamic_query.modal_encoders.1.fc1.bias: torch.Size([128])\n",
      "dynamic_query.modal_encoders.1.fc2.weight: torch.Size([256, 128])\n",
      "dynamic_query.modal_encoders.1.fc2.bias: torch.Size([256])\n",
      "dynamic_query.modal_encoders.1.norm.weight: torch.Size([256])\n",
      "dynamic_query.modal_encoders.1.norm.bias: torch.Size([256])\n",
      "dynamic_query.modal_encoders.2.fc1.weight: torch.Size([128, 256])\n",
      "dynamic_query.modal_encoders.2.fc1.bias: torch.Size([128])\n",
      "dynamic_query.modal_encoders.2.fc2.weight: torch.Size([256, 128])\n",
      "dynamic_query.modal_encoders.2.fc2.bias: torch.Size([256])\n",
      "dynamic_query.modal_encoders.2.norm.weight: torch.Size([256])\n",
      "dynamic_query.modal_encoders.2.norm.bias: torch.Size([256])\n",
      "dynamic_query.selector.0.weight: torch.Size([256, 768])\n",
      "dynamic_query.selector.0.bias: torch.Size([256])\n",
      "dynamic_query.selector.2.weight: torch.Size([128, 256])\n",
      "dynamic_query.selector.2.bias: torch.Size([128])\n",
      "dynamic_query.selector.4.weight: torch.Size([3, 128])\n",
      "dynamic_query.selector.4.bias: torch.Size([3])\n",
      "mixed_attention.multihead_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "mixed_attention.multihead_attn.in_proj_bias: torch.Size([2304])\n",
      "mixed_attention.multihead_attn.out_proj.weight: torch.Size([768, 768])\n",
      "mixed_attention.multihead_attn.out_proj.bias: torch.Size([768])\n",
      "fc1.weight: torch.Size([128, 1024])\n",
      "fc1.bias: torch.Size([128])\n",
      "bn1.weight: torch.Size([128])\n",
      "bn1.bias: torch.Size([128])\n",
      "fc2.weight: torch.Size([4, 128])\n",
      "fc2.bias: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# 设置设备为GPU或CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 检查 class_weights 是否已经在正确的设备上\n",
    "if not isinstance(class_weights, torch.Tensor):\n",
    "    class_weights = torch.FloatTensor(class_weights)\n",
    "\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# 模型实例化\n",
    "model_combined = CombinedModel(nb_words, embedding_dim, max_sequence_length, g_word_embedding_matrix, num_heads)\n",
    "\n",
    "# 打印模型摘要\n",
    "print(model_combined)\n",
    "\n",
    "print(\"Combined Model Summary:\")\n",
    "print_model_summary(model_combined, [(max_sequence_length,), (100, 34), (200, 189, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7c603d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:59:14.670233Z",
     "iopub.status.busy": "2024-09-18T17:59:14.669296Z",
     "iopub.status.idle": "2024-09-18T17:59:15.855322Z",
     "shell.execute_reply": "2024-09-18T17:59:15.853910Z"
    },
    "papermill": {
     "duration": 1.23316,
     "end_time": "2024-09-18T17:59:15.858188",
     "exception": false,
     "start_time": "2024-09-18T17:59:14.625028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "Output shape: torch.Size([128, 4])\n",
      "Weights shape: torch.Size([128, 3])\n"
     ]
    }
   ],
   "source": [
    "# 使用DataParallel包装模型\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model_combined = nn.DataParallel(model_combined)\n",
    "\n",
    "# 将模型移动到设备上\n",
    "model_combined.to(device)\n",
    "\n",
    "# 假设我们有一个batch\n",
    "batch = next(iter(train_loader))\n",
    "text = batch['text'].to(device).long()\n",
    "speech = batch['speech'].to(device)\n",
    "mocap = batch['mocap'].to(device)\n",
    "\n",
    "# 前向传播\n",
    "outputs, weights = model_combined(text, speech, mocap)\n",
    "\n",
    "# 打印输出形状\n",
    "print(\"Output shape:\", outputs.shape)  # 打印模型主要输出的形状\n",
    "print(\"Weights shape:\", weights.shape)  # 打印权重的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac03773e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T17:59:15.964461Z",
     "iopub.status.busy": "2024-09-18T17:59:15.963573Z",
     "iopub.status.idle": "2024-09-18T18:15:19.283305Z",
     "shell.execute_reply": "2024-09-18T18:15:19.281971Z"
    },
    "papermill": {
     "duration": 963.374979,
     "end_time": "2024-09-18T18:15:19.285576",
     "exception": false,
     "start_time": "2024-09-18T17:59:15.910597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], time: 9.70s\n",
      "Train - loss: 1.1955, F1: 0.4684, UW_Acc: 0.4731\n",
      "Train Weights - Text: 0.3356, Speech: 0.2264, Mocap: 0.4380\n",
      "Val - loss: 1.2933, F1: 0.3855, UW_Acc: 0.4520\n",
      "Val Weights - Text: 0.1360, Speech: 0.2895, Mocap: 0.5746\n",
      "Epoch [2/100], time: 9.42s\n",
      "Train - loss: 0.8886, F1: 0.6452, UW_Acc: 0.6484\n",
      "Train Weights - Text: 0.1572, Speech: 0.3463, Mocap: 0.4965\n",
      "Val - loss: 1.0031, F1: 0.5603, UW_Acc: 0.5816\n",
      "Val Weights - Text: 0.1072, Speech: 0.3224, Mocap: 0.5704\n",
      "Epoch [3/100], time: 9.34s\n",
      "Train - loss: 0.7579, F1: 0.7127, UW_Acc: 0.7132\n",
      "Train Weights - Text: 0.1344, Speech: 0.4022, Mocap: 0.4635\n",
      "Val - loss: 0.9595, F1: 0.5933, UW_Acc: 0.6128\n",
      "Val Weights - Text: 0.1177, Speech: 0.3731, Mocap: 0.5092\n",
      "Epoch [4/100], time: 9.41s\n",
      "Train - loss: 0.6733, F1: 0.7492, UW_Acc: 0.7493\n",
      "Train Weights - Text: 0.1399, Speech: 0.4079, Mocap: 0.4522\n",
      "Val - loss: 1.2069, F1: 0.5070, UW_Acc: 0.5523\n",
      "Val Weights - Text: 0.0987, Speech: 0.4834, Mocap: 0.4179\n",
      "Epoch [5/100], time: 9.41s\n",
      "Train - loss: 0.5863, F1: 0.7918, UW_Acc: 0.7913\n",
      "Train Weights - Text: 0.1201, Speech: 0.4663, Mocap: 0.4136\n",
      "Val - loss: 1.0079, F1: 0.6073, UW_Acc: 0.6177\n",
      "Val Weights - Text: 0.1200, Speech: 0.3693, Mocap: 0.5107\n",
      "Epoch [6/100], time: 9.43s\n",
      "Train - loss: 0.5250, F1: 0.8189, UW_Acc: 0.8190\n",
      "Train Weights - Text: 0.1368, Speech: 0.4523, Mocap: 0.4109\n",
      "Val - loss: 1.0812, F1: 0.5623, UW_Acc: 0.5851\n",
      "Val Weights - Text: 0.1164, Speech: 0.4562, Mocap: 0.4274\n",
      "Epoch [7/100], time: 9.41s\n",
      "Train - loss: 0.4784, F1: 0.8346, UW_Acc: 0.8348\n",
      "Train Weights - Text: 0.1283, Speech: 0.4536, Mocap: 0.4181\n",
      "Val - loss: 0.8943, F1: 0.6556, UW_Acc: 0.6589\n",
      "Val Weights - Text: 0.1220, Speech: 0.4676, Mocap: 0.4104\n",
      "Epoch [8/100], time: 9.56s\n",
      "Train - loss: 0.4167, F1: 0.8619, UW_Acc: 0.8617\n",
      "Train Weights - Text: 0.1309, Speech: 0.4691, Mocap: 0.4000\n",
      "Val - loss: 0.9266, F1: 0.6476, UW_Acc: 0.6529\n",
      "Val Weights - Text: 0.1263, Speech: 0.4446, Mocap: 0.4291\n",
      "Epoch [9/100], time: 9.41s\n",
      "Train - loss: 0.3778, F1: 0.8805, UW_Acc: 0.8795\n",
      "Train Weights - Text: 0.1471, Speech: 0.4588, Mocap: 0.3941\n",
      "Val - loss: 0.9033, F1: 0.6512, UW_Acc: 0.6562\n",
      "Val Weights - Text: 0.1477, Speech: 0.4653, Mocap: 0.3871\n",
      "Epoch [10/100], time: 9.54s\n",
      "Train - loss: 0.3496, F1: 0.8875, UW_Acc: 0.8875\n",
      "Train Weights - Text: 0.1695, Speech: 0.4620, Mocap: 0.3685\n",
      "Val - loss: 0.9983, F1: 0.6314, UW_Acc: 0.6408\n",
      "Val Weights - Text: 0.1308, Speech: 0.4470, Mocap: 0.4221\n",
      "Epoch [11/100], time: 9.43s\n",
      "Train - loss: 0.3123, F1: 0.9025, UW_Acc: 0.9024\n",
      "Train Weights - Text: 0.1497, Speech: 0.4678, Mocap: 0.3825\n",
      "Val - loss: 1.0928, F1: 0.6122, UW_Acc: 0.6222\n",
      "Val Weights - Text: 0.1607, Speech: 0.4523, Mocap: 0.3869\n",
      "Epoch [12/100], time: 9.58s\n",
      "Train - loss: 0.2864, F1: 0.9173, UW_Acc: 0.9171\n",
      "Train Weights - Text: 0.1717, Speech: 0.4690, Mocap: 0.3593\n",
      "Val - loss: 1.1256, F1: 0.6270, UW_Acc: 0.6335\n",
      "Val Weights - Text: 0.1603, Speech: 0.4910, Mocap: 0.3487\n",
      "Epoch [13/100], time: 9.56s\n",
      "Train - loss: 0.2693, F1: 0.9218, UW_Acc: 0.9217\n",
      "Train Weights - Text: 0.1853, Speech: 0.4618, Mocap: 0.3529\n",
      "Val - loss: 1.2945, F1: 0.5794, UW_Acc: 0.5939\n",
      "Val Weights - Text: 0.1382, Speech: 0.4647, Mocap: 0.3971\n",
      "Epoch 00013: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 00013: reducing learning rate of group 1 to 5.0000e-04.\n",
      "Epoch [14/100], time: 9.52s\n",
      "Train - loss: 0.2245, F1: 0.9409, UW_Acc: 0.9407\n",
      "Train Weights - Text: 0.1644, Speech: 0.4753, Mocap: 0.3603\n",
      "Val - loss: 0.9954, F1: 0.6565, UW_Acc: 0.6608\n",
      "Val Weights - Text: 0.1855, Speech: 0.4473, Mocap: 0.3673\n",
      "Epoch [15/100], time: 9.55s\n",
      "Train - loss: 0.1890, F1: 0.9561, UW_Acc: 0.9560\n",
      "Train Weights - Text: 0.1873, Speech: 0.4639, Mocap: 0.3488\n",
      "Val - loss: 1.2163, F1: 0.6058, UW_Acc: 0.6146\n",
      "Val Weights - Text: 0.1879, Speech: 0.4928, Mocap: 0.3193\n",
      "Epoch [16/100], time: 9.54s\n",
      "Train - loss: 0.1736, F1: 0.9608, UW_Acc: 0.9606\n",
      "Train Weights - Text: 0.2274, Speech: 0.4582, Mocap: 0.3144\n",
      "Val - loss: 1.0628, F1: 0.6409, UW_Acc: 0.6468\n",
      "Val Weights - Text: 0.2272, Speech: 0.4464, Mocap: 0.3264\n",
      "Epoch [17/100], time: 9.57s\n",
      "Train - loss: 0.1633, F1: 0.9632, UW_Acc: 0.9632\n",
      "Train Weights - Text: 0.2297, Speech: 0.4596, Mocap: 0.3106\n",
      "Val - loss: 0.9705, F1: 0.6850, UW_Acc: 0.6878\n",
      "Val Weights - Text: 0.2029, Speech: 0.4337, Mocap: 0.3634\n",
      "Epoch [18/100], time: 9.62s\n",
      "Train - loss: 0.1578, F1: 0.9601, UW_Acc: 0.9599\n",
      "Train Weights - Text: 0.2383, Speech: 0.4440, Mocap: 0.3176\n",
      "Val - loss: 0.9793, F1: 0.6885, UW_Acc: 0.6909\n",
      "Val Weights - Text: 0.2063, Speech: 0.4512, Mocap: 0.3426\n",
      "Epoch [19/100], time: 9.56s\n",
      "Train - loss: 0.1573, F1: 0.9613, UW_Acc: 0.9612\n",
      "Train Weights - Text: 0.2355, Speech: 0.4398, Mocap: 0.3247\n",
      "Val - loss: 1.1635, F1: 0.6415, UW_Acc: 0.6489\n",
      "Val Weights - Text: 0.2352, Speech: 0.4464, Mocap: 0.3184\n",
      "Epoch [20/100], time: 9.50s\n",
      "Train - loss: 0.1418, F1: 0.9716, UW_Acc: 0.9715\n",
      "Train Weights - Text: 0.2459, Speech: 0.4357, Mocap: 0.3184\n",
      "Val - loss: 1.0948, F1: 0.6480, UW_Acc: 0.6524\n",
      "Val Weights - Text: 0.2280, Speech: 0.4237, Mocap: 0.3483\n",
      "Epoch [21/100], time: 9.55s\n",
      "Train - loss: 0.1378, F1: 0.9690, UW_Acc: 0.9689\n",
      "Train Weights - Text: 0.2457, Speech: 0.4228, Mocap: 0.3316\n",
      "Val - loss: 1.1019, F1: 0.6457, UW_Acc: 0.6503\n",
      "Val Weights - Text: 0.2077, Speech: 0.4094, Mocap: 0.3829\n",
      "Epoch [22/100], time: 9.56s\n",
      "Train - loss: 0.1302, F1: 0.9727, UW_Acc: 0.9727\n",
      "Train Weights - Text: 0.2696, Speech: 0.4112, Mocap: 0.3192\n",
      "Val - loss: 1.1432, F1: 0.6540, UW_Acc: 0.6587\n",
      "Val Weights - Text: 0.1856, Speech: 0.4467, Mocap: 0.3677\n",
      "Epoch [23/100], time: 9.49s\n",
      "Train - loss: 0.1243, F1: 0.9751, UW_Acc: 0.9750\n",
      "Train Weights - Text: 0.2659, Speech: 0.4163, Mocap: 0.3179\n",
      "Val - loss: 1.1381, F1: 0.6548, UW_Acc: 0.6598\n",
      "Val Weights - Text: 0.1926, Speech: 0.4579, Mocap: 0.3496\n",
      "Epoch [24/100], time: 9.54s\n",
      "Train - loss: 0.1285, F1: 0.9706, UW_Acc: 0.9704\n",
      "Train Weights - Text: 0.2746, Speech: 0.4007, Mocap: 0.3247\n",
      "Val - loss: 1.0837, F1: 0.6760, UW_Acc: 0.6792\n",
      "Val Weights - Text: 0.2516, Speech: 0.4331, Mocap: 0.3153\n",
      "Epoch 00024: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 00024: reducing learning rate of group 1 to 2.5000e-04.\n",
      "Epoch [25/100], time: 9.49s\n",
      "Train - loss: 0.1053, F1: 0.9814, UW_Acc: 0.9813\n",
      "Train Weights - Text: 0.2747, Speech: 0.4007, Mocap: 0.3246\n",
      "Val - loss: 1.0840, F1: 0.6806, UW_Acc: 0.6834\n",
      "Val Weights - Text: 0.2339, Speech: 0.4421, Mocap: 0.3240\n",
      "Epoch [26/100], time: 9.48s\n",
      "Train - loss: 0.0908, F1: 0.9864, UW_Acc: 0.9864\n",
      "Train Weights - Text: 0.2845, Speech: 0.4066, Mocap: 0.3089\n",
      "Val - loss: 1.2754, F1: 0.6341, UW_Acc: 0.6398\n",
      "Val Weights - Text: 0.2347, Speech: 0.4567, Mocap: 0.3086\n",
      "Epoch [27/100], time: 9.50s\n",
      "Train - loss: 0.0902, F1: 0.9864, UW_Acc: 0.9864\n",
      "Train Weights - Text: 0.2918, Speech: 0.3961, Mocap: 0.3121\n",
      "Val - loss: 1.2207, F1: 0.6397, UW_Acc: 0.6460\n",
      "Val Weights - Text: 0.2627, Speech: 0.4255, Mocap: 0.3118\n",
      "Epoch [28/100], time: 9.61s\n",
      "Train - loss: 0.0881, F1: 0.9889, UW_Acc: 0.9889\n",
      "Train Weights - Text: 0.2967, Speech: 0.3898, Mocap: 0.3135\n",
      "Val - loss: 1.1766, F1: 0.6617, UW_Acc: 0.6667\n",
      "Val Weights - Text: 0.2143, Speech: 0.4249, Mocap: 0.3608\n",
      "Epoch [29/100], time: 9.52s\n",
      "Train - loss: 0.0874, F1: 0.9869, UW_Acc: 0.9868\n",
      "Train Weights - Text: 0.2766, Speech: 0.3969, Mocap: 0.3264\n",
      "Val - loss: 1.1816, F1: 0.6608, UW_Acc: 0.6641\n",
      "Val Weights - Text: 0.2624, Speech: 0.4261, Mocap: 0.3115\n",
      "Epoch [30/100], time: 9.55s\n",
      "Train - loss: 0.0900, F1: 0.9845, UW_Acc: 0.9845\n",
      "Train Weights - Text: 0.3100, Speech: 0.3910, Mocap: 0.2990\n",
      "Val - loss: 1.1884, F1: 0.6662, UW_Acc: 0.6694\n",
      "Val Weights - Text: 0.2465, Speech: 0.4425, Mocap: 0.3110\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 00030: reducing learning rate of group 1 to 1.2500e-04.\n",
      "Epoch [31/100], time: 9.79s\n",
      "Train - loss: 0.0763, F1: 0.9916, UW_Acc: 0.9916\n",
      "Train Weights - Text: 0.2968, Speech: 0.3925, Mocap: 0.3107\n",
      "Val - loss: 1.2160, F1: 0.6526, UW_Acc: 0.6563\n",
      "Val Weights - Text: 0.2263, Speech: 0.4216, Mocap: 0.3521\n",
      "Epoch [32/100], time: 9.54s\n",
      "Train - loss: 0.0749, F1: 0.9916, UW_Acc: 0.9916\n",
      "Train Weights - Text: 0.3133, Speech: 0.3848, Mocap: 0.3018\n",
      "Val - loss: 1.2442, F1: 0.6515, UW_Acc: 0.6554\n",
      "Val Weights - Text: 0.2377, Speech: 0.4470, Mocap: 0.3153\n",
      "Epoch [33/100], time: 9.53s\n",
      "Train - loss: 0.0698, F1: 0.9932, UW_Acc: 0.9932\n",
      "Train Weights - Text: 0.3100, Speech: 0.3863, Mocap: 0.3038\n",
      "Val - loss: 1.1889, F1: 0.6690, UW_Acc: 0.6714\n",
      "Val Weights - Text: 0.2555, Speech: 0.4146, Mocap: 0.3299\n",
      "Epoch [34/100], time: 9.53s\n",
      "Train - loss: 0.0695, F1: 0.9935, UW_Acc: 0.9935\n",
      "Train Weights - Text: 0.3182, Speech: 0.3712, Mocap: 0.3106\n",
      "Val - loss: 1.2299, F1: 0.6460, UW_Acc: 0.6495\n",
      "Val Weights - Text: 0.2613, Speech: 0.4122, Mocap: 0.3266\n",
      "Epoch [35/100], time: 9.64s\n",
      "Train - loss: 0.0661, F1: 0.9924, UW_Acc: 0.9924\n",
      "Train Weights - Text: 0.3177, Speech: 0.3775, Mocap: 0.3048\n",
      "Val - loss: 1.2153, F1: 0.6514, UW_Acc: 0.6534\n",
      "Val Weights - Text: 0.2736, Speech: 0.4212, Mocap: 0.3053\n",
      "Epoch [36/100], time: 9.60s\n",
      "Train - loss: 0.0679, F1: 0.9920, UW_Acc: 0.9919\n",
      "Train Weights - Text: 0.3283, Speech: 0.3740, Mocap: 0.2976\n",
      "Val - loss: 1.2734, F1: 0.6466, UW_Acc: 0.6509\n",
      "Val Weights - Text: 0.2791, Speech: 0.4082, Mocap: 0.3128\n",
      "Epoch 00036: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch 00036: reducing learning rate of group 1 to 6.2500e-05.\n",
      "Epoch [37/100], time: 9.53s\n",
      "Train - loss: 0.0633, F1: 0.9947, UW_Acc: 0.9946\n",
      "Train Weights - Text: 0.3327, Speech: 0.3627, Mocap: 0.3046\n",
      "Val - loss: 1.2838, F1: 0.6604, UW_Acc: 0.6632\n",
      "Val Weights - Text: 0.2603, Speech: 0.4176, Mocap: 0.3220\n",
      "Epoch [38/100], time: 9.64s\n",
      "Train - loss: 0.0592, F1: 0.9964, UW_Acc: 0.9964\n",
      "Train Weights - Text: 0.3249, Speech: 0.3683, Mocap: 0.3068\n",
      "Val - loss: 1.2512, F1: 0.6650, UW_Acc: 0.6677\n",
      "Val Weights - Text: 0.2566, Speech: 0.4184, Mocap: 0.3250\n",
      "Epoch [39/100], time: 9.61s\n",
      "Train - loss: 0.0598, F1: 0.9953, UW_Acc: 0.9953\n",
      "Train Weights - Text: 0.3198, Speech: 0.3790, Mocap: 0.3012\n",
      "Val - loss: 1.2887, F1: 0.6558, UW_Acc: 0.6591\n",
      "Val Weights - Text: 0.2654, Speech: 0.4185, Mocap: 0.3161\n",
      "Epoch [40/100], time: 9.49s\n",
      "Train - loss: 0.0634, F1: 0.9947, UW_Acc: 0.9946\n",
      "Train Weights - Text: 0.3378, Speech: 0.3609, Mocap: 0.3013\n",
      "Val - loss: 1.3380, F1: 0.6528, UW_Acc: 0.6569\n",
      "Val Weights - Text: 0.2760, Speech: 0.4125, Mocap: 0.3116\n",
      "Epoch [41/100], time: 9.75s\n",
      "Train - loss: 0.0632, F1: 0.9937, UW_Acc: 0.9937\n",
      "Train Weights - Text: 0.3334, Speech: 0.3656, Mocap: 0.3010\n",
      "Val - loss: 1.2648, F1: 0.6711, UW_Acc: 0.6739\n",
      "Val Weights - Text: 0.2746, Speech: 0.4039, Mocap: 0.3214\n",
      "Epoch [42/100], time: 9.57s\n",
      "Train - loss: 0.0608, F1: 0.9959, UW_Acc: 0.9959\n",
      "Train Weights - Text: 0.3341, Speech: 0.3628, Mocap: 0.3031\n",
      "Val - loss: 1.2948, F1: 0.6562, UW_Acc: 0.6589\n",
      "Val Weights - Text: 0.2555, Speech: 0.4249, Mocap: 0.3197\n",
      "Epoch 00042: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch 00042: reducing learning rate of group 1 to 3.1250e-05.\n",
      "Epoch [43/100], time: 9.55s\n",
      "Train - loss: 0.0572, F1: 0.9956, UW_Acc: 0.9956\n",
      "Train Weights - Text: 0.3398, Speech: 0.3593, Mocap: 0.3009\n",
      "Val - loss: 1.3430, F1: 0.6568, UW_Acc: 0.6599\n",
      "Val Weights - Text: 0.2629, Speech: 0.4269, Mocap: 0.3102\n",
      "Epoch [44/100], time: 9.56s\n",
      "Train - loss: 0.0534, F1: 0.9978, UW_Acc: 0.9978\n",
      "Train Weights - Text: 0.3364, Speech: 0.3625, Mocap: 0.3011\n",
      "Val - loss: 1.3094, F1: 0.6534, UW_Acc: 0.6560\n",
      "Val Weights - Text: 0.2707, Speech: 0.4150, Mocap: 0.3143\n",
      "Epoch [45/100], time: 9.71s\n",
      "Train - loss: 0.0557, F1: 0.9962, UW_Acc: 0.9962\n",
      "Train Weights - Text: 0.3361, Speech: 0.3621, Mocap: 0.3018\n",
      "Val - loss: 1.3497, F1: 0.6539, UW_Acc: 0.6573\n",
      "Val Weights - Text: 0.2674, Speech: 0.4170, Mocap: 0.3156\n",
      "Epoch [46/100], time: 9.60s\n",
      "Train - loss: 0.0557, F1: 0.9957, UW_Acc: 0.9956\n",
      "Train Weights - Text: 0.3402, Speech: 0.3600, Mocap: 0.2998\n",
      "Val - loss: 1.3749, F1: 0.6394, UW_Acc: 0.6437\n",
      "Val Weights - Text: 0.2700, Speech: 0.4198, Mocap: 0.3102\n",
      "Epoch [47/100], time: 9.63s\n",
      "Train - loss: 0.0561, F1: 0.9960, UW_Acc: 0.9960\n",
      "Train Weights - Text: 0.3428, Speech: 0.3567, Mocap: 0.3005\n",
      "Val - loss: 1.3405, F1: 0.6587, UW_Acc: 0.6614\n",
      "Val Weights - Text: 0.2665, Speech: 0.4200, Mocap: 0.3136\n",
      "Epoch [48/100], time: 9.62s\n",
      "Train - loss: 0.0549, F1: 0.9962, UW_Acc: 0.9962\n",
      "Train Weights - Text: 0.3464, Speech: 0.3565, Mocap: 0.2970\n",
      "Val - loss: 1.3400, F1: 0.6609, UW_Acc: 0.6632\n",
      "Val Weights - Text: 0.2781, Speech: 0.4147, Mocap: 0.3072\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch 00048: reducing learning rate of group 1 to 1.5625e-05.\n",
      "Epoch [49/100], time: 9.63s\n",
      "Train - loss: 0.0529, F1: 0.9975, UW_Acc: 0.9975\n",
      "Train Weights - Text: 0.3516, Speech: 0.3518, Mocap: 0.2965\n",
      "Val - loss: 1.3788, F1: 0.6501, UW_Acc: 0.6530\n",
      "Val Weights - Text: 0.2698, Speech: 0.4163, Mocap: 0.3139\n",
      "Epoch [50/100], time: 9.65s\n",
      "Train - loss: 0.0568, F1: 0.9957, UW_Acc: 0.9957\n",
      "Train Weights - Text: 0.3446, Speech: 0.3557, Mocap: 0.2997\n",
      "Val - loss: 1.3650, F1: 0.6574, UW_Acc: 0.6599\n",
      "Val Weights - Text: 0.2698, Speech: 0.4167, Mocap: 0.3135\n",
      "Epoch [51/100], time: 9.74s\n",
      "Train - loss: 0.0544, F1: 0.9970, UW_Acc: 0.9970\n",
      "Train Weights - Text: 0.3472, Speech: 0.3532, Mocap: 0.2996\n",
      "Val - loss: 1.3502, F1: 0.6552, UW_Acc: 0.6577\n",
      "Val Weights - Text: 0.2686, Speech: 0.4190, Mocap: 0.3124\n",
      "Epoch [52/100], time: 9.57s\n",
      "Train - loss: 0.0550, F1: 0.9960, UW_Acc: 0.9960\n",
      "Train Weights - Text: 0.3459, Speech: 0.3561, Mocap: 0.2979\n",
      "Val - loss: 1.3690, F1: 0.6532, UW_Acc: 0.6565\n",
      "Val Weights - Text: 0.2659, Speech: 0.4217, Mocap: 0.3125\n",
      "Epoch [53/100], time: 9.56s\n",
      "Train - loss: 0.0508, F1: 0.9978, UW_Acc: 0.9978\n",
      "Train Weights - Text: 0.3382, Speech: 0.3623, Mocap: 0.2995\n",
      "Val - loss: 1.3887, F1: 0.6458, UW_Acc: 0.6489\n",
      "Val Weights - Text: 0.2592, Speech: 0.4310, Mocap: 0.3098\n",
      "Epoch [54/100], time: 9.60s\n",
      "Train - loss: 0.0539, F1: 0.9967, UW_Acc: 0.9966\n",
      "Train Weights - Text: 0.3372, Speech: 0.3609, Mocap: 0.3018\n",
      "Val - loss: 1.3830, F1: 0.6568, UW_Acc: 0.6595\n",
      "Val Weights - Text: 0.2625, Speech: 0.4210, Mocap: 0.3166\n",
      "Epoch 00054: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch 00054: reducing learning rate of group 1 to 7.8125e-06.\n",
      "Epoch [55/100], time: 9.66s\n",
      "Train - loss: 0.0526, F1: 0.9978, UW_Acc: 0.9978\n",
      "Train Weights - Text: 0.3422, Speech: 0.3574, Mocap: 0.3005\n",
      "Val - loss: 1.3496, F1: 0.6598, UW_Acc: 0.6624\n",
      "Val Weights - Text: 0.2666, Speech: 0.4177, Mocap: 0.3157\n",
      "Epoch [56/100], time: 9.63s\n",
      "Train - loss: 0.0543, F1: 0.9964, UW_Acc: 0.9964\n",
      "Train Weights - Text: 0.3423, Speech: 0.3558, Mocap: 0.3019\n",
      "Val - loss: 1.3799, F1: 0.6520, UW_Acc: 0.6550\n",
      "Val Weights - Text: 0.2663, Speech: 0.4180, Mocap: 0.3157\n",
      "Epoch [57/100], time: 9.65s\n",
      "Train - loss: 0.0522, F1: 0.9969, UW_Acc: 0.9969\n",
      "Train Weights - Text: 0.3470, Speech: 0.3549, Mocap: 0.2981\n",
      "Val - loss: 1.3430, F1: 0.6587, UW_Acc: 0.6607\n",
      "Val Weights - Text: 0.2677, Speech: 0.4174, Mocap: 0.3149\n",
      "Epoch [58/100], time: 9.67s\n",
      "Train - loss: 0.0524, F1: 0.9975, UW_Acc: 0.9975\n",
      "Train Weights - Text: 0.3438, Speech: 0.3574, Mocap: 0.2987\n",
      "Val - loss: 1.3826, F1: 0.6470, UW_Acc: 0.6509\n",
      "Val Weights - Text: 0.2639, Speech: 0.4214, Mocap: 0.3147\n",
      "Epoch [59/100], time: 9.61s\n",
      "Train - loss: 0.0524, F1: 0.9976, UW_Acc: 0.9976\n",
      "Train Weights - Text: 0.3438, Speech: 0.3587, Mocap: 0.2976\n",
      "Val - loss: 1.3377, F1: 0.6590, UW_Acc: 0.6608\n",
      "Val Weights - Text: 0.2670, Speech: 0.4187, Mocap: 0.3143\n",
      "Epoch [60/100], time: 9.59s\n",
      "Train - loss: 0.0541, F1: 0.9958, UW_Acc: 0.9958\n",
      "Train Weights - Text: 0.3415, Speech: 0.3587, Mocap: 0.2998\n",
      "Val - loss: 1.3641, F1: 0.6512, UW_Acc: 0.6538\n",
      "Val Weights - Text: 0.2703, Speech: 0.4170, Mocap: 0.3127\n",
      "Epoch 00060: reducing learning rate of group 0 to 3.9063e-07.\n",
      "Epoch 00060: reducing learning rate of group 1 to 3.9063e-06.\n",
      "Epoch [61/100], time: 9.66s\n",
      "Train - loss: 0.0540, F1: 0.9971, UW_Acc: 0.9971\n",
      "Train Weights - Text: 0.3458, Speech: 0.3553, Mocap: 0.2989\n",
      "Val - loss: 1.3664, F1: 0.6527, UW_Acc: 0.6556\n",
      "Val Weights - Text: 0.2692, Speech: 0.4175, Mocap: 0.3134\n",
      "Epoch [62/100], time: 9.62s\n",
      "Train - loss: 0.0560, F1: 0.9958, UW_Acc: 0.9958\n",
      "Train Weights - Text: 0.3452, Speech: 0.3566, Mocap: 0.2982\n",
      "Val - loss: 1.3637, F1: 0.6568, UW_Acc: 0.6589\n",
      "Val Weights - Text: 0.2686, Speech: 0.4195, Mocap: 0.3119\n",
      "Epoch [63/100], time: 9.64s\n",
      "Train - loss: 0.0539, F1: 0.9975, UW_Acc: 0.9975\n",
      "Train Weights - Text: 0.3456, Speech: 0.3559, Mocap: 0.2985\n",
      "Val - loss: 1.3881, F1: 0.6498, UW_Acc: 0.6526\n",
      "Val Weights - Text: 0.2710, Speech: 0.4157, Mocap: 0.3133\n",
      "Epoch [64/100], time: 9.95s\n",
      "Train - loss: 0.0522, F1: 0.9976, UW_Acc: 0.9976\n",
      "Train Weights - Text: 0.3460, Speech: 0.3562, Mocap: 0.2978\n",
      "Val - loss: 1.3502, F1: 0.6543, UW_Acc: 0.6563\n",
      "Val Weights - Text: 0.2682, Speech: 0.4164, Mocap: 0.3154\n",
      "Epoch [65/100], time: 9.69s\n",
      "Train - loss: 0.0522, F1: 0.9978, UW_Acc: 0.9978\n",
      "Train Weights - Text: 0.3477, Speech: 0.3541, Mocap: 0.2983\n",
      "Val - loss: 1.4023, F1: 0.6507, UW_Acc: 0.6542\n",
      "Val Weights - Text: 0.2729, Speech: 0.4170, Mocap: 0.3101\n",
      "Epoch [66/100], time: 9.58s\n",
      "Train - loss: 0.0540, F1: 0.9966, UW_Acc: 0.9965\n",
      "Train Weights - Text: 0.3452, Speech: 0.3570, Mocap: 0.2978\n",
      "Val - loss: 1.3578, F1: 0.6557, UW_Acc: 0.6579\n",
      "Val Weights - Text: 0.2716, Speech: 0.4175, Mocap: 0.3109\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.9531e-07.\n",
      "Epoch 00066: reducing learning rate of group 1 to 1.9531e-06.\n",
      "Epoch [67/100], time: 9.60s\n",
      "Train - loss: 0.0537, F1: 0.9969, UW_Acc: 0.9969\n",
      "Train Weights - Text: 0.3478, Speech: 0.3559, Mocap: 0.2963\n",
      "Val - loss: 1.3782, F1: 0.6531, UW_Acc: 0.6562\n",
      "Val Weights - Text: 0.2732, Speech: 0.4166, Mocap: 0.3102\n",
      "Epoch [68/100], time: 9.63s\n",
      "Train - loss: 0.0530, F1: 0.9966, UW_Acc: 0.9965\n",
      "Train Weights - Text: 0.3473, Speech: 0.3557, Mocap: 0.2970\n",
      "Val - loss: 1.4024, F1: 0.6551, UW_Acc: 0.6579\n",
      "Val Weights - Text: 0.2731, Speech: 0.4190, Mocap: 0.3079\n",
      "Epoch [69/100], time: 9.71s\n",
      "Train - loss: 0.0552, F1: 0.9962, UW_Acc: 0.9962\n",
      "Train Weights - Text: 0.3458, Speech: 0.3556, Mocap: 0.2986\n",
      "Val - loss: 1.4116, F1: 0.6482, UW_Acc: 0.6515\n",
      "Val Weights - Text: 0.2716, Speech: 0.4178, Mocap: 0.3106\n",
      "Epoch [70/100], time: 9.63s\n",
      "Train - loss: 0.0524, F1: 0.9971, UW_Acc: 0.9971\n",
      "Train Weights - Text: 0.3470, Speech: 0.3554, Mocap: 0.2976\n",
      "Val - loss: 1.3838, F1: 0.6452, UW_Acc: 0.6489\n",
      "Val Weights - Text: 0.2716, Speech: 0.4163, Mocap: 0.3122\n",
      "Epoch [71/100], time: 9.72s\n",
      "Train - loss: 0.0530, F1: 0.9953, UW_Acc: 0.9953\n",
      "Train Weights - Text: 0.3483, Speech: 0.3542, Mocap: 0.2975\n",
      "Val - loss: 1.3702, F1: 0.6538, UW_Acc: 0.6565\n",
      "Val Weights - Text: 0.2712, Speech: 0.4134, Mocap: 0.3154\n",
      "Epoch [72/100], time: 9.69s\n",
      "Train - loss: 0.0516, F1: 0.9971, UW_Acc: 0.9971\n",
      "Train Weights - Text: 0.3455, Speech: 0.3558, Mocap: 0.2987\n",
      "Val - loss: 1.4382, F1: 0.6525, UW_Acc: 0.6562\n",
      "Val Weights - Text: 0.2742, Speech: 0.4174, Mocap: 0.3084\n",
      "Epoch 00072: reducing learning rate of group 0 to 9.7656e-08.\n",
      "Epoch 00072: reducing learning rate of group 1 to 9.7656e-07.\n",
      "Epoch [73/100], time: 9.63s\n",
      "Train - loss: 0.0519, F1: 0.9969, UW_Acc: 0.9969\n",
      "Train Weights - Text: 0.3466, Speech: 0.3553, Mocap: 0.2981\n",
      "Val - loss: 1.3598, F1: 0.6598, UW_Acc: 0.6626\n",
      "Val Weights - Text: 0.2714, Speech: 0.4162, Mocap: 0.3123\n",
      "Epoch [74/100], time: 9.82s\n",
      "Train - loss: 0.0505, F1: 0.9987, UW_Acc: 0.9987\n",
      "Train Weights - Text: 0.3433, Speech: 0.3569, Mocap: 0.2998\n",
      "Val - loss: 1.3977, F1: 0.6498, UW_Acc: 0.6526\n",
      "Val Weights - Text: 0.2716, Speech: 0.4169, Mocap: 0.3115\n",
      "Epoch [75/100], time: 9.67s\n",
      "Train - loss: 0.0533, F1: 0.9958, UW_Acc: 0.9957\n",
      "Train Weights - Text: 0.3459, Speech: 0.3552, Mocap: 0.2989\n",
      "Val - loss: 1.3875, F1: 0.6499, UW_Acc: 0.6536\n",
      "Val Weights - Text: 0.2688, Speech: 0.4216, Mocap: 0.3095\n",
      "Epoch [76/100], time: 9.64s\n",
      "Train - loss: 0.0534, F1: 0.9963, UW_Acc: 0.9963\n",
      "Train Weights - Text: 0.3457, Speech: 0.3550, Mocap: 0.2993\n",
      "Val - loss: 1.3810, F1: 0.6563, UW_Acc: 0.6591\n",
      "Val Weights - Text: 0.2724, Speech: 0.4146, Mocap: 0.3130\n",
      "Epoch [77/100], time: 9.84s\n",
      "Train - loss: 0.0543, F1: 0.9962, UW_Acc: 0.9962\n",
      "Train Weights - Text: 0.3475, Speech: 0.3553, Mocap: 0.2972\n",
      "Val - loss: 1.3667, F1: 0.6560, UW_Acc: 0.6583\n",
      "Val Weights - Text: 0.2713, Speech: 0.4152, Mocap: 0.3134\n",
      "Epoch [78/100], time: 9.69s\n",
      "Train - loss: 0.0562, F1: 0.9957, UW_Acc: 0.9956\n",
      "Train Weights - Text: 0.3454, Speech: 0.3565, Mocap: 0.2981\n",
      "Val - loss: 1.4176, F1: 0.6547, UW_Acc: 0.6569\n",
      "Val Weights - Text: 0.2721, Speech: 0.4161, Mocap: 0.3118\n",
      "Epoch 00078: reducing learning rate of group 0 to 4.8828e-08.\n",
      "Epoch 00078: reducing learning rate of group 1 to 4.8828e-07.\n",
      "Epoch [79/100], time: 9.70s\n",
      "Train - loss: 0.0517, F1: 0.9975, UW_Acc: 0.9975\n",
      "Train Weights - Text: 0.3445, Speech: 0.3565, Mocap: 0.2991\n",
      "Val - loss: 1.3713, F1: 0.6507, UW_Acc: 0.6526\n",
      "Val Weights - Text: 0.2699, Speech: 0.4175, Mocap: 0.3126\n",
      "Epoch [80/100], time: 9.62s\n",
      "Train - loss: 0.0529, F1: 0.9971, UW_Acc: 0.9971\n",
      "Train Weights - Text: 0.3478, Speech: 0.3536, Mocap: 0.2986\n",
      "Val - loss: 1.3594, F1: 0.6529, UW_Acc: 0.6562\n",
      "Val Weights - Text: 0.2707, Speech: 0.4151, Mocap: 0.3143\n",
      "Epoch [81/100], time: 9.79s\n",
      "Train - loss: 0.0560, F1: 0.9961, UW_Acc: 0.9961\n",
      "Train Weights - Text: 0.3460, Speech: 0.3553, Mocap: 0.2987\n",
      "Val - loss: 1.3691, F1: 0.6532, UW_Acc: 0.6560\n",
      "Val Weights - Text: 0.2707, Speech: 0.4170, Mocap: 0.3122\n",
      "Epoch [82/100], time: 9.67s\n",
      "Train - loss: 0.0529, F1: 0.9973, UW_Acc: 0.9973\n",
      "Train Weights - Text: 0.3475, Speech: 0.3547, Mocap: 0.2978\n",
      "Val - loss: 1.3797, F1: 0.6538, UW_Acc: 0.6565\n",
      "Val Weights - Text: 0.2731, Speech: 0.4121, Mocap: 0.3148\n",
      "Epoch [83/100], time: 9.68s\n",
      "Train - loss: 0.0514, F1: 0.9971, UW_Acc: 0.9971\n",
      "Train Weights - Text: 0.3488, Speech: 0.3534, Mocap: 0.2978\n",
      "Val - loss: 1.3739, F1: 0.6540, UW_Acc: 0.6569\n",
      "Val Weights - Text: 0.2717, Speech: 0.4158, Mocap: 0.3125\n",
      "Epoch [84/100], time: 9.80s\n",
      "Train - loss: 0.0521, F1: 0.9973, UW_Acc: 0.9973\n",
      "Train Weights - Text: 0.3455, Speech: 0.3552, Mocap: 0.2993\n",
      "Val - loss: 1.3739, F1: 0.6596, UW_Acc: 0.6624\n",
      "Val Weights - Text: 0.2714, Speech: 0.4186, Mocap: 0.3099\n",
      "Epoch 00084: reducing learning rate of group 0 to 2.4414e-08.\n",
      "Epoch 00084: reducing learning rate of group 1 to 2.4414e-07.\n",
      "Epoch [85/100], time: 9.67s\n",
      "Train - loss: 0.0525, F1: 0.9970, UW_Acc: 0.9970\n",
      "Train Weights - Text: 0.3453, Speech: 0.3555, Mocap: 0.2992\n",
      "Val - loss: 1.3779, F1: 0.6579, UW_Acc: 0.6606\n",
      "Val Weights - Text: 0.2715, Speech: 0.4159, Mocap: 0.3125\n",
      "Epoch [86/100], time: 9.73s\n",
      "Train - loss: 0.0531, F1: 0.9968, UW_Acc: 0.9968\n",
      "Train Weights - Text: 0.3475, Speech: 0.3542, Mocap: 0.2983\n",
      "Val - loss: 1.3940, F1: 0.6529, UW_Acc: 0.6556\n",
      "Val Weights - Text: 0.2734, Speech: 0.4155, Mocap: 0.3111\n",
      "Epoch [87/100], time: 9.90s\n",
      "Train - loss: 0.0554, F1: 0.9950, UW_Acc: 0.9950\n",
      "Train Weights - Text: 0.3455, Speech: 0.3553, Mocap: 0.2991\n",
      "Val - loss: 1.3551, F1: 0.6593, UW_Acc: 0.6618\n",
      "Val Weights - Text: 0.2693, Speech: 0.4147, Mocap: 0.3161\n",
      "Epoch [88/100], time: 9.69s\n",
      "Train - loss: 0.0538, F1: 0.9964, UW_Acc: 0.9964\n",
      "Train Weights - Text: 0.3469, Speech: 0.3546, Mocap: 0.2985\n",
      "Val - loss: 1.3871, F1: 0.6527, UW_Acc: 0.6562\n",
      "Val Weights - Text: 0.2726, Speech: 0.4192, Mocap: 0.3082\n",
      "Epoch [89/100], time: 9.74s\n",
      "Train - loss: 0.0533, F1: 0.9960, UW_Acc: 0.9960\n",
      "Train Weights - Text: 0.3477, Speech: 0.3539, Mocap: 0.2984\n",
      "Val - loss: 1.3767, F1: 0.6594, UW_Acc: 0.6620\n",
      "Val Weights - Text: 0.2730, Speech: 0.4146, Mocap: 0.3125\n",
      "Epoch [90/100], time: 9.80s\n",
      "Train - loss: 0.0522, F1: 0.9970, UW_Acc: 0.9970\n",
      "Train Weights - Text: 0.3467, Speech: 0.3554, Mocap: 0.2979\n",
      "Val - loss: 1.3696, F1: 0.6593, UW_Acc: 0.6618\n",
      "Val Weights - Text: 0.2724, Speech: 0.4136, Mocap: 0.3140\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.2207e-08.\n",
      "Epoch 00090: reducing learning rate of group 1 to 1.2207e-07.\n",
      "Epoch [91/100], time: 9.75s\n",
      "Train - loss: 0.0518, F1: 0.9969, UW_Acc: 0.9969\n",
      "Train Weights - Text: 0.3472, Speech: 0.3550, Mocap: 0.2979\n",
      "Val - loss: 1.3824, F1: 0.6493, UW_Acc: 0.6529\n",
      "Val Weights - Text: 0.2708, Speech: 0.4144, Mocap: 0.3148\n",
      "Epoch [92/100], time: 9.75s\n",
      "Train - loss: 0.0532, F1: 0.9964, UW_Acc: 0.9964\n",
      "Train Weights - Text: 0.3487, Speech: 0.3533, Mocap: 0.2980\n",
      "Val - loss: 1.3839, F1: 0.6579, UW_Acc: 0.6602\n",
      "Val Weights - Text: 0.2687, Speech: 0.4171, Mocap: 0.3142\n",
      "Epoch [93/100], time: 9.72s\n",
      "Train - loss: 0.0528, F1: 0.9974, UW_Acc: 0.9974\n",
      "Train Weights - Text: 0.3468, Speech: 0.3551, Mocap: 0.2981\n",
      "Val - loss: 1.4102, F1: 0.6452, UW_Acc: 0.6486\n",
      "Val Weights - Text: 0.2743, Speech: 0.4144, Mocap: 0.3114\n",
      "Epoch [94/100], time: 9.87s\n",
      "Train - loss: 0.0517, F1: 0.9978, UW_Acc: 0.9978\n",
      "Train Weights - Text: 0.3463, Speech: 0.3548, Mocap: 0.2988\n",
      "Val - loss: 1.3858, F1: 0.6558, UW_Acc: 0.6585\n",
      "Val Weights - Text: 0.2709, Speech: 0.4176, Mocap: 0.3115\n",
      "Epoch [95/100], time: 9.69s\n",
      "Train - loss: 0.0516, F1: 0.9974, UW_Acc: 0.9974\n",
      "Train Weights - Text: 0.3463, Speech: 0.3544, Mocap: 0.2994\n",
      "Val - loss: 1.3807, F1: 0.6545, UW_Acc: 0.6565\n",
      "Val Weights - Text: 0.2699, Speech: 0.4185, Mocap: 0.3117\n",
      "Epoch [96/100], time: 9.73s\n",
      "Train - loss: 0.0533, F1: 0.9971, UW_Acc: 0.9971\n",
      "Train Weights - Text: 0.3463, Speech: 0.3549, Mocap: 0.2988\n",
      "Val - loss: 1.3819, F1: 0.6500, UW_Acc: 0.6530\n",
      "Val Weights - Text: 0.2698, Speech: 0.4171, Mocap: 0.3132\n",
      "Epoch 00096: reducing learning rate of group 1 to 6.1035e-08.\n",
      "Epoch [97/100], time: 9.84s\n",
      "Train - loss: 0.0524, F1: 0.9975, UW_Acc: 0.9975\n",
      "Train Weights - Text: 0.3466, Speech: 0.3547, Mocap: 0.2987\n",
      "Val - loss: 1.4059, F1: 0.6527, UW_Acc: 0.6556\n",
      "Val Weights - Text: 0.2731, Speech: 0.4149, Mocap: 0.3120\n",
      "Epoch [98/100], time: 9.78s\n",
      "Train - loss: 0.0517, F1: 0.9977, UW_Acc: 0.9977\n",
      "Train Weights - Text: 0.3433, Speech: 0.3578, Mocap: 0.2989\n",
      "Val - loss: 1.3767, F1: 0.6585, UW_Acc: 0.6610\n",
      "Val Weights - Text: 0.2708, Speech: 0.4140, Mocap: 0.3152\n",
      "Epoch [99/100], time: 9.74s\n",
      "Train - loss: 0.0544, F1: 0.9960, UW_Acc: 0.9960\n",
      "Train Weights - Text: 0.3467, Speech: 0.3555, Mocap: 0.2978\n",
      "Val - loss: 1.3642, F1: 0.6538, UW_Acc: 0.6568\n",
      "Val Weights - Text: 0.2719, Speech: 0.4148, Mocap: 0.3133\n",
      "Epoch [100/100], time: 9.94s\n",
      "Train - loss: 0.0540, F1: 0.9962, UW_Acc: 0.9962\n",
      "Train Weights - Text: 0.3461, Speech: 0.3537, Mocap: 0.3002\n",
      "Val - loss: 1.3797, F1: 0.6548, UW_Acc: 0.6573\n",
      "Val Weights - Text: 0.2695, Speech: 0.4156, Mocap: 0.3149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [1.1955225731051244,\n",
       "  0.8886402512705603,\n",
       "  0.7579318215680677,\n",
       "  0.673343769339628,\n",
       "  0.5863422231618748,\n",
       "  0.5249941640121992,\n",
       "  0.47843838015268014,\n",
       "  0.41670882563258327,\n",
       "  0.3778486071630966,\n",
       "  0.3495812104191891,\n",
       "  0.3122910687396693,\n",
       "  0.2863878105268922,\n",
       "  0.2693114758923996,\n",
       "  0.2245076620994612,\n",
       "  0.1889611766781918,\n",
       "  0.17359101113884948,\n",
       "  0.16333338393028393,\n",
       "  0.15777981177318928,\n",
       "  0.15733104060555614,\n",
       "  0.14182114826385364,\n",
       "  0.13777344448621884,\n",
       "  0.1302212157914805,\n",
       "  0.12430706672197164,\n",
       "  0.12848506521346958,\n",
       "  0.10527856613314429,\n",
       "  0.09076940649470618,\n",
       "  0.09024466210326483,\n",
       "  0.0880931698132393,\n",
       "  0.08740681617758995,\n",
       "  0.08998939074402632,\n",
       "  0.07632826615211576,\n",
       "  0.07485216162925543,\n",
       "  0.0698351708435735,\n",
       "  0.06950563382963802,\n",
       "  0.06614254692266154,\n",
       "  0.06785599098995675,\n",
       "  0.06325140023647352,\n",
       "  0.05924138790646265,\n",
       "  0.059794145441332526,\n",
       "  0.06343710223256155,\n",
       "  0.06321996502404989,\n",
       "  0.06084943701361501,\n",
       "  0.05721440244206162,\n",
       "  0.05335386365998623,\n",
       "  0.05572749041887217,\n",
       "  0.055704221995764,\n",
       "  0.05613735205558843,\n",
       "  0.054919591996558874,\n",
       "  0.05288055149275203,\n",
       "  0.056811080422512324,\n",
       "  0.05439449959369593,\n",
       "  0.0550446147315724,\n",
       "  0.050797976553440094,\n",
       "  0.053938324108373285,\n",
       "  0.05263985009040943,\n",
       "  0.05426842294806658,\n",
       "  0.05221747069857841,\n",
       "  0.05240198978504469,\n",
       "  0.052400215680516046,\n",
       "  0.05413991888595182,\n",
       "  0.05400744253812834,\n",
       "  0.05602068332738654,\n",
       "  0.053923406140055766,\n",
       "  0.052216293593478756,\n",
       "  0.05215888266819854,\n",
       "  0.05398495135785535,\n",
       "  0.05374638580305632,\n",
       "  0.052959549565647926,\n",
       "  0.05524672602498254,\n",
       "  0.052357892830704536,\n",
       "  0.05297294718235038,\n",
       "  0.051647693005412124,\n",
       "  0.05188717615119247,\n",
       "  0.05051947722947875,\n",
       "  0.053328667095927304,\n",
       "  0.05336441359547681,\n",
       "  0.05429855205638464,\n",
       "  0.05624432030112245,\n",
       "  0.05170196320774943,\n",
       "  0.05289737291114275,\n",
       "  0.05603297409969707,\n",
       "  0.05293899202762648,\n",
       "  0.05142653854780419,\n",
       "  0.05213115134731282,\n",
       "  0.05248276282881582,\n",
       "  0.05311456911785658,\n",
       "  0.05544104721657066,\n",
       "  0.05379849291125009,\n",
       "  0.05330317701364672,\n",
       "  0.05224548800047054,\n",
       "  0.0518353219295657,\n",
       "  0.05323427257149718,\n",
       "  0.05276163346892179,\n",
       "  0.05165401101112366,\n",
       "  0.05155924569035685,\n",
       "  0.05331142870492713,\n",
       "  0.0523893887740235,\n",
       "  0.05165151402700779,\n",
       "  0.05439842795563299,\n",
       "  0.05399158960858057],\n",
       " 'train_f1': [0.4683688846779072,\n",
       "  0.645186904908524,\n",
       "  0.7126614279860316,\n",
       "  0.7492341837676298,\n",
       "  0.7917596556764799,\n",
       "  0.8188884099416894,\n",
       "  0.8346452623345856,\n",
       "  0.8618679415693368,\n",
       "  0.8805493347088281,\n",
       "  0.8874895652155778,\n",
       "  0.9024585060937812,\n",
       "  0.9172846796965267,\n",
       "  0.921780118338017,\n",
       "  0.9409306386776998,\n",
       "  0.9561242238766049,\n",
       "  0.9607698167816644,\n",
       "  0.9632423441424344,\n",
       "  0.9601140291603477,\n",
       "  0.9612874882962321,\n",
       "  0.9715972452182676,\n",
       "  0.9690210111305636,\n",
       "  0.9727386855825032,\n",
       "  0.9751244576708594,\n",
       "  0.9705646695949411,\n",
       "  0.981364836887896,\n",
       "  0.9864284131047993,\n",
       "  0.9864273779745959,\n",
       "  0.988923329748493,\n",
       "  0.9868546696503111,\n",
       "  0.9845366115297105,\n",
       "  0.9915576247189676,\n",
       "  0.9915671074436784,\n",
       "  0.9932258560217259,\n",
       "  0.9934751200357712,\n",
       "  0.9923770068647544,\n",
       "  0.9919611250391018,\n",
       "  0.9946624632651727,\n",
       "  0.9963764708633177,\n",
       "  0.9952828985765232,\n",
       "  0.9946504133997425,\n",
       "  0.993718265874696,\n",
       "  0.9959343142124085,\n",
       "  0.9955596149804351,\n",
       "  0.997821016042832,\n",
       "  0.9961871058710341,\n",
       "  0.995656342185772,\n",
       "  0.9960064228649798,\n",
       "  0.9961938030599936,\n",
       "  0.9974549365546259,\n",
       "  0.9956609097857992,\n",
       "  0.9970184842679258,\n",
       "  0.9960079611964061,\n",
       "  0.9978246679639409,\n",
       "  0.9966555414982121,\n",
       "  0.9978241104575902,\n",
       "  0.9963679434731585,\n",
       "  0.9969252409534238,\n",
       "  0.9974639644655062,\n",
       "  0.9975798453271019,\n",
       "  0.9958369197496364,\n",
       "  0.9971049333735368,\n",
       "  0.9957609720213055,\n",
       "  0.9974573179360905,\n",
       "  0.9976401879151987,\n",
       "  0.9978291257472516,\n",
       "  0.9965578522405983,\n",
       "  0.99691777838181,\n",
       "  0.9965599450167897,\n",
       "  0.9962219722899303,\n",
       "  0.9971130277310531,\n",
       "  0.9952988888394233,\n",
       "  0.9971034607751701,\n",
       "  0.9969252051802454,\n",
       "  0.9987298261331192,\n",
       "  0.9957504051936747,\n",
       "  0.9962966779956415,\n",
       "  0.9961893579675204,\n",
       "  0.9956618128513104,\n",
       "  0.9974612321284635,\n",
       "  0.9971009885366773,\n",
       "  0.9961250202966827,\n",
       "  0.9972825196375154,\n",
       "  0.9970937304813349,\n",
       "  0.9972796936163618,\n",
       "  0.9970119196663374,\n",
       "  0.9968415574958799,\n",
       "  0.9950229323378107,\n",
       "  0.9963751202223206,\n",
       "  0.9960461848257025,\n",
       "  0.997011793254896,\n",
       "  0.9969193058425745,\n",
       "  0.9963789182412193,\n",
       "  0.9973792295794977,\n",
       "  0.9978253288561704,\n",
       "  0.9973775641119659,\n",
       "  0.9971032332959205,\n",
       "  0.997469588795796,\n",
       "  0.9977410509602344,\n",
       "  0.9960170154109718,\n",
       "  0.996196798315772],\n",
       " 'train_uw_acc': [0.4731269820295983,\n",
       "  0.6483879492600423,\n",
       "  0.713150766384778,\n",
       "  0.7492897727272727,\n",
       "  0.7913253171247356,\n",
       "  0.8189581131078225,\n",
       "  0.834830866807611,\n",
       "  0.8617369186046512,\n",
       "  0.8795421511627907,\n",
       "  0.8874537526427062,\n",
       "  0.9023520084566597,\n",
       "  0.9170520613107822,\n",
       "  0.921742864693446,\n",
       "  0.9407042811839325,\n",
       "  0.9560484936575052,\n",
       "  0.9606236786469344,\n",
       "  0.963150766384778,\n",
       "  0.9598969344608879,\n",
       "  0.9612017706131077,\n",
       "  0.9715083245243129,\n",
       "  0.9689482029598309,\n",
       "  0.9726645084566597,\n",
       "  0.9750264270613108,\n",
       "  0.970418208245243,\n",
       "  0.9813028541226215,\n",
       "  0.9863735465116279,\n",
       "  0.9863900634249472,\n",
       "  0.9889171511627907,\n",
       "  0.9868360200845667,\n",
       "  0.9844741014799155,\n",
       "  0.991559857293869,\n",
       "  0.991559857293869,\n",
       "  0.9931950317124737,\n",
       "  0.9934593023255814,\n",
       "  0.9923691860465116,\n",
       "  0.9919397463002114,\n",
       "  0.9946485200845667,\n",
       "  0.9963662790697675,\n",
       "  0.9952761627906976,\n",
       "  0.9946485200845667,\n",
       "  0.9936740221987315,\n",
       "  0.995920322410148,\n",
       "  0.9955569503171248,\n",
       "  0.9978197674418605,\n",
       "  0.9961845930232558,\n",
       "  0.9956395348837209,\n",
       "  0.9960029069767442,\n",
       "  0.9961845930232558,\n",
       "  0.9974563953488372,\n",
       "  0.9956560517970402,\n",
       "  0.9970104386892178,\n",
       "  0.9960029069767442,\n",
       "  0.9978197674418605,\n",
       "  0.9966470665961946,\n",
       "  0.9978197674418605,\n",
       "  0.9963662790697675,\n",
       "  0.9969113372093024,\n",
       "  0.9974563953488372,\n",
       "  0.9975720137420719,\n",
       "  0.9958212209302325,\n",
       "  0.997093023255814,\n",
       "  0.9957551532769556,\n",
       "  0.9974563953488372,\n",
       "  0.9976380813953488,\n",
       "  0.9978197674418605,\n",
       "  0.9965479651162791,\n",
       "  0.9969113372093024,\n",
       "  0.9965479651162791,\n",
       "  0.9962011099365751,\n",
       "  0.9971095401691332,\n",
       "  0.9952926797040169,\n",
       "  0.997093023255814,\n",
       "  0.9969113372093024,\n",
       "  0.9987281976744186,\n",
       "  0.9957386363636365,\n",
       "  0.9962836945031713,\n",
       "  0.9961845930232558,\n",
       "  0.9956395348837209,\n",
       "  0.9974563953488372,\n",
       "  0.997093023255814,\n",
       "  0.9961185253699789,\n",
       "  0.9972747093023255,\n",
       "  0.997093023255814,\n",
       "  0.9972747093023255,\n",
       "  0.9970104386892178,\n",
       "  0.9968287526427062,\n",
       "  0.9950118921775899,\n",
       "  0.9963662790697675,\n",
       "  0.9960029069767442,\n",
       "  0.9970104386892178,\n",
       "  0.9969113372093024,\n",
       "  0.9963662790697675,\n",
       "  0.9973738107822411,\n",
       "  0.9978197674418605,\n",
       "  0.9973738107822411,\n",
       "  0.997093023255814,\n",
       "  0.9974563953488372,\n",
       "  0.9977371828752644,\n",
       "  0.9960029069767442,\n",
       "  0.9961845930232558],\n",
       " 'val_loss': [1.2933046966791153,\n",
       "  1.0030938014388084,\n",
       "  0.9595299139618874,\n",
       "  1.2069122195243835,\n",
       "  1.0079002007842064,\n",
       "  1.0811675190925598,\n",
       "  0.8943198025226593,\n",
       "  0.9266140162944794,\n",
       "  0.9033144935965538,\n",
       "  0.9983487352728844,\n",
       "  1.0927591621875763,\n",
       "  1.1255723536014557,\n",
       "  1.2945361882448196,\n",
       "  0.9954319894313812,\n",
       "  1.2162793204188347,\n",
       "  1.0628159493207932,\n",
       "  0.9704921022057533,\n",
       "  0.9792620241641998,\n",
       "  1.1634876877069473,\n",
       "  1.0948221683502197,\n",
       "  1.1019080206751823,\n",
       "  1.143165573477745,\n",
       "  1.1380738317966461,\n",
       "  1.0836683362722397,\n",
       "  1.083994098007679,\n",
       "  1.2753633931279182,\n",
       "  1.220728375017643,\n",
       "  1.176632359623909,\n",
       "  1.1816325187683105,\n",
       "  1.1884079426527023,\n",
       "  1.2160346060991287,\n",
       "  1.2442407310009003,\n",
       "  1.1888718456029892,\n",
       "  1.2298968732357025,\n",
       "  1.2152575254440308,\n",
       "  1.273384004831314,\n",
       "  1.283826783299446,\n",
       "  1.2511794865131378,\n",
       "  1.2886746227741241,\n",
       "  1.3379663228988647,\n",
       "  1.2647894471883774,\n",
       "  1.2947592586278915,\n",
       "  1.343004271388054,\n",
       "  1.3094181269407272,\n",
       "  1.349690929055214,\n",
       "  1.3749392181634903,\n",
       "  1.3404607623815536,\n",
       "  1.340022936463356,\n",
       "  1.3787607699632645,\n",
       "  1.3649676740169525,\n",
       "  1.3501764088869095,\n",
       "  1.3689754009246826,\n",
       "  1.3886521309614182,\n",
       "  1.3829699605703354,\n",
       "  1.3496134132146835,\n",
       "  1.3798600137233734,\n",
       "  1.3430195450782776,\n",
       "  1.382636547088623,\n",
       "  1.3377287834882736,\n",
       "  1.364148125052452,\n",
       "  1.3663793802261353,\n",
       "  1.363694742321968,\n",
       "  1.3880741894245148,\n",
       "  1.3502492159605026,\n",
       "  1.402305081486702,\n",
       "  1.3578184098005295,\n",
       "  1.3782308399677277,\n",
       "  1.4024035036563873,\n",
       "  1.4116104394197464,\n",
       "  1.3837617337703705,\n",
       "  1.3701866567134857,\n",
       "  1.4381675273180008,\n",
       "  1.3597547262907028,\n",
       "  1.3977274894714355,\n",
       "  1.3875228315591812,\n",
       "  1.3810127675533295,\n",
       "  1.3667437136173248,\n",
       "  1.4175714254379272,\n",
       "  1.3712500631809235,\n",
       "  1.3593635708093643,\n",
       "  1.3691194504499435,\n",
       "  1.3797090649604797,\n",
       "  1.3739011585712433,\n",
       "  1.3739208728075027,\n",
       "  1.3779405355453491,\n",
       "  1.3939772993326187,\n",
       "  1.3551252633333206,\n",
       "  1.3870918303728104,\n",
       "  1.3767101615667343,\n",
       "  1.3695694506168365,\n",
       "  1.3823979049921036,\n",
       "  1.3838723003864288,\n",
       "  1.4101998507976532,\n",
       "  1.3857915699481964,\n",
       "  1.380660355091095,\n",
       "  1.381884977221489,\n",
       "  1.4058536291122437,\n",
       "  1.376716509461403,\n",
       "  1.3641937971115112,\n",
       "  1.3797272145748138],\n",
       " 'val_f1': [0.3854633766157468,\n",
       "  0.5602558472540914,\n",
       "  0.5933351466780816,\n",
       "  0.5069808006542779,\n",
       "  0.6073087831980764,\n",
       "  0.5622828278936916,\n",
       "  0.6556431574123884,\n",
       "  0.6476290589674969,\n",
       "  0.6511555777504037,\n",
       "  0.6314094680093016,\n",
       "  0.6122093247046668,\n",
       "  0.6269954001379262,\n",
       "  0.5794013354659054,\n",
       "  0.6565326700284497,\n",
       "  0.6057508747929274,\n",
       "  0.6409079814029608,\n",
       "  0.684975819216086,\n",
       "  0.6884858626275125,\n",
       "  0.6415117467818023,\n",
       "  0.6479674209002756,\n",
       "  0.6456658515556623,\n",
       "  0.6539659519960714,\n",
       "  0.6547568947895603,\n",
       "  0.6760398756410664,\n",
       "  0.6806236411601544,\n",
       "  0.6340839404989341,\n",
       "  0.6396945013336104,\n",
       "  0.6616671552029154,\n",
       "  0.6608338235883034,\n",
       "  0.6662224760098046,\n",
       "  0.6526350423345835,\n",
       "  0.6514911637150439,\n",
       "  0.669024989099955,\n",
       "  0.6460079491356534,\n",
       "  0.6513522531086084,\n",
       "  0.6465804536311819,\n",
       "  0.6604182519322067,\n",
       "  0.665020249472028,\n",
       "  0.6558314006329815,\n",
       "  0.6528417174108695,\n",
       "  0.6711277047193025,\n",
       "  0.6562101621832279,\n",
       "  0.65676310123748,\n",
       "  0.6533693564573047,\n",
       "  0.6539430765243825,\n",
       "  0.6394095838412853,\n",
       "  0.658677678215556,\n",
       "  0.6609075591567891,\n",
       "  0.6501294187884074,\n",
       "  0.6573595198613468,\n",
       "  0.6552160991283794,\n",
       "  0.6532338686911373,\n",
       "  0.6457761835501923,\n",
       "  0.6568319489275067,\n",
       "  0.6597594296236478,\n",
       "  0.6519596535862375,\n",
       "  0.6586792284723039,\n",
       "  0.6469577846628263,\n",
       "  0.6590253750942353,\n",
       "  0.6511765308249046,\n",
       "  0.652694964219741,\n",
       "  0.6568102058877784,\n",
       "  0.6497906945512321,\n",
       "  0.6542580713106355,\n",
       "  0.650663831740985,\n",
       "  0.6556508999517213,\n",
       "  0.6530540185394458,\n",
       "  0.6550934999240052,\n",
       "  0.6482333531782453,\n",
       "  0.6451716067496904,\n",
       "  0.6538441194131023,\n",
       "  0.6524812447515085,\n",
       "  0.6598480836115485,\n",
       "  0.6497775653591796,\n",
       "  0.6499425626480204,\n",
       "  0.6562931627886359,\n",
       "  0.6559582446476041,\n",
       "  0.6547042134574145,\n",
       "  0.6506782738434562,\n",
       "  0.652925051044238,\n",
       "  0.6532072542085999,\n",
       "  0.6538394227146223,\n",
       "  0.6540383730091717,\n",
       "  0.6596395802356961,\n",
       "  0.6579218312002444,\n",
       "  0.6528951823116962,\n",
       "  0.6592884563279421,\n",
       "  0.6527137418353577,\n",
       "  0.6593602700143437,\n",
       "  0.6593087152424131,\n",
       "  0.6493066706032969,\n",
       "  0.657938394680647,\n",
       "  0.6452239437761758,\n",
       "  0.6558385899249894,\n",
       "  0.6545454514739443,\n",
       "  0.6499806051267979,\n",
       "  0.6527136392260737,\n",
       "  0.6584664930640919,\n",
       "  0.653766057391679,\n",
       "  0.6548176000127377],\n",
       " 'val_uw_acc': [0.45197860054347827,\n",
       "  0.5815641983695652,\n",
       "  0.6128141983695652,\n",
       "  0.5523097826086957,\n",
       "  0.6176545516304348,\n",
       "  0.5851307744565217,\n",
       "  0.6589249320652174,\n",
       "  0.6528532608695652,\n",
       "  0.6561650815217391,\n",
       "  0.6407523777173914,\n",
       "  0.6221976902173914,\n",
       "  0.6334918478260869,\n",
       "  0.5939198369565217,\n",
       "  0.6608355978260869,\n",
       "  0.6145974864130435,\n",
       "  0.6467815896739131,\n",
       "  0.6877547554347826,\n",
       "  0.6908967391304348,\n",
       "  0.6489045516304348,\n",
       "  0.6524286684782609,\n",
       "  0.6502632472826086,\n",
       "  0.6586701766304348,\n",
       "  0.6598165760869565,\n",
       "  0.6791779891304348,\n",
       "  0.6834239130434783,\n",
       "  0.6397758152173914,\n",
       "  0.6460173233695652,\n",
       "  0.6666949728260869,\n",
       "  0.6641049592391304,\n",
       "  0.6694123641304348,\n",
       "  0.6563349184782609,\n",
       "  0.6553583559782609,\n",
       "  0.6713654891304348,\n",
       "  0.6494989809782609,\n",
       "  0.6534052309782609,\n",
       "  0.6509001358695652,\n",
       "  0.6631708559782609,\n",
       "  0.6676715353260869,\n",
       "  0.6590947690217391,\n",
       "  0.6569293478260869,\n",
       "  0.6739130434782609,\n",
       "  0.6588824728260869,\n",
       "  0.6598590353260869,\n",
       "  0.6559527853260869,\n",
       "  0.6573114809782609,\n",
       "  0.6436820652173914,\n",
       "  0.6614300271739131,\n",
       "  0.6631708559782609,\n",
       "  0.6530230978260869,\n",
       "  0.6598590353260869,\n",
       "  0.6577360733695652,\n",
       "  0.6565472146739131,\n",
       "  0.6489470108695652,\n",
       "  0.6594769021739131,\n",
       "  0.6624065896739131,\n",
       "  0.6549762228260869,\n",
       "  0.6606657608695652,\n",
       "  0.6509001358695652,\n",
       "  0.6608355978260869,\n",
       "  0.6538298233695652,\n",
       "  0.6555706521739131,\n",
       "  0.6588824728260869,\n",
       "  0.6526409646739131,\n",
       "  0.6563349184782609,\n",
       "  0.6542119565217391,\n",
       "  0.6579059103260869,\n",
       "  0.6561650815217391,\n",
       "  0.6579059103260869,\n",
       "  0.6514945652173914,\n",
       "  0.6489470108695652,\n",
       "  0.6565472146739131,\n",
       "  0.6561650815217391,\n",
       "  0.6625764266304348,\n",
       "  0.6526409646739131,\n",
       "  0.6536175271739131,\n",
       "  0.6590947690217391,\n",
       "  0.6582880434782609,\n",
       "  0.6569293478260869,\n",
       "  0.6526409646739131,\n",
       "  0.6561650815217391,\n",
       "  0.6559527853260869,\n",
       "  0.6565472146739131,\n",
       "  0.6569293478260869,\n",
       "  0.6624065896739131,\n",
       "  0.6606233016304348,\n",
       "  0.6555706521739131,\n",
       "  0.6618121603260869,\n",
       "  0.6561650815217391,\n",
       "  0.6619819972826086,\n",
       "  0.6618121603260869,\n",
       "  0.6528532608695652,\n",
       "  0.6602411684782609,\n",
       "  0.6485648777173914,\n",
       "  0.6585003396739131,\n",
       "  0.6565472146739131,\n",
       "  0.6530230978260869,\n",
       "  0.6555706521739131,\n",
       "  0.6610478940217391,\n",
       "  0.6567595108695652,\n",
       "  0.6573114809782609],\n",
       " 'train_weights': [tensor([0.3356, 0.2264, 0.4380], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1572, 0.3463, 0.4965], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1344, 0.4022, 0.4635], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1399, 0.4079, 0.4522], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1201, 0.4663, 0.4136], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1368, 0.4523, 0.4109], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1283, 0.4536, 0.4181], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1309, 0.4691, 0.4000], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1471, 0.4588, 0.3941], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1695, 0.4620, 0.3685], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1497, 0.4678, 0.3825], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1717, 0.4690, 0.3593], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1853, 0.4618, 0.3529], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1644, 0.4753, 0.3603], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.1873, 0.4639, 0.3488], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2274, 0.4582, 0.3144], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2297, 0.4596, 0.3106], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2383, 0.4440, 0.3176], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2355, 0.4398, 0.3247], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2459, 0.4357, 0.3184], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2457, 0.4228, 0.3316], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2696, 0.4112, 0.3192], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2659, 0.4163, 0.3179], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2746, 0.4007, 0.3247], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2747, 0.4007, 0.3246], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2845, 0.4066, 0.3089], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2918, 0.3961, 0.3121], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2967, 0.3898, 0.3135], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2766, 0.3969, 0.3264], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3100, 0.3910, 0.2990], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.2968, 0.3925, 0.3107], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3133, 0.3848, 0.3018], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3100, 0.3863, 0.3038], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3182, 0.3712, 0.3106], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3177, 0.3775, 0.3048], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3283, 0.3740, 0.2976], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3327, 0.3627, 0.3046], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3249, 0.3683, 0.3068], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3198, 0.3790, 0.3012], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3378, 0.3609, 0.3013], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3334, 0.3656, 0.3010], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3341, 0.3628, 0.3031], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3398, 0.3593, 0.3009], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3364, 0.3625, 0.3011], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3361, 0.3621, 0.3018], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3402, 0.3600, 0.2998], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3428, 0.3567, 0.3005], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3464, 0.3565, 0.2970], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3516, 0.3518, 0.2965], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3446, 0.3557, 0.2997], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3472, 0.3532, 0.2996], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3459, 0.3561, 0.2979], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3382, 0.3623, 0.2995], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3372, 0.3609, 0.3018], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3422, 0.3574, 0.3005], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3423, 0.3558, 0.3019], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3470, 0.3549, 0.2981], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3438, 0.3574, 0.2987], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3438, 0.3587, 0.2976], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3415, 0.3587, 0.2998], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3458, 0.3553, 0.2989], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3452, 0.3566, 0.2982], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3456, 0.3559, 0.2985], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3460, 0.3562, 0.2978], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3477, 0.3541, 0.2983], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3452, 0.3570, 0.2978], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3478, 0.3559, 0.2963], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3473, 0.3557, 0.2970], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3458, 0.3556, 0.2986], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3470, 0.3554, 0.2976], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3483, 0.3542, 0.2975], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3455, 0.3558, 0.2987], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3466, 0.3553, 0.2981], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3433, 0.3569, 0.2998], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3459, 0.3552, 0.2989], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3457, 0.3550, 0.2993], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3475, 0.3553, 0.2972], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3454, 0.3565, 0.2981], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3445, 0.3565, 0.2991], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3478, 0.3536, 0.2986], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3460, 0.3553, 0.2987], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3475, 0.3547, 0.2978], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3488, 0.3534, 0.2978], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3455, 0.3552, 0.2993], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3453, 0.3555, 0.2992], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3475, 0.3542, 0.2983], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3455, 0.3553, 0.2991], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3469, 0.3546, 0.2985], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3477, 0.3539, 0.2984], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3467, 0.3554, 0.2979], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3472, 0.3550, 0.2979], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3487, 0.3533, 0.2980], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3468, 0.3551, 0.2981], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3463, 0.3548, 0.2988], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3463, 0.3544, 0.2994], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3463, 0.3549, 0.2988], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3466, 0.3547, 0.2987], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3433, 0.3578, 0.2989], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3467, 0.3555, 0.2978], device='cuda:0', grad_fn=<DivBackward0>),\n",
       "  tensor([0.3461, 0.3537, 0.3002], device='cuda:0', grad_fn=<DivBackward0>)],\n",
       " 'val_weights': [tensor([0.1360, 0.2895, 0.5746], device='cuda:0'),\n",
       "  tensor([0.1072, 0.3224, 0.5704], device='cuda:0'),\n",
       "  tensor([0.1177, 0.3731, 0.5092], device='cuda:0'),\n",
       "  tensor([0.0987, 0.4834, 0.4179], device='cuda:0'),\n",
       "  tensor([0.1200, 0.3693, 0.5107], device='cuda:0'),\n",
       "  tensor([0.1164, 0.4562, 0.4274], device='cuda:0'),\n",
       "  tensor([0.1220, 0.4676, 0.4104], device='cuda:0'),\n",
       "  tensor([0.1263, 0.4446, 0.4291], device='cuda:0'),\n",
       "  tensor([0.1477, 0.4653, 0.3871], device='cuda:0'),\n",
       "  tensor([0.1308, 0.4470, 0.4221], device='cuda:0'),\n",
       "  tensor([0.1607, 0.4523, 0.3869], device='cuda:0'),\n",
       "  tensor([0.1603, 0.4910, 0.3487], device='cuda:0'),\n",
       "  tensor([0.1382, 0.4647, 0.3971], device='cuda:0'),\n",
       "  tensor([0.1855, 0.4473, 0.3673], device='cuda:0'),\n",
       "  tensor([0.1879, 0.4928, 0.3193], device='cuda:0'),\n",
       "  tensor([0.2272, 0.4464, 0.3264], device='cuda:0'),\n",
       "  tensor([0.2029, 0.4337, 0.3634], device='cuda:0'),\n",
       "  tensor([0.2063, 0.4512, 0.3426], device='cuda:0'),\n",
       "  tensor([0.2352, 0.4464, 0.3184], device='cuda:0'),\n",
       "  tensor([0.2280, 0.4237, 0.3483], device='cuda:0'),\n",
       "  tensor([0.2077, 0.4094, 0.3829], device='cuda:0'),\n",
       "  tensor([0.1856, 0.4467, 0.3677], device='cuda:0'),\n",
       "  tensor([0.1926, 0.4579, 0.3496], device='cuda:0'),\n",
       "  tensor([0.2516, 0.4331, 0.3153], device='cuda:0'),\n",
       "  tensor([0.2339, 0.4421, 0.3240], device='cuda:0'),\n",
       "  tensor([0.2347, 0.4567, 0.3086], device='cuda:0'),\n",
       "  tensor([0.2627, 0.4255, 0.3118], device='cuda:0'),\n",
       "  tensor([0.2143, 0.4249, 0.3608], device='cuda:0'),\n",
       "  tensor([0.2624, 0.4261, 0.3115], device='cuda:0'),\n",
       "  tensor([0.2465, 0.4425, 0.3110], device='cuda:0'),\n",
       "  tensor([0.2263, 0.4216, 0.3521], device='cuda:0'),\n",
       "  tensor([0.2377, 0.4470, 0.3153], device='cuda:0'),\n",
       "  tensor([0.2555, 0.4146, 0.3299], device='cuda:0'),\n",
       "  tensor([0.2613, 0.4122, 0.3266], device='cuda:0'),\n",
       "  tensor([0.2736, 0.4212, 0.3053], device='cuda:0'),\n",
       "  tensor([0.2791, 0.4082, 0.3128], device='cuda:0'),\n",
       "  tensor([0.2603, 0.4176, 0.3220], device='cuda:0'),\n",
       "  tensor([0.2566, 0.4184, 0.3250], device='cuda:0'),\n",
       "  tensor([0.2654, 0.4185, 0.3161], device='cuda:0'),\n",
       "  tensor([0.2760, 0.4125, 0.3116], device='cuda:0'),\n",
       "  tensor([0.2746, 0.4039, 0.3214], device='cuda:0'),\n",
       "  tensor([0.2555, 0.4249, 0.3197], device='cuda:0'),\n",
       "  tensor([0.2629, 0.4269, 0.3102], device='cuda:0'),\n",
       "  tensor([0.2707, 0.4150, 0.3143], device='cuda:0'),\n",
       "  tensor([0.2674, 0.4170, 0.3156], device='cuda:0'),\n",
       "  tensor([0.2700, 0.4198, 0.3102], device='cuda:0'),\n",
       "  tensor([0.2665, 0.4200, 0.3136], device='cuda:0'),\n",
       "  tensor([0.2781, 0.4147, 0.3072], device='cuda:0'),\n",
       "  tensor([0.2698, 0.4163, 0.3139], device='cuda:0'),\n",
       "  tensor([0.2698, 0.4167, 0.3135], device='cuda:0'),\n",
       "  tensor([0.2686, 0.4190, 0.3124], device='cuda:0'),\n",
       "  tensor([0.2659, 0.4217, 0.3125], device='cuda:0'),\n",
       "  tensor([0.2592, 0.4310, 0.3098], device='cuda:0'),\n",
       "  tensor([0.2625, 0.4210, 0.3166], device='cuda:0'),\n",
       "  tensor([0.2666, 0.4177, 0.3157], device='cuda:0'),\n",
       "  tensor([0.2663, 0.4180, 0.3157], device='cuda:0'),\n",
       "  tensor([0.2677, 0.4174, 0.3149], device='cuda:0'),\n",
       "  tensor([0.2639, 0.4214, 0.3147], device='cuda:0'),\n",
       "  tensor([0.2670, 0.4187, 0.3143], device='cuda:0'),\n",
       "  tensor([0.2703, 0.4170, 0.3127], device='cuda:0'),\n",
       "  tensor([0.2692, 0.4175, 0.3134], device='cuda:0'),\n",
       "  tensor([0.2686, 0.4195, 0.3119], device='cuda:0'),\n",
       "  tensor([0.2710, 0.4157, 0.3133], device='cuda:0'),\n",
       "  tensor([0.2682, 0.4164, 0.3154], device='cuda:0'),\n",
       "  tensor([0.2729, 0.4170, 0.3101], device='cuda:0'),\n",
       "  tensor([0.2716, 0.4175, 0.3109], device='cuda:0'),\n",
       "  tensor([0.2732, 0.4166, 0.3102], device='cuda:0'),\n",
       "  tensor([0.2731, 0.4190, 0.3079], device='cuda:0'),\n",
       "  tensor([0.2716, 0.4178, 0.3106], device='cuda:0'),\n",
       "  tensor([0.2716, 0.4163, 0.3122], device='cuda:0'),\n",
       "  tensor([0.2712, 0.4134, 0.3154], device='cuda:0'),\n",
       "  tensor([0.2742, 0.4174, 0.3084], device='cuda:0'),\n",
       "  tensor([0.2714, 0.4162, 0.3123], device='cuda:0'),\n",
       "  tensor([0.2716, 0.4169, 0.3115], device='cuda:0'),\n",
       "  tensor([0.2688, 0.4216, 0.3095], device='cuda:0'),\n",
       "  tensor([0.2724, 0.4146, 0.3130], device='cuda:0'),\n",
       "  tensor([0.2713, 0.4152, 0.3134], device='cuda:0'),\n",
       "  tensor([0.2721, 0.4161, 0.3118], device='cuda:0'),\n",
       "  tensor([0.2699, 0.4175, 0.3126], device='cuda:0'),\n",
       "  tensor([0.2707, 0.4151, 0.3143], device='cuda:0'),\n",
       "  tensor([0.2707, 0.4170, 0.3122], device='cuda:0'),\n",
       "  tensor([0.2731, 0.4121, 0.3148], device='cuda:0'),\n",
       "  tensor([0.2717, 0.4158, 0.3125], device='cuda:0'),\n",
       "  tensor([0.2714, 0.4186, 0.3099], device='cuda:0'),\n",
       "  tensor([0.2715, 0.4159, 0.3125], device='cuda:0'),\n",
       "  tensor([0.2734, 0.4155, 0.3111], device='cuda:0'),\n",
       "  tensor([0.2693, 0.4147, 0.3161], device='cuda:0'),\n",
       "  tensor([0.2726, 0.4192, 0.3082], device='cuda:0'),\n",
       "  tensor([0.2730, 0.4146, 0.3125], device='cuda:0'),\n",
       "  tensor([0.2724, 0.4136, 0.3140], device='cuda:0'),\n",
       "  tensor([0.2708, 0.4144, 0.3148], device='cuda:0'),\n",
       "  tensor([0.2687, 0.4171, 0.3142], device='cuda:0'),\n",
       "  tensor([0.2743, 0.4144, 0.3114], device='cuda:0'),\n",
       "  tensor([0.2709, 0.4176, 0.3115], device='cuda:0'),\n",
       "  tensor([0.2699, 0.4185, 0.3117], device='cuda:0'),\n",
       "  tensor([0.2698, 0.4171, 0.3132], device='cuda:0'),\n",
       "  tensor([0.2731, 0.4149, 0.3120], device='cuda:0'),\n",
       "  tensor([0.2708, 0.4140, 0.3152], device='cuda:0'),\n",
       "  tensor([0.2719, 0.4148, 0.3133], device='cuda:0'),\n",
       "  tensor([0.2695, 0.4156, 0.3149], device='cuda:0')]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# 优化器\n",
    "if isinstance(model_combined, nn.DataParallel):\n",
    "    optimizer = AdamW([\n",
    "        {'params': [p for n, p in model_combined.module.named_parameters() if 'dynamic_query' not in n]},\n",
    "        {'params': model_combined.module.dynamic_query.parameters(), 'lr': 0.001}\n",
    "    ], lr=0.0001, weight_decay=0.001)\n",
    "else:\n",
    "    optimizer = AdamW([\n",
    "        {'params': [p for n, p in model_combined.named_parameters() if 'dynamic_query' not in n]},\n",
    "        {'params': model_combined.dynamic_query.parameters(), 'lr': 0.001}\n",
    "    ], lr=0.0001, weight_decay=0.001)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# 调用训练函数进行训练\n",
    "train_model(model_combined, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=100, device=device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 213609,
     "sourceId": 464671,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1834494,
     "sourceId": 2993857,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5134695,
     "sourceId": 8586922,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5137704,
     "sourceId": 8589552,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5137710,
     "sourceId": 8589561,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3698.97512,
   "end_time": "2024-09-18T18:15:23.270283",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-18T17:13:44.295163",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
