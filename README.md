In this work, we leverage text, audio, and motion capture data to recognize speakers' emotions.

We developed and fine-tuned models from scratch, including depthwise separable convolution and LSTMs, to analyze multimodal features (audio, text, movement data). In the meantime, we integrated self- and cross-attention to extract temporal and cross-modality information, improving F1 score and accuracy by over 10\% in 4-way emotion recognition tasks.

The feature abstraction is primarily based on the work presented in https://arxiv.org/abs/1804.05788, although We made several improvements.

If you find it helpful and interesting, please give it a star. Any comments are also welcome at zichenxu[at]gmail[dot]com. Thanks!
