In this work, we leverage text, audio, and motion capture data to recognize the emotions of speakers.

We present 9 types of models, some of which approach or even surpass the SOTA (SDT: Weighted F1: 74.080% and Unweighted Accuracy: 73.95%). For example, Model-8 achieves 70.49% on Weighted F1 and 70.98% on Unweighted Accuracy, while Model-3 achieves 76.38% on Weighted F1 and 76.44% on Unweighted Accuracy.

The feature abstraction is primarily based on the work presented in https://arxiv.org/abs/1804.05788. We made several improvements by incorporating depthwise separable convolution, self & cross attention mechanisms, and dynamic information borrowing structures.

If you find it helpful and interesting, please feel free to give it a star. Any comments are also welcome at zichenxu[at]gmail[dot]com. Thanks!
