In this work, we leverage on text + audio + movement capture data to recognize the emotion of speakers. 

We present 9 kinds of models and some of them get closer or even surpass the SOTA(SDT: Weighted F1: 74.080) For example, model-8 get 70.49 on weighted F1 and 70.98 on unweighted accuracy. Model-3 get 76.38 on weighted F1 and 76.44 on unweighted accuracy.

The feature abstraction is mainly based on this work: https://arxiv.org/abs/1804.05788. And we made a lots of improvements by trying to use depthwise separable convolution, self&cross attention mechanisms and dynamic info borrowing structures.

If you find it helpful and interesting, please feel free to give it a star. Any comments are also welcome at zichenxu[at]gmail[dot]com. Thanks!
